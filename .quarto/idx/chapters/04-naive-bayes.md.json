{"title":"The Naive Bayes classification method","markdown":{"headingText":"The Naive Bayes classification method","containsRefs":false,"markdown":"\\lstset{columns=fullflexible,breaklines=true,basicstyle=\\small\\ttfamily,backgroundcolor=\\color{gray!10}}\n\n\n\n## Introduction\n\nIn our discussion of Bayes Theorem, we looked at a situation in which we had a population consisting of\npeople infected with COVID-19 and people not infected, and we had a test that we could apply to\ndetermine which class an individual belonged to.  Because our test was not 100 percent reliable,\na positive test result didn't guarantee that a person was infected, and we used Bayes Theorem\nto evaluate how to interpret the positive test result.  More specifically, our information\nabout the test performance gave us the the conditional probabilities of positive and negative\ntest results given infection status -- so for example we were given $P(+|\\mathrm{infected})$,\nthe chance of getting a positive test assuming the person is infected -- and we used\nBayes Theorem to determine $P(\\mathrm{infected}|+)$, the chance that a person was infected\ngiven a positive test result.\n\nThe Naive Bayes classification method is a generalization of this idea.  We have data that\nbelongs to one of two classes, and based on the results of a series of tests, we wish to decide\nwhich class a particular data point belongs to.  For one example, we are given a collection\nof product reviews from a website and we wish to classify those reviews as either\n\"positive\" or \"negative.\"  This type of problem is called \"sentiment analysis.\" For another, related example,\nwe have a collection of emails or text messages and we wish to label those that are likely \"spam\" emails.\nIn both of these examples, the \"test\" that we will apply is to look for the appearance or absence\nof certain key words that make the text more or less likely to belong to a certain class.  For example,\nwe might find that a movie review that contains the word \"great\" is more likely to be positive than negative,\nwhile a review that contains the word \"boring\" is more likely to be negative.\n\nThe reason for the word \"naive\" in the name of this method is that we will derive our\nprobabilities from empirical data, rather than from any deeper theory.\nFor example, to find the probability that a negative\nmovie review contains the word \"boring\", we will look at a bunch of reviews that our experts\nhave said are negative, and compute the proportion of those that contain the word boring.  Indeed,\nto develop our family of tests, we will rely on a training set of already classified data\nfrom which we can determine estimates of probabilities that we need.\n\n## An example dataset\n\nTo illustrate the Naive Bayes algorithm, we will work with the \"Sentiment Labelled Sentences Data Set\"\n(@sentences). This dataset contains 3 files, \neach containing 1000 documents labelled\n$0$ or $1$ for \"negative\" or \"positive\" sentiment.  There are 500 of each type of document\nin each file.  One file contains reviews of products from amazon.com;\none contains yelp restaurant reviews, and one contains movie reviews from imdb.com.\n\nLet's focus on the amazon reviews data.  Here are some samples:\n```\nSo there is no way for me to plug it in here \n    in the US unless I go by a converter.\t0\nGood case, Excellent value.\t1\nGreat for the jawbone.\t1\nTied to charger for conversations lasting more than \n    45 minutes.MAJOR PROBLEMS!!\t0\nThe mic is great.\t1\nI have to jiggle the plug to get it to line up right to \n    get decent volume.\t0\nIf you have several dozen or several hundred contacts, then \n    imagine the fun of sending each of them one by one.\t0\nIf you are Razr owner...you must have this!\t1\nNeedless to say, I wasted my money.\t0\nWhat a waste of money and time!.\t0\n```\nAs you can see, each line consists of a product review followed by a $0$ or $1$; \nin this file the review is separated from the text by a tab character.\n\n## Bernoulli tests \n\nWe will describe the \"Bernoulli\" version of a Naive Bayes classifier for this data.  The building\nblock of this method is a test based on a single word.  For example, let's consider the word\n**great** among all of our amazon reviews.  It turns out that **great** occurs $5$ times in negative\nreviews and $92$ times in positive reviews among our $1000$ examples.\nSo it seems that seeing the word **great** in a review makes it more likely to be positive.\nThe appearances of great are summarized in @tbl-great . We write ~**great** for the case where\n**great** does *not* appear.\n\n\n|            | + | - | total |\n|------------|---|---|-------|\n| **great**  |92   | 5  |  97     |\n| ~**great** |408   | 495  | 903      |\n| total      | 500  | 500  | 1000      |\n\n: Ocurrences of **great** by type of review {#tbl-great} \n\n\nIn this data, positive and negative reviews are equally likely so $P(+)=P(-)=.5$\nFrom this table, and Bayes Theorem, we obtain the empirical probabilities (or \"naive\" probabilities).\n\n$$\nP(\\mathbf{great} | +) = \\frac{92}{500} = .184\n$$\n\nand\n\n$$\nP(\\mathbf{great}) = \\frac{97}{1000} = .097\n$$\n\nTherefore\n\n$$\nP(+|\\mathbf{great}) = \\frac{.184}{.097}(.5) = 0.948.\n$$\n\nIn other words, *if* you see the word **great** in a review, there's a 95% chance\nthat the review is positive.\n\nWhat if you *do not* see the word **great**? A similar calculation from the table yields\n\n$$\nP(+|\\sim\\mathbf{great}) = \\frac{408}{903} = .452\n$$\n\nIn other words, *not* seeing the word great gives a little evidence that the review\nis negative (there's a 55% chance it's negative) but it's not that conclusive.\n\nThe word **waste** is associated with negative reviews.  It's statistics are summarized\nin @tbl-waste . \n\n|            | + | - | total |\n|------------|---|---|-------|\n| **waste**  | 0  | 14  | 14     |\n| ~**waste** | 500  | 486   | 986      |\n| total      | 500  | 500  |  1000     |\n\n: Ocurrences of **waste** by type of review {#tbl-waste} \n\nBased on this data, the \"naive\" probabilities we are interested in are:\n\n\\begin{align*}\nP(+|\\mathbf{waste}) &= 0\\\\\nP(+|\\sim\\mathbf{waste}) &= .51\n\\end{align*}\n\nIn other words, if you see **waste** you definitely have a negative review,\nbut if you don't, you're only slightly more likely to have a positive one.\n\nWhat about combining these two tests?  Or using even more words?\nWe could analyze our data\nto count cases in which both **great** and **waste** occur, in which only one occurs,\nor in which neither occurs, within the two different categories of reviews,\nand then use those counts to estimate empirical probabilities of the joint events.\nBut while this might be feasible with two words, if we want to use many words,\nthe number of combinations quickly becomes huge.  So instead, \nwe make a basic, and probably false,\nassumption, but one that makes a simple analysis possible.  \n\n**Assumption:** We assume that the presence or absence of the words **great** and\n**waste** in a particular review (positive or negative) are independent events. More generally, given\na collection of words $w_1,\\ldots, w_k$, we assume that their occurences in a\ngiven review are independent events.\n\nIndependence means that we have\n\\begin{align*}\nP(\\mathbf{great},\\mathbf{waste}|\\pm) &= P(\\mathbf{great}|\\pm)P(\\mathbf{waste}|\\pm)\\\\\nP(\\mathbf{great},\\sim\\mathbf{waste}|\\pm) &= P(\\mathbf{great}|\\pm)P(\\sim\\mathbf{waste}|\\pm)\\\\\n &\\vdots \\\\\n\\end{align*}\n\nSo for example, if a document contains the word **great** and does *not* contain the word **waste**,\nthen the probability of it being a positive review is:\n$$\nP(+|\\mathbf{great},\\sim\\mathbf{waste}) = \\frac{P(\\mathbf{great}|+)P(\\sim\\mathbf{waste}|+)P(+)}{P(\\mathbf{great},\\sim\\mathbf{waste})}\n$$\nwhile the probability of it being a negative review is \n$$\nP(-|\\mathbf{great},\\sim\\mathbf{waste}) = \\frac{P(\\mathbf{great}|-)P(\\sim\\mathbf{waste}|-)P(-)}{P(\\mathbf{great},\\sim\\mathbf{waste})}\n$$\nRather than compute these probabilities (which involves working out the denominators), let's just compare them.\nSince they have the same denominators, we just need to compare numerators, which we call $L$ for likelihood:\nUsing the data from @tbl-great and @tbl-waste , we obtain:\n$$\nL(+|\\mathbf{great},\\sim\\mathbf{waste}) = (.184)(1)(.5) = .092\n$$\nand\n$$\nL(-|\\mathbf{great},\\sim\\mathbf{waste}) = (.01)(.028)(.5) = .00014\n$$\nso our data suggests strongly that this is a positive review.\n\n\n## Feature vectors\n\nTo generalize this, suppose that we have extracted keywords $w_1,\\ldots, w_k$ from our data\nand we have computed the empirical values\n$P(w_{i}|+)$ and $P(w_{i}|-)$ by counting the fraction of positive and negative reviews\nthat contain the word $w_{i}$:\n\n$$\nP(w_{i}|\\pm) = \\frac{\\hbox{ number of $\\pm$ reviews that mention $w_{i}$}}{\\hbox{ number of $\\pm$ reviews total}}\n$$\n\nNotice that we only count *reviews*, not *ocurrences*,\nso that if a word occurs multiple times in a review it only contributes 1 to the count.  That's why this\nis called the *Bernoulli* Naive Bayes -- we are thinking of each keyword as yielding a yes/no test \non each review.  \n\n\nGiven a review, we look to see whether each of our $k$ keywords appears or does not.  We encode\nthis information as a vector of length $k$ containing $0$'s and $1$'s indicating the absence\nor presence of the $k$th keyword.  Let's call this vector the *feature vector* for the review.\n\nFor example, if our keywords are $w_1=\\mathbf{waste}$, $w_2=\\mathbf{great}$, and $w_3=\\mathbf{useless}$, \nand our review says\n```\nThis phone is useless, useless, useless!  What a waste!\n```\nthen the associated feature vector is $f=(1,0,1)$. \n\nFor the purposes of classification of our reviews, we are going to forget entirely about the text\nof our reviews and work only with the feature vectors.  From an abstract perspective, then, by choosing our $k$ keywords,\nour \"training set\" of $N$ labelled reviews can be replaced by an $N\\times k$ matrix $X=(x_{ij})$ with entries $0$ or $1$,\nwhere $x_{ij}=1$ if and only if the $j^{th}$ keyword appears in the $i^{th}$ review.\n\nThe labels of $0$ or $1$ for unfavorable or favorable reviews can also be packaged up into a $N\\times 1$ vector $Y$\nthat serves as our \"target\" variable. \n\nSetting things up this way lets us express the computations of our probabilities $P(w_{i}|\\pm)$ in vector form.\nIn fact, $Y^{\\intercal}X$ is the sum of the rows of $X$ corresponding to positive reviews, and therefore, letting $N_{\\pm}$ denote the number of $\\pm$ reviews, \n$$\nP_{+} = \\frac{1}{N_{+}}Y^{\\intercal}X = \\left[\\begin{array}{cccc} P(w_{1}|+)& P(w_{2}|+) & \\cdots &P(w_{k}|+)\\end{array}\\right].\n$$\nSimilarly, since $Y$ and $X$ have zero and one entries only, if we write $1-Y$ and $1-X$ for the matrices obtained\nby replacing every entry $z$ by $1-z$ in each matrix, we have:\n$$\nP_{-} = \\frac{1}{N_{-}}(1-Y)^{\\intercal}X =  \\left[\\begin{array}{cccc} P(w_{1}|-)& P(w_{2}|-) & \\cdots &P(w_{k}|-)\\end{array}\\right].\n$$\n\nNote that the number of positive reviews is $N_{+}=Y^{\\intercal}Y$ and the number of negative ones is $N_{-}=N-N_{+}$.\nSince  $P(+)$ is the fraction of positive reviews among all reviews, we can compute it as  $P(+)=\\frac{1}{N}Y^{\\intercal}Y$, and $P(-)=1-P(+)$.\n\n## Likelihood\n\nIf a review has an associated feature vector $f=(f_1,\\ldots, f_k)$, then by independence\nthe probability of that\nfeature vector ocurring within one of the $\\pm$ classes is\n$$\nP(f|\\pm) = \\prod_{i: f_{i}=1} P(w_{i}|\\pm)\\prod_{i: f_{i}=0}(1-P(w_{i}|\\pm))\n$$\nwhich we can also write\n$$\nP(f|\\pm) = \\prod_{i=1}^{k} P(w_{i}|\\pm)^{f_{i}}(1-P(w_{i}|\\pm))^{(1-f_{i})}.\n$${#eq-likelihood}\n\nThese products aren't practical to work with -- they are often the product of many, many small\nnumbers and are therefore really tiny. Therefore it's much more practical to work with their logarithms.\n$$\n\\log P(f|\\pm) = \\sum_{i=1}^{k} f_{i}\\log P(w_{i}|\\pm) + (1-f_{i})\\log(1-P(w_{i}|\\pm))\n$${#eq-loglikelihood}\n\nIf we have a group of reviews $N$ organized in a matrix $X$, where each row is the feature vector associated\nto the corresponding review, then we can compute all of this at once.  We'll write $\\log P_{\\pm}=\\log P(X|\\pm)$ as the\nrow vector whose $i^{th}$ entry is $\\log P(f_{i}|\\pm)$:\n\n$$\n\\log P(X|\\pm) = X(\\log P_{\\pm})^{\\intercal}+(1-X)(\\log (1-P_{\\pm}))^{\\intercal}.\n$${#eq-matrixlikelihood}\n\n\nBy Bayes Theorem, we can express the chance that our review with feature vector $f$ is positive or negative\nby the formula:\n$$\n\\log P(\\pm|f) = \\log P(f|\\pm)+\\log P(\\pm) - \\log P(f)\n$$\nwhere \n$$\nP(\\pm) = \\frac{\\hbox{ the number of $\\pm$ reviews}}{\\hbox{ total number of reviews}}\n$$\nand $P(f)$ is the fraction of reviews with the given feature vector.  (Note: in practice,\nsome of these probabilities will be zero, and so the log will not be defined.  A common \npractical approach\nto dealing with this is to introduce a \"fake document\" into both classes in\nwhich every vocabulary word appears -- this guarantees that the frequency matrix will have no zeros in it).\n\nA natural classification rule would be to say that a review is positive if $\\log P(+|f)>\\log P(-|f)$,\nand negative otherwise.  In applying this, we can avoid computing $P(f)$ by just comparing \n$\\log P(f|+)+\\log P(+)$ and $\\log P(f|-)+\\log P(-)$ computed using @eq-loglikelihood.  Then we say:\n\n- a review is positive if $\\log P(f|+)+\\log P(+)>\\log P(f|-)+\\log P(-)$ and negative otherwise.\n\nAgain we can exploit the matrix structure to do this for a bunch of reviews at once.   Using @eq-matrixlikelihood\nand the vectors $P_{\\pm}$ we can compute column vectors corresponding to both sides of our decision\ninequality and subtract them.  The positive entries indicate positive reviews, and the negative ones, negative reviews.\n\n## The Bag of Words\n\nIn our analysis above, we thought of the presence or absence of certain key words as a\nset of independent tests that provided evidence of whether our review was positive or negative.\nThis approach is suited to short pieces of text, but what about longer documents?  In that\ncase, we might want to consider not just the presence or absence of words,\nbut the frequency with which they appear.  Multinomial\nNaive Bayes, based on the \"bag of words\" model, is a classification method similar to Bernoulli Naive Bayes\nbut which takes term frequency into account.\n\nLet's consider, as above, the problem of classifying documents into one of two classes.  We assume that\nwe have a set of keywords $w_1,\\ldots, w_k$.  For each class $\\pm$, we have a set of probabilities\n$P(w_i|\\pm)$ with the property that\n$$\n\\sum_{i=1}^{k}P(w_{i}|\\pm)=1.\n$$\n\nThe \"bag of words\" model says that we construct a document of length $N$ in, say, the $+$ class\nby independently\ndrawing a word $N$ times from the set $w_1,\\ldots, w_k$ with probabilities $P(w_{i}|+)$.  The name\n\"bag of words\" comes from thinking \nof each class as having an associated bag containing the words $w_1,\\ldots, w_k$ with relative\nfrequencies given by the probabilities, and generating a document by repeatedly drawing a word from the bag.\n\nIn the Multinomial Naive Bayes method, we estimate the probabilities $P(w_{i}|\\pm)$\nby counting the number of times each word occurs in a document of the given class:\n$$\nP(w_{i}|\\pm) = \\frac{\\hbox{ number of times word $i$ occurs in $\\pm$ documents}}{\\hbox{ total number of words in $\\pm$ documents}}\n$$\nThis is the \"naive\" part of the algorithm.  Package up these probabilities in vectors:\n$$\nP_{\\pm} = \\left[\\begin{array}{ccc} P(w_{1}|\\pm) & \\cdots & P(w_{k}|\\pm)\\end{array}\\right].\n$$\n\nAs in the Bernoulli case, we often add a fake document to each class where all of the words\noccur once, in order to avoid having zero frequencies when we take a logarithm later.\n\nNow, given a document, we associate a feature vector $\\mathbf{f}$ whose $i^{th}$ entry is the\nfrequency with which word $i$ appears in that document.  The probability of obtaining\na particular document with feature vector $\\mathbf{f}=(f_1,\\ldots, f_k)$ from the bag of words\nassociated with class $\\pm$ is given by the \"multinomial\" distribution:\n$$\nP(\\mathbf{f}|\\pm)=\\frac{N!}{f_1!f_2!\\cdots f_k!} \\prod_{i=1}^{k} P(w_{i}|\\pm)^{f_{i}}\n$$\nwhich generalizes the binomial distribution to multiple choices. The constant will prove irrelevant,\nso let's call the interesting part $L_{\\pm}$:\n$$\nL(\\mathbf{f}|\\pm)= \\prod_{i=1}^{k} P(w_{i}|\\pm)^{f_{i}}\n$$\n\nFrom Bayes Theorem, we have \n$$\nP(\\pm|\\mathbf{f}) = \\frac{P(\\mathbf{f}|\\pm)P(\\pm)}{P(\\mathbf{f})}\n$$\nwhere $P(\\pm)$ is estimated by the fraction of documents (total) in each class.\n\nWe classify our document by considering $P(\\pm|\\mathbf{f})$ and concluding:\n\n- a document with feature vector $\\mathbf{f}$  is in class $+$ if $\\log P(+|\\mathbf{f})>\\log P(-|\\mathbf{f})$.\n\nIn this comparison, both the constant (the multinomial coefficient) and the denominator cancel out,\nso we only need to compare $\\log L(\\mathbf{f}|+)+\\log P(+)$ with $\\log L(\\mathbf{f}|-)+\\log P(-)$\nWe have\n$$\n\\log L(\\mathbf{f}|\\pm) = \\sum_{i=1}^{k} f_{i}\\log P(w_{i}|\\pm)\n$$\nor, in vector form, \n$$\n\\log P(\\mathbf{f}|\\pm) = \\mathbf{f}\\log P_{\\pm}^{\\intercal}\n$$\n\nTherefore, just as in the Bernoulli case, we can package up our document $i$ as an $N\\times k$ data matrix $X$,\nwhere position $ij$ gives the number of times word $j$ occurs in document $i$.  Then we can compute\nthe vector \n$$\n\\hat{Y} = X\\log P_{+}^{\\intercal} + \\log P(+)-X\\log P_{-}^{\\intercal} - \\log P(-)\n$$\nand assign those documents where $\\hat{Y}>0$ to the $+$ class and the rest to the $-$ class.\n\n\n## Other applications\n\nWe developed the Naive Bayes method for sentiment analysis, but once\nwe chose a set of keywords our training data was reduced to an\n$N\\times k$ matrix $X$ of $0/1$ entries, together with an $N\\times 1$\ntarget column vector $Y$.  Then our classification problem is to\ndecide whether a given vector of $k$ entries, all $0$ or $1$, is more\nlikely to carry a $0$ or $1$ label. All of the parameters we needed\nfor Naive Bayes -- the various probabilities -- can be extracted from\nthe matrix $X$.\n\nFor example, suppose\nwe have a collection of images represented as black/white pixels in a grid that belong to one of two classes.\nFor example, we might have $28x28$ bitmaps of handwritten zeros and ones that are labelled, and we wish to construct\na classifier that can decide whether a new $28x28$ bitmap is a zero or one.  An example of such a bitmap\nis given in @fig-mnist0.   We can view each $28x28$ bitmap as\na vector of length $784$ with $0/1$ entries and apply the same approach outlined above. \nHowever, there are other methods that are more commonly used for this problem, such as logistic regression\nand  neural networks. \n\n![Handwritten 0](img/mnist_data_10_0.png){#fig-mnist0 width=2in}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"04-naive-bayes.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":[{"text":"\\usepackage{listings}\n"},{"file":"../chapters/macros.tex"}],"output-file":"04-naive-bayes.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","documentclass":"scrreprt"},"extensions":{"book":{}}}}}