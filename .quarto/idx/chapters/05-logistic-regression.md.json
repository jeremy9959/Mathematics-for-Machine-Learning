{"title":"Logistic Regression","markdown":{"headingText":"Logistic Regression","containsRefs":false,"markdown":"\nSuppose that we are trying to convince customers to buy our product by showing them advertising.  Our experience teaches\nus that there is no deterministic relationship between how often a potential customer sees one of our ads and whether or not\nthey purchase our product, nevertheless it is the case that as they see more ads they become more likely to make a purchase.\nLogistic regression is a statistical model that can capture the essence of this idea.\n\nTo make this problem more abstract, let's imagine that we are trying to model a random event that depends on a parameter.\nAs in our introduction above, the random event might be a user deciding to make  a purchase from a website, which, in our very simple model,\ndepends on how many times the user saw an advertisement for the product in question.  But we could imagine other situations where\nthe chance of an event happening depends on a paramter.  For example, we could imagine that a student's score on a certain test depends on how much studying they\ndo, with the likelihood of passing the test increasing with the amount of studying.  \n\nTo construct this model, we assume that the probability of a certain event $p$ is related to some parameter $x$ by the following relationship:\n\n$$\n\\log\\frac{p}{1-p} = ax+b \n$${#eq-logistic_1}\n\nwhere $a$ and $b$ are constants.  The quantity $\\frac{p}{1-p}$ is the \"odds\" of the event occurring.  We often use this quantity colloquially; if\nthe chance of our team winning a football game is $1$ in $3$, then we would say the odds of a win are $1$-to-$2$, which we can interpret as meaning they \nare twice as likely to lose as to win.  The quantity $\\log\\frac{p}{1-p}$ is, for obvious reasons, called the log-odds of the event.\n\nThe assumption in @eq-logistic_1 can be written\n$$\n\\frac{p}{1-p} = e^{ax+b}\n$$\nand we interpret this as telling us that if the parameter $x$ increases by $1$, the odds of our event happening go up by a factor of $e^{a}$. \nSo, to be even more concrete, if $a=\\log 2$, then our logistic model would say that an increase of $1$ in our parameter $x$ doubles the odds of\nour event taking place. \n\nIn terms of the probability $p$, @eq-logistic_1 can be rewritten\n$$\np = \\frac{1}{1+e^{-ax-b}}\n$$\nThis proposed relationship between the probability $p$ and the parameter $x$ is called the *logistic model.*  The function\n$$\n\\sigma(x) = \\frac{1}{1+e^{-x}}\n$$\nis called the *logistic function* and yields an S-shaped curve. \n\n![Logistic Curve](img/logistic_curve.png){#fig-logistic_curve width=50%}\n\nTo fully put the logistic model in perspective, let's choose some explicit parameters and look at what data arising from such a model\nwould look like.  Imagine therefore that $a=\\log 2$ and $b=0$, so that the probability of the event we are interested occurring is given by the\nformula\n$$\np(x) = \\frac{1}{1+e^{-(\\log 2)x}} = \\frac{1}{1+(.5)^x}.\n$$\nOur data consists of counts of how often our event happened for a range of values of $x$.  To generate this data, we can pick $x$ values from the\nset $\\{-3,-2,-1,0,1,2,3\\}$ yielding probabilities $\\{.11,.2,.33,.4,.56,.67,.8\\}$.  Now our data consists of,  for each value of $x$, the result of $100$ independent\nBernoulli trials with probability $p(x)$.  For example, we might find that our event occurred $\\{10, 18, 38, 50, 69, 78, 86\\}$ times respectively for each of the $x$ values.\n\n\n## Likelihood and Logistic Regression\n\nIn applications, our goal is to choose the parameters of a logistic model to accurately predict the likelihood of the event under study occurring as a function\nof the measured parameter.  Let's imagine that we collected the data that we generated above, without knowing that it's source was a logistic model.  So\n@tbl-logistic_data shows the number of times the event occurred, for each of the measured values of the $x$ parameter.\n\n|$x$ |-3 | -2 | -1 | 0 | 1 | 2 | 3 |\n|---|---|---|---|---|---|---|---|\n|Occurrences (out of 100)|10  |18 | 38 | 50 | 69 | 78 | 86|\n\n: Sample Data {#tbl-logistic_data}\n\nOur objective now is to find a logistic model which best explains this data. Concretely, we need to estimate the coefficients $a$ and $b$ that yield \n$$\np(x) = \\frac{1}{1+e^{-ax-b}}\n$${#eq-logistic_a_b}\n\nwhere the resulting probabilities best estimate the data. As we have seen, this notion of \"best\" can have different interpretations.\nFor example, we could approach this from a Bayesian point of view, adopt a prior distribution on the parameters $a$ and $b$, and use the data to obtain\nthis prior and obtain a posterior distribution on $a$ and $b$.  For this first look at logistic regression, we will instead adopt a \"maximum likelihood\"\nnotion of \"best\" and ask what is the most likely choice of $a$ and $b$ to yield this data.\n\nTo apply the maximum likelihood approach, we need to ask \"for (fixed, but unknown) values of $a$ and $b$, what is the likelihood that a logistic model\nwith those parameters would yield the data we have collected?\" Each column in @tbl-logistic_data represents $100$ Bernoulli trials with a fixed probability\n$p(x)$.  So, for example,  the chance $q$ of obtaining $10$ positive results with $x=-3$ is given by\n$$\nq(-3)=C p(-3)^{10}(1-p(-3))^{90}\n$$\nwhere $C$ is a constant (it would be a binomial coefficient).  Combining this for different values of $x$, we see that the likelihood of the data is\nthe product\n$$\nL(a,b) = C' p(-3)^{10}(1-p(-3))^{90}p(-2)^{18}(1-p(-2))^{82}\\cdots p(3)^{86}(1-p(3))^{14}\n$$\nwhere $C'$ is another constant.  Each $p(x)$ is a function of the parameters $a$ and $b$, so all together this is a function of those two parameters.\nOur goal is to maximize it. \n\nOne step that simplifies matters is to consider the logarithm of the likelihood:\n$$\n\\log L (a,b)= \\sum_{i=0}^{6} \\left[ x_{i}\\log(p(x_{i})) + (100-x_{i})\\log(1-p(x_{i}))\\right] +C''\n$$\nwhere $C''$ is yet another constant.  Since our ultimate goal is to maximize this, the value of $C''$ is irrelevant and we can drop it.\n\n### Another point of view on logistic regression\n\nIn @tbl-logistic_data we summarize the results of our experiments in groups by the value of the $x$ parameter.  We can think of the data somewhat differently,\nby instead considering each event separately, corresponding to a parameter value $x$ and an outcome $0$ or $1$.  From this point of view the data summarized in @tbl-logistic_data\nwould correspond to a vector with  $700$ rows.  The first $100$ rows (corresponding to the first column of the table) would have first entry $-3$, the next $100$ would have $-2$, or so on.\nSo our parameter values form a  vector $X$. Meanwhile, the outcomes form a vector $Y$ with entries $0$ or $1$. \n\nMore generally,  imagine we are studying our advertising data and, for each potential customer, we record how many times they saw our ad.  We create a vector $X$ whose entries are\nthese numbers.  Then we create another vector $Y$, of the same length, whose entries are \neither $0$ or $1$ depending of whether or not the customer purchased our product.  \n\nOne way to think about logistic regression in this setting is that we are trying to fit a function that, given the value $x_i$, tries to yield the corresponding value $y_i$.\nHowever, instead of finding a deterministic function, as we did in linear regression,\ninstead we try to fit a logistic function that captures the likelihood that the $y$-value is a $1$ given the $x$-value.  This \"curve-fitting perspective\" is why this is considered\na regression problem.\n\nIf, as above, we think of each row of the matrix as an independent trial, then the chance that $y_i=1$ is $p(x_i)$ and the chance that $y_i=0$ is $1-p(x_i)$, where $p(x)$ is given by the logistic \nfunction as in @eq-logistic_a_b. \nThe likelihood of the results we obtained is therefore:\n$$\nL(a,b) = C \\prod_{i=0}^{N-1} p(x_i)^{y_i}(1-p(x_i))^{(1-y_i)}\n$$\nwhere $C$ is a constant and we are exploiting the trick that, since $y_i$ is either zero or one, $1-y_i$ is correspondingly one or zero.  Thus only $p(x_i)$ or $1-p(x_i)$\noccurs in each term of the product.  If we group the terms according to $x_i$ we obtain our earlier formula for $L(a,b)$.\n\nThis expresssion yields an apparently similar formula for the log-likelihood (up to an irrelevant constant):\n$$\n\\log L(X,a,b) = \\sum_{i=0}^{N-1} y_i\\log p(x_i) + (1-y_i)\\log (1-p(x_i)).\n$$\nUsing vector notation, this can be further simplified, where again we drop irrelevant constants:\n$$\n\\log L(X,a,b) = Y\\cdot\\log p(X) + (1-Y)\\cdot \\log(1-p(X)).\n$$\nTo be absolutely concrete, in this formula, $p(X)$ is a vector \n$$\np(X)=[p(x_i)]_{i=0}^{N-1} = \\left[\\frac{1}{1+e^{-ax_i-b}} \\right]_{i=0}^{N-1}\n$$\nso its entries are functions of the unknown parameters $a$ and $b$. \n\nWe might naively try to maximize this by taking the derivatives with respect to $a$ and $b$ and setting them to zero, but this turns out to be impractical.\nSo we need a different approach to finding the parameters $a$ and $b$ which maximize this likelihood function.  We will return to this problem later, but before\nwe do so we will look at some generalizations and broader applications of the logistic model.\n\n### Logistic regression with multiple features\n\nThe next generalization we can consider of the logistic model is the situation where the log-odds of our event of interest depend linearly on multiple parameters.\nIn other words, we have\n$$\n\\log\\frac{p}{1-p} = m_0 x_0 + m_1 x _1 + \\cdots + m_{k-1} x_{k-1} + b \n$$\nwhere the $a_i$ and $b$ are constants.  Under this model, notice that *the incremental effects of changes to the different parameters $x_i$ have independent effects on the probability.*\nSo, for example, if $x_1$ were the number of times our potential customer saw an online advertisement and $x_2$ were the number of times they saw a print advertisement, by adopting this model\nwe are assuming that the impact of seeing more online ads is completely unrelated to the impact of seeing more print ads.\n\nThe probability is again given by a sigmoid function\n$$\np(x_1,\\ldots, x_k) = \\frac{1}{1+e^{-\\sum_{i=0}^{k-1} m_i x_i +b}}\n$$\n\nThis model has an $N\\times k$ feature matrix whose rows are the values $x_0,\\ldots, x_{k-1}$ for each sample.  The outcome of our experimemt is recorded in an $N\\times 1$ column vector $Y$ whose entries\nare $0$ or $1$.  The likelihood function is formally equivalent to what we computed in the case of a single feature, but it will be useful to be a bit careful about vector notation.  \n\nFollowing the same pattern we adopted for linear regression, let $X$ be the $N\\times (k+1)$ matrix whose first $k$ columns contain the values $x_i$ for each sample, and whose last column is all $1$.\nRename the \"intercept\" variable as $a_{k+1}$ and organize these parameters into a $(k+1)\\times 1$ matrix $M$.  Then\n$$\np(X)=\\sigma(XM)\n$$\nand our likelihood becomes\n$$\n\\log L(M) = Y\\cdot \\log\\sigma(XM) + (1-Y)\\cdot(1-\\log\\sigma(XM)).\n$${#eq-logisticregressionlikelihood}\n\n## Finding the maximum likelihood solution by gradient descent\n\nGiven a set of features $X$ and targets $Y$ for a logistic model, we now want to find the values $M$ so that the log-likelihood of the model for those paramters, given\nthe data, is maximized.  While in linear regression we could find a nice closed form solution to this problem, the presence of the non-linear function $\\sigma(x)$ in the\nlikelihood makes that impossible for logistic regression.  Thus we need to use a numerical approximation.  The most straightforward such method is called gradient descent.\nIt is at the foundation of many numerical optimization algorithms, and so while we will develop it here for logistic regression we will have other opportunities to apply it\nand we will discuss it more thoroughly on its own later.\n\n### Gradient descent\n\nSuppose that we have a function $f(x_0,\\ldots, x_{k-1})$ and we wish to find its minimum value.  To apply gradient descent, we choose an initial starting point $c=(c_0,\\ldots, c_{k-1})$\nand we iteratively adjust the values of $c$ so that the values $f(c)$ decrease.  When we can no longer do that, we've found what is at least a local minimum of $f$.\n\nHow should we make these adjustments?  Let us remember the idea of the *directional derivative* from multivariate calculus.  The directional derivative $D_{v}f$ measures the rate of change of $f$ as one moves\nwith velocity vector $v$ from the point $x$ and it is defined as\n$$\nD_{v}f(x) = \\frac{d}{dt}f(x+tv)|_{t=0}\n$$\nFrom the chain rule, we can compute that\n$$\nD_{v}f(x) = \\sum_{i=0}^{k-1} \\frac{\\partial f}{\\partial x_{i}}\\frac{dx_{i}}{dt} = (\\nabla f)\\cdot v\n$$\nwhere\n$$\n\\nabla f = \\left[\\frac{\\partial f}{\\partial x_{i}}\\right]_{i=0}^{k-1}\n$$\nis the gradient of $f$.  This argument yields the following result.\n\n**Proposition:** Let $f(x_0,\\ldots, x_{k-1})$ be a smooth function from $\\mathbb{R}^{k}\\to\\mathbb{R}$.\nThen for every point $c=(c_0,\\ldots, c_{k-1})$, if $\\nabla f\\not=0$ then $\\nabla f$ is a vector pointing in \nthe direction in which $f$ increases most rapidly, and $-\\nabla f$ is a vector pointing\nin the direction in which $f$ decreases most rapidly.  If $\\nabla f = 0$, then $c$ is a critical\npoint of $f$. \n\n**Proof:** The directional derivative $D_{v}(f)=(\\nabla f)\\cdot v$ measures the rate of change of $f$ if we\ntravel with velocity $v$ from a point $x$.  To remove the dependence on the magnitude of $v$ (since obviously\n$f$ will change more quickly if we travel more quickly in a given direction), we scale $v$ to be a unit vector. Then the dot product giving the rate is maximized when $v$ is parallel to $\\nabla f$.\n\nThis observation about the gradient yields the algorithm for gradient descent.  \n\n**Gradient Descent Algorithm:** Given a function $f:\\mathbb{R}^{k}\\to \\mathbb{R}$, choose a point\n$c^{(0)}$, a small constant $\\nu$ (called the *learning rate*) and a small constant $\\epsilon$ (the *tolerance*).  Then iteratively compute\n$$\nc^{(n+1)}=c^{(n)} -\\nu\\nabla f(c^{(n)})\n$$\nuntil $|c^{(n+1)}-c^{(n)}|<\\epsilon$.  Then $c^{(n+1)}$ is an (approximate) critical point of $f$.\n\n![Gradient Descent](img/GradientDescentIllustration.png){#fig-graddescentillust width=50%}\n\nThe behavior of gradient descent is illustrated in @fig-graddescentillust  for the function\n$$\nf(x,y) = \\frac{xy}{\\sigma\\sqrt{2\\pi}}e^{(-x^2-y^2)/2\\sigma^2}\n$$\nwhere $\\sigma=4$. This function has two \"upward\" humps and two \"downward\" humps.  Starting on the inside\nslope of one of the upward humps, gradient descent finds the bottom of an adjacent \"downward\" hump.\n\nTo get a little more perspective on gradient descent, consider the one-dimensional case, with $f(x)=4x^3-6x^2$.  This is a cubic polynomial whose graph has a local maximum and a local minimum, depicted in @fig-graddescentcubic.\n\n![A cubic polynomial](img/GradDescentCubic.png){#fig-graddescentcubic width=50%}\n\nIn this case the gradient is just the derivative $f'(x)=12x^2-12x$ and the iteration is\n$$\nc^{(n+1)} = c^{(n)}-12\\nu((c^{(n)})^2-c^{(n)}).\n$$\n\nEven from this simple example we can see the power and also the pitfalls of this method.  Suppose we choose\n$x_0=2$, $\\nu=.01$, and $\\epsilon=.001$.  Then the iteration yields:\n\n| Step | 0 | 1 | 2 | 3 | 4 | 5 | 6 | \n|---|---|---|---|---|---|---|---|\n|x|2.0| 0.8       | 0.896     | 0.952 | 0.979|  0.991| 0.997|\n\n: Gradient Descent Iterations {#tbl-graddescentiters}\n\nAs you can see, the points move quickly to the (local) minimum at $x=1$.    \n\nThere are two ways (at least) that things can go wrong, however.  First suppose we use $x_0=-1$, instead\nof $x_0=2$, as our first guess.  Then we are on the downslope on the left side of the graph, and following\nthe gradient quickly takes us off to $-\\infty$.\n\n\n| Step | 0 | 1 | 2 | 3 | 4 | 5 |\n|---|---|---|---|---|---|---|\n|x| -1.00 |-2.20| -6.42| -35.04| -792.70| -378296.27 |\n\n: Gradient Descent Iterations (first failure mode) {#tbl-graddescentfailone}\n\nSecond, suppose we choose $x_0=2$, but choose a somewhat larger learning rate -- say, $\\nu=.1$.\nIn this case, initially things look good, but the addition of the gradient causes an overshoot\nwhich once again takes us over the hump at $x=0$ and off to $-\\infty$ heading to the left.\n\n| Step | 0 | 1 | 2 | 3 | 4 | 5 |6|\n|---|---|---|---|---|---|---|---|\n|x|2.00|-0.11|-0.24|-0.56|-1.49|-5.42|-42.23|\n\n: Gradient Descent Iterations (second failure mode) {#tbl-graddescentfailetwo}\n\nBased on these considerations, we see that, for general functions, \n *if gradient descent converges,* then it will converge\nto a local minimum of the function.  But *it may not converge,* and even if it does, we can't conclude\nanything about whether we've reached a *global* minimum.  \n\n## Gradient Descent and Logistic Regression\n\nWe can use gradient descent to find the maximum likelihood set of parameters for our logistic model.\nAs we saw earlier, in @eq-logisticregressionlikelihood, we have the log likelihood function\n$$\nL(M) = Y\\cdot \\log\\sigma(XM) + (1-Y)\\cdot\\log(1-\\sigma(XM))\n$$\nwhere $Y$ are the target $0/1$ values, $X$ is our $N\\times (k+1)$ data matrix whose last column\nis all ones, and $M$ is the $k+1\\times 1$ column vector of unknown parameters.\nSince gradient descent is naturally a *minimizing* algorithm, we will minimize the function $-L(M)$.\n\nThe key piece of information that we need is the gradient $-\\nabla L$, where the variables are the entries\nof $M$.  The complicating features is the presence of the nonlinear function $\\sigma$, so let's start\nwith a simple observation about this function.\n\n**Lemma:** The logistic function $\\sigma(x)$ satisfies the differential equation\n$$\n\\frac{d\\sigma}{dx} = \\sigma(x)(1-\\sigma(x)).\n$$\n\n**Proof:** Since \n$$\n\\sigma(x)= \\frac{1}{1+e^{-x}},\n$$\n$$\n1-\\sigma(x) = \\frac{e^{-x}}{1+e^{-x}}.\n$$\nThen we calculate\n$$\n\\frac{d\\sigma}{dx}=\\left(\\frac{1}{(1+e^{-x})}\\right)^2e^{-x} = \\left(\\frac{1}{1+e^{-x}}\\right)\n                                                                  \\left(\\frac{e^{-x}}{1+e^{-x}}\\right)=\\sigma(x)(1-\\sigma(x))\n$$\nwhich is what we claimed.\n\nWe apply this differential equation to compute the gradient of $L$.\n\n**Proposition:** The gradient $-\\nabla L(M)$ is given by \n$$\n-\\nabla L(M) = (\\sigma(XM)-Y)^{\\intercal} X.\n$$\nNotice that the right side of this equation yields a $1\\times (k+1)$ row vector.  The entries of this\nvector are the partial derivatives with respect to the coefficients $m_{i}$ for $i=0,\\ldots, k$.\n\n**Proof:** This is yet another exercise in the chain rule and keeping track of indices.\nLet's first look at the term $Y\\cdot \\log\\sigma(XM)$.  Writing it out, we have\n$$\nY\\cdot \\log\\sigma(XM)=\\sum_{i=0}^{N-1}y_{i}\\log\\sigma(\\sum_{j=0}^{k}x_{ij}m_{j}).\n$$\nApplying $\\partial/\\partial m_{s}$ to this yields\n$$\n\\sum_{i=0}^{N-1}y_{i}(1-\\sigma(\\sum_{j=0}^{k}x_{ij}m_{j}))x_{is}\n$$\nwhere we've used the chain rule and the differential equation for $\\sigma$ discussed above. At the same\ntime, we can apply $\\partial/\\partial m_{s}$ to the second term $(1-Y)\\cdot\\log(1-\\sigma(XM))$\nand obtain\n$$\n-\\sum_{i=0}^{N-1}(1-y_{i})\\sigma(\\sum_{j=0}^{k}x_{ij}m_{j})x_{is}.\n$$\nThe term $\\sum_{i=0}^{N-1} y_{i}\\sigma(\\sum_{j=0}^{k}x_{ij}m_{j})x_{is}$ cancels, yielding\n$$\n\\frac{\\partial L(M)}{m_{s}} = -\\sum_{i=0}^{N-1} (y_{i}-\\sigma(\\sum_{j=0}^{k}x_{ij}m_{j}))x_{is}.\n$$  \nLooked at properly this is our desired formula:\n$$\n-\\nabla L(M) = (\\sigma(XM)-Y)^{\\intercal}X.\n$$\n\n### Gradient Descent on our synthetic data\n\nNow we can apply gradient descent to find a maximum likelihood logistic model \nfor the sample data that we generated from the logistic model and reported in @tbl-logistic_data.\nWith the probability given as\n$$\np(x) = \\frac{1}{1+e^{-ax-b}}\n$$\nwe make an initial guess of $a=1$ and $b=0$ set a learning rate $\\nu=.001$, and run the\ngradient descent algorithm for $30$ iterations.  We plot the negative log-likelihood \nfor this algorithm one the left in @fig-logisticloglike, where we see that it drops swiftly to a minimum value.\nThe corresponding parameter values are $a=.6717$ and $b=-.0076$, and the fit of the the corresponding\nlogistic curve to the observed data is shown on the right in @fig-logisticloglike.\n\n![Max Likelihood Gradient Descent for Logistic Fitting](img/LogisticLogLikelihoodAndFit.png){#fig-logisticloglike width=100%}\n\n\nThe parameters used to generate the data are close to this; they were $a=log(2)=$.6931$ and $b=0$.\n\n### Gradient Descent and Logistic Regression on \"real\" data\n\nWe conclude this first look at logistic regression and gradient descent by analyzing some simple real data.\nThis dataset consists of about $2200$ customers who patronize a certain food store.  Among the features in the data set is a field giving the total dollars spent at the store by a customer; we will study that\nfeature and its relationship to the question of whether or not the customer accepted a special offer from the store. (see @KaggleFoodData for the original data source).\n\n![Food Marketing Data: Histograms of Expenditures and Response](img/FoodDataPlot.png){#fig-fooddataplot width=100%}\n\nThe two plots in @fig-fooddataplot summarize the data.  The first plot is a histogram showing the amounts\nspent by the customers; the second shows the distribution of responses.   \n\nWe would like to know how expenditures increase the likelihood of customers accepting our offer.\nWe therefore fit a logistic model to the data.  The result is shown in @fig-foodlogisticfit.\n\n![Logistic Model for Food Marketing](img/FoodLogisticFit.png){#fig-foodlogisticfit width=50%}\n\n## Logistic Regression and classification\n\nBeyond the kind of probability prediction that we have discussed up to this point, logistic regression is one of the most powerful\ntechniques for attacking the classification problem.  Let's start our discussion with a sample problem that is a simplified version of one\nof the most famous machine learning benchmark problems, the MNIST (Modified National Institute of Science\nand Technology) dataset of handwritten numerals.  This dataset consists of $60000$ labelled grayscale \nimages of handwritten digits from $0$ to $9$.  Each image is stored as a $28x28$ array of integers from $0$ to $255$.  Each\ncell of the array corresponds to a \"pixel\" in the image, and the contents of that cell is a grayscale value.  See @MNISTDatabase\nfor the a more detailed description of how the dataset was constructed.\n\nIn @fig-MNISTOne is a picture of a handwritten \"1\" from the MNIST dataset.  \n\n![Handwritten One from MNIST](img/MNISTOne.png){#fig-MNISTOne width=100%}\n\n**Classification Problem for MNIST:** Given a $28x28$ array of grayscale values, determine which digit is represented. \n\nAt first glance, this does not look like a logistic regression problem.  To make the connection clearer, let's simplify the problem\nand imagine that our database contains only labelled images of zeros and ones -- we'll worry about how to handle the full problem later.\nSo now our task is to determine which images are zeros, and which are ones.  \n\nOur approach will be to view each image as a vector of length $784=28*28$ by stringing the pixel values row by row into a one dimensional vector,\nwhich following our conventions yields a matrix of size $N\\times 784$ where $N$ is the number of images.   Since we may also need an \"intercept\",\nwe add a column of $1$'s to our images yielding a data matrix $X$ of size $N\\times 785$. The labels $y$ form a column vector \nof size $N$ containing zeros and ones.\n\nWe will also simplify the data but converting the gray-scale images to monochrome by converting gray levels\nup to $128$ as \"white\" and beyond $128$ as \"black\".\n\nThe logistic regression approach asks us to find the \"best\" vector $M$ so that, for a given image vector $x$ (extended by adding a one at the end), the function\n$$\np(x)=\\frac{1}{1+e^{-xM}}\n$$ \nis close to $1$ if $x$ represents a one, and is close to zero if $x$ represents zero.  Essentially we think of $p(x)$ as giving the probability that the vector $x$\nrepresents an image of a one.  If we want a definite choice, then we can set a threshold value $p_0$ and say that the image $x$ is a one if $p(x)>p_0$ and\nzero otherwise.  The natural choice of $p_0=.5$ amounts to saying that we choose the more likely of the two options under the model.\n\n Since we are applying the logistic model we are assuming:\n\n-  that the value of each pixel in the image contributes something towards the chance of the total\nimage being one;\n-  and the different pixels have independent, and additive effects on the odds of getting a one. \n\nIf we take this point of view, then we can ask for the vector\n$M$ that is *most likely* to account for the labellings, and we can use our maximum likelihood gradient descent method to find $M$.\n\nThis approach is surprisingly effective.  With the MNIST zeros and ones, and the gradient descent method discussed above,\none can easily find $M$ so that the logistic model predicts the correct classification with accuracy in the high 90% range. \n\n### Weights as filters\n\nOne interesting aspect of using logistic regression on images for classification is that the we can interpret the optimum set of coefficients $M$\nas a kind of filter for our images.  Remember that $M$ is a vector with $785$ entries, the last of which is an \"intercept\".  \nThe logistic model\nsays that, for an image vector $x$, the log-odds that the image is a one is\ngiven by\n$$\n\\log \\frac{p}{1-p} = \\sum_{i=0}^{783} M_{i}x_{i} + M_{784}.\n$$\nThis means that if the value of $M_{i}$ is positive, then large values in the $i^{th}$ pixel *increase* the chance that our image is a one;\nwhile if $M_{i}$ is negative, large values *decrease* the chance.  If $M_{i}$ is negative, the reverse is true.  However, the values $x_{i}$ are\nthe gray scale \"darkness\" of the image, so the entries of $M$ emphasize or de-emphasize dark pixels according to whether that dark pixel is more or less\nlikely to occur in a one compared to a zero. \n\nThis observation allows us to interpret the weights $M$ as a kind of \"filter\" for the image.  In fact, if we rescale the entries of $M$ (omitting the intercept) so that they lie between $0$ and $255$,\nwe can arrange them as a $28\\times 28$ array and plot them as an image.  The result of doing this for a selection of MNIST zeros and ones is shown on the left in @fig-weights.  The red (or positive) weights in the middle of the image tell us that if those pixels are dark, the image is more likely to be a one;\nthe blue (or negative) weights scattered farther out tell us that if those pixels are dark, the image is more likely to be a zero. \n\n![Rescaled weights (blue is negative). Top: 0 vs 1. Bottom: 3 vs 8.](img/weights.png){#fig-weights width=100%}\n\nWhat's important to notice here is that we did not design this \"filter\" by hand, based on our understanding of the differences between a handwritten zero and one; instead, the algorithm \"learned\"\nthe \"best\" filter to optimize its ability to distinguish these digits.\n\nHere's another example.  Suppose we redo the MNIST problem above, but we try to distinguish 3's from 8's.  \nWe have about 4500 of each digit, and we label the 3's with zero and the 8's with one.  Then we use our\nmaximum likelihood optimization.  In this case, the filter is shown on the bottom in @fig-weights.\n\n## Multiclass Logistic Regression\n\nOne could attack the problem of classifying the ten distinct classes of digits by,  for example,\nlabelling all of the zeros as class zero and everything else as class one, and finding a set of weights\nthat distinguishes zero from everything else.  Then, in turn, one could do the same for each of the other digits.\nGiven an unknown image, this would yield a set of probabilities from which one could choose the most likely\nclass.  This type of classification is called \"one vs rest\", for obvious reasons.  It seems more natural,\nhowever, to construct a model that, given an image, assigns a probability that it belongs to each of the\ndifferent possibilities.  It is this type of multiclass logistic regression that we will study now.\n\nOur goal is to build a model that, given an unknown image, returns a vector of ten probabilities,\neach of which we can interpret as the chance that our unknown image is in fact of a particular digit.\nIf we *know* the image's class, then it's probability vector *should* be nine zeros with a single one\nin the position corresponding to the digit.  So, for example, if our image is of a two, then\nthe vector of probabilities\n\n$$\n\\left[ \\begin{matrix} p_0 & p_1 & p_2 &\\cdots & p_8 & p_9\\\\\\end{matrix}\\right]=\\left[\\begin{matrix} 0 &0 & 1 & \\cdots & 0 & 0\\\\\\end{matrix}\\right]\n$$\n\nwhere $p_i$ is the probability that our image is the digit $i$. Notice also that the probabilities $p_i$ must sum to one.  We encode the class membership of our samples by constructing an $N\\times r$ matrix $Y$,\neach row of which has a one in column $j$ if that sample belongs to class $j$, and zeros elsewhere.\nThis type of representation is sometimes called \"one-hot\" encoding. \n\nSo let's  assume we have $N$ data points, each with $k$ features, and a one-hot encoded, $N\\times r$\nmatrix of labels $Y$ encoding the data into $r$ classes.\nSo our data matrix will be $N\\times k$.   \nOur goal will be to find a $k\\times r$ matrix of \"weights\" $M$ so that, for each sample, we compute\n$r$ values, given by the rows of the matrix $NM$.\nThese $r$ values are linear functions of the features, but we need probabilities.  In the one-dimensional\ncase, we used the logistic functions $\\sigma$ to convert our linear function to probabilities.  In\nthis higher dimensional case we use a generalization of $\\sigma$ called the \"softmax\" function.\n\n**Definition:** Let $F:\\mathbf{R}^r\\to\\mathbf{R}^{r}$ be the function\n$$\nF(z_1,\\ldots, z_r) = \\sum_{j=1}^{r} e^{z_{i}}\n$$\nand let $\\sigma:\\mathbf{R}^{r}\\to \\mathbf{R}^{r}$ be the function\n$$\n\\sigma(z_1,\\ldots, z_n) = \\left[\\begin{matrix} \\frac{e^{z_1}}{F} & \\cdots & \\frac{e^{z_{r}}}{F}\\end{matrix}\\right].\n$$\nNotice that the coordinates of the vector $\\sigma(z_1,\\ldots,z_n)$ are all between $0$ and $1$, and their\nsum is one.\n\nOur multiclass logistic model will say that the probability vector that gives the probabilities\nthat a particular sample belongs to a particular class is given by the rows of the matrix\n$\\sigma(XM)$, where $\\sigma(XM)$ means applying the function $\\sigma$ to each row of the $N\\times r$\nmatrix $XM$.\n\n### Multiclass logistic regression - the likelihood\n\nThe rows of the matrix $\\sigma(XM)$ are the vectors of probabilities summing to one which are supposed\nto be the chances that the corresponding sample belongs to each of the possible classes.  If the\n$j^{th}$ sample belongs to class $i$, then the probability of this outcome assigned by our model\nis $p_{ji}$.  Recall that we have captured the class membership of the samples in a $k\\times r$ matrix $Y$\nwhich is \"one-hot\" encoded.  So we can represent the chance that sample $j$ belongs to class $i$\nas\n$$\nP(\\hbox{ sample i in class j})=\\prod_{i=1}^{r} p_{ji}^{y_{ji}}.\n$$\nTaking the logarithm, we find\n$$\n\\log P = y_{ji}\\sum_{i=1}^{r} \\log p_{ji} = \\sum_{i=1}^{r} y_{ji} \\log \\sigma(XM[j,:])[i].\n$$\nSince each sample is independent, the total likelihood is the product of these probabilites, and\nthe log-likelihood the corresponding sum.\n$$\n\\log L(M) = \\sum_{j=1}^{N} \\sum_{i=1}^{r}y_{ji}\\log\\sigma(XM[j,:])[i]\n$$\nor, using matrix notation,\n$$\n\\log L(M) = \\mathrm{trace}(Y^{\\intercal}\\log\\sigma(XM))\n$$\nwhere trace of the $r\\times r$ matrix $Y^{\\intercal}\\log\\sigma(XM)$ is the sum\nof its diagonal elements.\n\nThis is the multiclass generalization of @eq-logisticregressionlikelihood.  To see the connection,\nnotice that, in the case where we have only two\nclasses, the two columns of $Y$ sum to one, as do the two probabilities in $\\sigma(XM)$.\n\n\n### Multiclass logistic regression - the gradient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"05-logistic-regression.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":[{"text":"\\usepackage{listings}\n"},{"file":"../chapters/macros.tex"}],"output-file":"05-logistic-regression.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","documentclass":"scrreprt"},"extensions":{"book":{}}}}}