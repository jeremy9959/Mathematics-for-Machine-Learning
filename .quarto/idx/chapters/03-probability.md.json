{"title":"Probability and Bayes Theorem","markdown":{"headingText":"Probability and Bayes Theorem","containsRefs":false,"markdown":"\n\n## Introduction\n\nProbability theory is one of the three central mathematical tools in machine learning, along with\nmultivariable calculus and linear algebra.  Tools from probability allow us to manage the uncertainty\ninherent in data collected from real world experiments, and to measure the reliability of predictions\nthat we might make from that data.  In these notes, we will review some of the basic terminology\nof probability and introduce Bayesian inference as a technique in machine learning problems.\n\nThis will only be a superficial introduction to ideas from probability.  For a thorough treatment,\nsee [this open-source introduction to probability.](https://probability.oer.math.uconn.edu/3160-oer)\nFor a  more applied emphasis, I recommend the excellent online course\n[Probabilistic Systems Analysis and Applied Probability](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/) and its associated\ntext @Bertsekas.\n\n## Probability Basics\n\nThe theory of probability begins with a set $X$ of possible events or outcomes, together with a\n\"probability\" function $P$ on (certain)\nsubsets of $X$ that measures \"how likely\" that combination of events is to occur.\n\nThe set $X$ can be discrete or continuous. For example,\nwhen flipping a coin, our set of possible events would be the discrete set $\\{H,T\\}$ corresponding to the\npossible events of flipping heads or tails.  When measuring the temperature using a thermometer,\nour set of possible outcomes might be the set of real numbers, or perhaps an interval in $\\mathbb{R}$.\nThe thermometer's measurement is random because it\nis affected by, say, electronic noise, and so its reading is the true temperature perturbed by a random amount.\n\nThe values of $P$ are between $0$, meaning that the event *will not* happen, and $1$, meaning that it is\ncertain to occur.  As part of our set up, we assume that the total chance of some event from $X$ occurring\nis $1$, so that $P(X)=1$; and the chance of \"nothing\" happening is zero, so $P(\\emptyset)=0$.\nAnd if $U\\subset X$ is some collection, then $P(U)$ is the chance of an event from $U$ occurring.\n\nThe last ingredient of this picture of probability is additivity.  Namely, we assume that\nif $U$ and $V$ are subsets of $X$ that are disjoint, then\n$$\nP(U\\cup V)=P(U)+P(V).\n$$\nEven more generally, we assume that this holds for (countably) infinite collections of disjoint subsets\n$U_1,U_2,\\ldots$, where\n$$\nP(U_1\\cup U_2\\cup\\cdots)=\\sum_{i=1}^{\\infty} P(U_i)\n$$\n\n**Definition:** The combination of a set $X$ of possible outcomes and a probability function $P$\non subsets of $X$ that satisfies $P(X)=1$, $0\\le P(U)\\le 1$ for all $U$, and is additive on\ncountable disjoint collections of subsets of $X$ is called a (naive) probability space.  $X$ is called\nthe *sample space* and the subsets of $X$ are called *events*.\n\n**Warning:** The reason for the term \"naive\" in the above definition is that, if $X$ is an uncountable\nset such as the real numbers $\\mathbb{R}$, then the conditions in the definition are self-contradictory.\nThis is a deep and rather surprising fact. To make a sensible definition of a probability space,\none has to restrict the domain of the probability function $P$ to certain subsets of $X$.  These ideas\nform the basis of the mathematical subject known as measure theory.  In these notes we will work\nwith explicit probability functions and simple subsets such as intervals that avoid these technicalities.\n\n### Discrete probability examples\n\nThe simplest probability space arises in the analysis of coin-flipping.\nAs mentioned earlier, the set $X$ contains two elements $\\{H,T\\}$.  The probability function\n$P$ is determined by its value $P(\\{H\\})=p$,\nwhere $0\\le p\\le 1$, which is the chance of the coin yielding a \"head\".  Since $P(X)=1$, we have\n$P(\\{T\\})=1-p$.\n\nOther examples of discrete probability spaces arise from dice-rolling and playing cards.  For\nexample, suppose we roll two six-sided dice.  There are $36$ possible outcomes from\nthis experiment, each equally likely.  If instead we consider the sum of the two values on the\ndice, our outcomes range from $2$ to $12$ and the probabilities of these outcomes\nare given by\n\n|2  |3  |4  |5  |6  |7  |8  |9  |10 |11 |12 |\n|---|---|---|---|---|---|---|---|---|---|---|\n|1/36|1/18|1/12|1/9|5/36|1/6|5/36|1/9|1/12|1/18|1/36|\n\nA traditional deck of $52$ playing cards contains $4$ aces.  Assuming that the chance\nof drawing any card is the same (and is therefore equal to $1/52$), the probability of drawing an ace\nis $4/52=1/13$ since\n$$\nP(\\{A_{\\clubsuit},A_{\\spadesuit},A_{\\heartsuit},A_{\\diamondsuit}\\}) = 4P(\\{A_{\\clubsuit}\\})=4/52=1/13\n$$\n\n### Continuous probability examples\n\nWhen the set $X$ is continuous, such as in the temperature measurement, we measure $P(U)$,\nwhere $U\\subset X$, by giving a \"probability density function\" $f:X\\to \\mathbb{R}$\nand declaring that\n$$\nP(U) = \\int_{U}f(x) dX.\n$$\nNotice that our function $f(x)$ has to satisfy the condition\n$$\nP(X)=\\int_{X} f(x)dX = 1.\n$$\n\nFor example, in our temperature measurement example, suppose the \"true\" outside temperature is $t_0$,\nand our thermometer gives a reading $t$. Then a good model for the random error is to assume\nthat the error $x=t-t_0$ is governed by the density function\n$$\nf_\\sigma(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-x^2/2\\sigma^2}\n$$\nwhere $\\sigma$ is a parameter.   In a continuous situation such as this one, the probability\nof any particular outcome in $X$ is zero since\n$$\nP(\\{t\\})=\\int_{t}^{t}f_{\\sigma}(x)dx = 0\n$$\nStill, the shape of the density function does tell you where the values are concentrated -- values\nwhere the density function is larger are more likely than those where it is smaller.\n\nWith this density function, and x=$t-t_0$, the error in our measurement is given by\n$$\nP(|t-t_0|<\\delta)=\\int_{-\\delta}^{\\delta} \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-x^2/2\\sigma^2} dx\n$${#eq-normal}\n\nThe parameter $\\sigma$ (called the *standard deviation*) controls how tightly the thermometer's measurement\nis clustered around the true value $t_0$; when $\\sigma$ is large, the measurements are scattered widely,\nwhen small, they are clustered tightly.   See @fig-density.\n\n![Normal Density](img/density.png){#fig-density}\n\n## Conditional Probability and Bayes Theorem\n\nThe theory of conditional probability gives a way to study how partial information about an event\ninforms us about the event as a whole.  For example, suppose you draw a card at random from a deck.\nAs we've seen earlier, the chance that card is an ace is $1/13$.  Now suppose that you learn that\n(somehow) that the card is definitely not a jack, king, or queen.  Since there are 12 cards in the\ndeck that are jacks, kings, or queens, the card you've drawn is one of the remaining 40 cards,\nwhich includes 4 aces.  Thus the chance you are holding an ace is now $4/40=1/10$.  \n\nIn terms of notation, if $A$ is the event \"my card is an ace\" and $B$ is the event \"my card is not a jack,\nqueen, or king\" then we say that *the probability of $A$ given $B$* is $1/10$.  The notation\nfor this is\n$$\nP(A|B) = 1/10.\n$$\n\nMore generally, if $A$ and $B$ are events from a sample space $X$, and $P(B)>0$, then\n$$\nP(A|B) = \\frac{P(A\\cap B)}{P(B)},\n$$\nso that $P(A|B)$ measures the chance that $A$ occurs among those situations in which $B$ occurs.\n\n### Bayes Theorem\n\nBayes theorem is a foundational result in probability.  \n\n**Theorem:** Bayes Theorem says\n$$\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}.\n$$\n\nIf we use the definition of conditional probability given above, this is straightforward:\n$$\n\\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B\\cap A)}{P(B)} = P(A|B).\n$$\n\n### An example\n\nTo illustrate conditional probability, let's consider what happens when we administer\nthe most reliable COVID-19 test, the PCR test, to an individual drawn from the population\nat large.    There are two possible test results (positive and negative) and two possible\ntrue states of the person being tested (infected and not infected). Suppose I go to the doctor\nand get a COVID test which comes back positive.  What is the probability that I actually have\nCOVID?  \n\nLet's let $S$ and $W$ stand\nfor infected (sick)  and not infected (well), and let $+/-$ stand for test positive or negative.\nNote that there are four possible outcomes of our experiment:\n\n- test positive and infected (S+) -- this is a *true positive*.\n- test positive and not infected (W+) -- this is a *false positive*.\n- test negative and infected (S-) -- this is a *false negative*.\n- test negative and not infected (W-) -- this is a *true negative*.\n\nThe [CDC says](https://www.icd10monitor.com/false-positives-in-pcr-tests-for-covid-19) that the chance\nof a false positive\n-- that is, the percentage of samples from well people that incorrectly yields a positive result\n-- is about one-half of one percent, or 5 in 1000.\n\nIn other words,\n$$\nP(+|W) = P(W+)/P(W) = 5/1000=1/200\n$$\n\nOn the other hand, the CDC tells us that chance of a false negative is 1 in 4, so\n$$\nP(-|S) = P(S-)/P(S) = .25.\n$$\nSince $P(S-)+P(S+)=P(S).$ since every test is either positive or negative, we have\n$$\nP(+|S) = .75.\n$$\n\nSuppose furthermore that the overall incidence of COVID-19 in the population is p.  In other\nwords, $P(S)=p$ so $P(W)=1-p$.  Then\n$$P(S+)=P(S)P(+|S)=.75p$$\nand\n$$\nP(W+)=P(W)P(+|W)=.005(1-p).\n$$\nPutting these together we get $P(+)=.005+.745p$\n\nWhat I'm interested in is $P(S|+)$ -- the chance that I'm sick, given that my test result was positive.\nBy Bayes Theorem,\n$$\nP(S|+)=\\frac{P(+|S)P(S)}{P(+)}=.75p/(.005+.745p)=\\frac{750p}{5+745p}.\n$$\n\nAs @fig-covidfn shows, if the population incidence is low then a positive test\nis far from conclusive.  Indeed, if the overall incidence of COVID is one percent, then\na positive test result only implies a 60 percent chance that I am in fact infected.\n\nJust to fill out the picture, we have\n$$\nP(-) = P(S-)+P(W-)=(P(S)-P(S+))+(P(W)-P(W+))\n$$\nwhich yields\n$$\nP(-)=1-.005+.005p-.75p = .995-.745p.\n$$\nUsing Bayes Theorem, we obtain\n$$\nP(S|-) = \\frac{P(-|S)P(S)}{P(-)} = .25p/(.995-.745p) =\\frac{250p}{995-745p}.\n$$\nIn this case, even though the false negative rate is pretty high (25 percent) overall,\nif the population incidence is one percent, then the probability that you're sick given a negative result\nis only about $.25$ percent.  So negative results  are very likely correct!\n\n![P(S|+) vs P(S)](img/covidfn.png){#fig-covidfn}\n\n## Independence\n\nIndependence is one of the fundamental concepts in probability theory.  Conceptually, two events\nare independent if the occurrence of one has does not influence the likelihood of the occurrence of the\nother.  For example, successive flips of a coin are independent events, since the result of the second\nflip doesn't have anything to do with the result of the first.  On the other hand, whether or not it rains\ntoday and tomorrow are not independent events, since the weather tomorrow depends (in a complicated way)\non the weather today.\n\nWe can formalize this idea of independence using the following definition.\n\n**Definition:** Let $X$ be a sample space and let $A$ and $B$ be two events.  Then $A$ and $B$\nare *independent* if $P(A\\cap B)=P(A)P(B)$.  Equivalently, $A$ and $B$ are independent if\n$P(A|B)=P(A)$ and $P(B|A)=P(B)$.  \n\n### Examples\n\n#### Coin Flipping\n\nSuppose our coin has a probability of heads given by a real number $p$ between $0$ and $1$, and we flip\nour coin $N$ times.  What is the chance of gettting $k$ heads, where $0\\le k\\le N$?  Any particular\nsequence of heads and tails containing $k$ heads and $N-k$ tails has probability\n$$\nP(\\hbox{a particular sequence of $k$ heads among $N$ flips}) = p^{k}(1-p)^{N-k}.\n$$\nIn addition, there are $\\binom{N}{k}$ sequences of heads and tails containing $k$ heads.  Thus\nthe probability $P(k,N)$ of $k$ heads among $N$ flips is\n$$\nP(k,N) = \\binom{N}{k}p^{k}(1-p)^{N-k}.\n$${#eq-binomial}\n\nNotice that the binomial theorem gives us $\\sum_{k=0}^{N} P(k,N) =1$ which is a reassuring check on our work.\n\nThe probability distribution on the set $X=\\{0,1,\\ldots,N\\}$ given by $P(k,N)$ is called the *binomial distribution*\nwith parameters $N$ and $p$.\n\n#### A simple 'mixture'\n\nNow let's look at an example of events that are not independent.  Suppose that we have two coins,\nwith probabilities of heads $p_1$ and $p_2$ respectively; and assume these probabilities\nare different.  We play the a game in which we first\nchoose one of the two coins (with equal chance) and then flip it twice.  Is the result of the\nsecond flip independent of the first? In other words, is $P(HH)=P(H)^2$?\n\nThis type of situation is called a 'mixture distribution' because the probability of a head is a \"mixture\"\nof the probability coming from the two different coins.\n\nThe chance that the first flip is a head is $(p_1+p_2)/2$ because it's the chance of picking the first\ncoin, and then getting a head, plus the chance of picking the second, and then getting a head.\nThe chance of getting two heads in a row is $(p_1^2+p_2^2)/2$ because it's the chance, having picked\nthe first coin, of getting two heads, plus the chance, having picked the second, of getting two heads.\n\nSince\n$$\n\\frac{p_1^2+p_2^2}{2}\\not=\\left(\\frac{p_1+p_2}{2}\\right)^2\n$$\nwe see these events are not independent.\n\nIn terms of conditional probabilities, the chance that the second flip is a head, given that\nthe first flip is, is computed as:\n$$\nP(HH|H) = \\frac{p_1^2+p_2^2}{p_1+p_2}.\n$$\nFrom the Cauchy-Schwartz inequality one can show that\n$$\n\\frac{p_1^2+p_2^2}{p_1+p_2}>\\frac{p_1+p_2}{2}.\n$$\n\nWhy should this be?  Why should the chance of getting a head on the second flip go up given that the\nfirst flip was a head? One way to think of this is that the first coin flip contains a little bit of\ninformation about which coin we chose.  If, for example $p_1>p_2$, and our first flip is heads, then\nit's just a bit more likely that we chose the first coin.  As a result, the chance of getting another\nhead is just a bit more likely than if we didn't have that information.  We can make this precise\nby considering the conditional probability $P(p=p_1|H)$ that we've chosen the first coin given\nthat we flipped a head.  From Bayes' theorem:\n\n$$\nP(p=p_1|H) = \\frac{P(H|p=p_1)P(p=p_1)}{P(H)}=\\frac{p_1}{p_1+p_2}=\\frac{1}{1+(p_2/p_1)}>\\frac{1}{2}\n$$\nsince $(1+(p_2/p_1))<2$.\n\n**Exercise:** Push this argument a bit further.  Let $p_1=\\max(p_1,p_2)$\nLet $P_N$ be the conditional probability of getting\nheads assuming that the first $N$ flips were heads.  Show that $P_N\\to p_1$ as $N\\to\\infty$.\nAll those heads piling up make it more and more likely that you're flipping the first coin and\nso the chance of getting heads approaches $p_1$.\n\n#### An example with a continuous distribution\n\nSuppose that we return to our example of a thermometer which measures the ambient temperature\nwith an error that is distributed according to the normal distribution, as in @eq-normal.\nSuppose that we make 10 independent measurements $t_1,\\ldots, t_{10}$ of the true temperature $t_0$.\nWhat can we say about the\ndistribution of these measurements?\n\nIn this case, independence means that\n$$\nP=P(|t_1-t_0|<\\delta,|t_2-t_0|<\\delta,\\ldots) = P(|t_1-t_0|<\\delta)P(|t_2-t_0|<\\delta)\\cdots P(|t_{10}-t_{0}|<\\delta)\n$$\nand therefore\n$$\nP = \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^{10}\\int_{-\\delta}^{\\delta}\\cdots\\int_{-\\delta}^{\\delta}\ne^{-(\\sum_{i=1}^{10} x_i^2)/2\\sigma^2} dx_1\\cdots dx_{10}\n$$\n\nOne way to look at this is that the vector $\\mathbf{e}$ of errors $(|t_1-t_0|,\\ldots,|t_{10}-t_0|)$\nis distributed according to a *multivariate gaussian distribution*:\n$$\nP(\\mathbf{e}\\in U) =\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^{10}\\int_{U}\ne^{-\\|x\\|^2/2\\sigma^2} d\\mathbf{x}\n$${#eq-multivariategaussian}\n\nwhere $U$ is a region in $\\mathbf{R}^{10}$.\n\nThe multivariate gaussian can also describe situations where independence does not hold.  For simplicity,\nlet's work in two dimensions and consider the probability density on $\\mathbf{R}^{2}$ given by\n$$\nP(\\mathbf{e}\\in U) = A\\int_{U} e^{-(x_1^2-x_1x_2+x_2^2)/2\\sigma^2} d\\mathbf{x}.\n$$\nwhere the constant $A$ is chosen so that\n$$\nA\\int_{\\mathbf{R}^{2}}e^{-(x_1^2-x_1x_2+x_2^2)/2\\sigma^2}d\\mathbf{x} = 1.\n$$\n\nThis density function as a \"bump\" concentrated near the origin in $\\mathbf{R}^{2}$, and its level curves\nare a family of ellipses centered at the origin.  See @fig-multivariate for a plot of this function\nwith $\\sigma=1$.\n\n![Multivariate Gaussian](img/ellipse.png){#fig-multivariate}\n\nIn this situation we can look at the conditional probability of the first variable given the second,\nand see that the two variables are not independent.  Indeed, if we fix  $x_2$, then\nthe distribution of $x_1$ depends on our choice of $x_2$.  We could see this by a calculation,\nor we can just look at the graph: if $x_2=0$, then the most likely values of $x_1$ cluster near zero,\nwhile if $x_2=1$, then the most likely values of $x_1$ cluster somewhere above zero.  \n\n## Random Variables, Mean, and Variance\n\nTypically, when we are studying a random process, we aren't necessarily accessing the underlying events,\nbut rather we are making measurements that provide us with some information about the underlying events.\nFor example, suppose our sample space $X$ is the set of throws of a pair of dice, so $X$ contains\nthe $36$ possible combinations that can arise from the throws.  What we are actually interested is\nthe sum of the values of the two dice -- that's our \"measurement\" of this system.  This rather\nvague notion of a measurement of a random system is captured by the very general\nidea of a *random variable*.\n\n**Definition:** Let $X$ be a sample space with probability function $P$.  A *random variable* on $X$\nis a function $f:X\\to \\mathbb{R}$.\n\nGiven a random variable $f$, we can use the probability measure to decide how likely $f$ is to take\na particular value, or values in a particular set by the formula\n$$\nP(f(x)\\in U) = P(f^{-1}(U))\n$$\n\nIn the dice rolling example, the random variable $S$\nthat assigns their sum to the pair of values obtained on two dice is a random variable.\nThose values lie between $2$ and $12$ and we have\n$$\nP(S=k) = P(S^{-1}(\\{k\\}))=P(\\{(x,y): x+y=k\\})\n$$\nwhere $(x,y)$ runs through $\\{1,2,\\ldots,6\\}^{2}$ representing the two values and $P((x,y))=1/36$\nsince all throws are equally likely.\n\nLet's look at a few more examples, starting with what is probably the most fundamental of all.\n\n**Definition:** Let $X$ be a sample space with two elements, say $H$ and $T$, and suppose that\n$P(H)=p$ for some $0\\le p\\le 1$.  Then the random variable that satisfies $f(H)=1$ and $f(T)=0$\nis called a Bernoulli random variable with parameter $p$.\n\nIn other words, a Bernoulli random variable gives the value $1$ when a coin flip is heads, and $0$ for tails.\n\nNow let's look at what we earlier called the binomial distribution.\n\n**Definition:** Let $X$ be a sample space consisting of strings of $H$ and $T$ of length $N$,\nwith the probability of a *particular string* $S$ with $k$ heads and $N-k$ tails given by\n$$\nP(S)=p^{k}(1-p)^{N-k}\n$$\nfor some $0\\le p\\le 1$.  In other words, $X$ is the sample space consisting of $N$ independent flips\nof a coin with probability of heads given by $p$.  \n\nLet $f:X\\to \\mathbb{R}$ be the function which counts the number of $H$\nin the string.  We can express $f$ in terms of Bernoulli random variables;\nindeed,\n$$\nf=X_1+\\ldots+X_N\n$$\nwhere each $X_i$ is a Bernoulli random variable with parameter $p$.  \n\nNow\n$$\nP(f=k) = \\binom{N}{k}p^{k}(1-p)^{N-k}\n$$\nsince $f^{-1}(\\{k\\})$ is the number of elements in\nthe subset of strings of $H$ and $T$ of length $N$ containing exactly $k$ $H$'s.\nThis is our old friend the binomial distribution.  So *a binomial distribution is the distribution\nof the sum of $N$ independent Bernoulli random variables.*\n\nFor an example with a continuous random variable, suppose our sample space is $\\mathbf{R}^{2}$\nand the probability density is the simple multivariate normal\n$$\nP(\\mathbf{x}\\in U) = \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2\\int_{U} e^{-\\|\\mathbf{x}\\|^2/2} d\\mathbf{x}.\n$$\nLet $f$ be the random variable $f(\\mathbf{x})=\\|\\mathbf{x}\\|$.  The function $f$ measures the Euclidean distance\nof a randomly drawn point from the origin.    The set $$U=f^{-1}([0,r))\\subseteq\\mathbf{R}^{2}$$ is the circle\nof radius $r$ in $\\mathbf{R}^{2}$.  The probability that a randomly drawn point lies in this circle\nis\n$$\nP(f<r) = \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2\\int_{U} e^{-\\|\\mathbf{x}\\|^2/2} d\\mathbf{x}.\n$$\n\nWe can actually evaluate this integral in closed form by using polar coordinates.  We obtain\n$$\nP(f<r) = \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2\\int_{\\theta=0}^{2\\pi}\\int_{\\rho=0}^{r} e^{-\\rho^2/2}\\rho d\\rho d\\theta.\n$$\nSince\n$$\n\\frac{d}{d\\rho}e^{-\\rho^2/2}=-\\rho e^{-\\rho^2/2}\n$$\nwe have\n\\begin{align*}\nP(f<r)&=-\\frac{1}{2\\pi}\\theta e^{-\\rho^2/2}|_{\\theta=0}^{2\\pi}|_{\\rho=0}^{r}\\cr\n&=1-e^{-r^2/2}\\cr\n\\end{align*}\n\nThe probability density associated with this random variable is the derivative of $1-e^{-r^2/2}$\n$$\nP(f\\in [a,b])=\\int_{r=a}^{b} re^{-r^2/2} dr\n$$\nas you can see by the fundamental theorem of calculus.  This density is drawn in @fig-maxwell\nwhere you can see that the points are clustered at a distance of $1$ from the origin.\n\n![Density of the Norm](img/maxwell.png){#fig-maxwell}\n\n### Independence and Random Variables\n\nWe can extend the notion of independence from events to random variables.\n\n**Definition:** Let $f$ and $g$ be two random variables on a sample space $X$ with probability $P$.\nThen $f$ and $g$ are independent if, for all intervals $U$ and $V$ in $\\mathbb{R}$, the\nevents $f^{-1}(U)$ and $g^{-1}(V)$ are independent.\n\nFor discrete probability distributions, this means that, for all $a,b\\in\\mathbb{R}$,\n$$\nP(f=a\\hbox{\\ and\\ }g=b)=P(f=a)P(g=b).\n$$\n\nFor continous probability distributions given by a density function $P(x)$, independence can\nbe more complicated to figure out.\n\n### Expectation, Mean and Variance\n\nThe most fundamental tool in the study of random variables is the concept of \"expectation\",\nwhich is a fancy version of average.  The word \"mean\" is a synonym for expectation -- the mean\nof a random variable is the same as its expectation or \"expected value.\"\n\n**Definition:** Let $X$ be a sample space with probability measure $P$.  Let $f:X\\to \\mathbb{R}$\nbe a random variable.  Then the *expectation* or *expected value* $E[f]$ of $f$ is\n$$\nE[f] = \\int_X f(x)dP.\n$$\nMore specifically, if $X$ is discrete, then\n$$\nE[f] = \\sum_{x\\in X} f(x)P(x)\n$$\nwhile if $X$ is continuous with probability density function $p(x)dx$ then\n$$\nE[f] = \\int_{X} f(x)p(x)dx.\n$$\n\nIf $f$ is a Bernoulli random variable with parameter $p$, then\n$$\nE[f] = 1\\cdot p+0\\cdot (1-p) = p\n$$\n\nIf $f$ is a binomial random variable with parameters $p$ and $N$, then\n$$\nE[f] = \\sum_{i=0}^{N} i\\binom{N}{i}p^{i}(1-p)^{N-i}\n$$\nOne can evaluate this using some combinatorial tricks, but it's easier to apply this basic\nfact about expectations.\n\n**Proposition:** Expectation is linear: $E[aX+bY]=aE[X]+bE[Y]$ for random variables $X,Y$\nand constants $a$ and $b$.\n\nThe proof is an easy consequence of the expression of $E$ as a sum (or integral).\n\nSince a binomial random variable $Z$ with parameters $N$ and $p$ is the sum of $N$ Bernoulli random\nvariables, its expectation is\n$$\nE[X_1+\\cdots+X_N]=Np.\n$$\n\nA more sophisticated property of expectation is that it is multiplicative when the random variables\nare independent.\n\n**Proposition:** Let $f$ and $g$ be two independent random variables.  Then $E[fg]=E[f]E[g]$.\n\n**Proof:** Let's suppose that the sample space $X$ is discrete.  By definition,\n$$\nE[f]=\\sum_{x\\in X}f(x)P(x)\n$$\nand we can rewrite this as\n$$\nE[f]=\\sum_{a\\in\\mathbf{R}} aP(\\{x: f(x)=a\\}).\n$$\nLet $Z\\subset\\mathbb{R}$ be the range of $f$.\nThen\n\\begin{align*}\nE[fg]&=\\sum_{a\\in Z} aP(\\{x: fg(x)=a\\}) \\\\\n&=\\sum_{a\\in Z}\\sum_{(u,v)\\in\\genfrac{}{}{0pt}{}{\\mathbf{Z}^{2}}{uv=a}}aP(\\{x:f(x)=u\\hbox{\\ and\\ }g(x)=v\\}) \\\\\n&=\\sum_{a\\in Z}\\sum_{\\genfrac{}{}{0pt}{}{\\mathbf{Z}^{2}}{uv=a}}uvP(\\{x:f(x)=u\\})P(\\{x:g(x)=v\\}) \\\\\n&=\\sum_{u\\in Z}uP(\\{x:f(x)=u\\})\\sum_{v\\in Z}vP(\\{x:f(x)=v\\}) \\\\\n&=E[f]E[g]\n\\end{align*}\n\n#### Variance\n\nThe variance  of a random variable is a measure of its dispersion around its mean.\n\n**Definition:** Let $f$ be a random variable.  Then the variance is the expression\n$$\n\\sigma^2(f) = E[(f-E[f])^2]=E[f^2]-(E[f])^2\n$$\nThe square root of the variance is called the \"standard deviation.\"  \n\nThe two formulae for the variance arise from the calculation\n$$\nE[(f-E[f])^2]=E[(f^2-2fE[f]+E[f]^2)]=E[f^2]-2E[f]^2+E[f]^2=E[f^2]-E[f]^2.\n$$\n\nTo compute the variance of the Bernoulli random variable $f$ with parameter $p$, we first\ncompute\n$$\nE[f^2]=p(1)^2+(1-p)0^2=p.\n$$\nSince $E[f]=p$, we have\n$$\n\\sigma^2(f)=p-p^2=p(1-p).\n$$\n\nIf $f$ is the binomial random variable with parameters $N$ and $p$, we can again use the fact\nthat $f$ is the sum of $N$ Bernoulli random variables $X_1+\\cdots+X_n$ and compute\n\n\\begin{align*}\nE[(\\sum_{i}X_i)^2]-E[\\sum_{i} X_{i}]^2 &=E[\\sum_{i} X_i^2+\\sum_{i,j}X_{i}X_{j}]-N^2p^2\\\\\n&=Np+N(N-1)p^2-N^2p^2 \\\\\n&=Np(1-p)\n\\end{align*}\n\nwhere we have used the fact that the square $X^2$ of a Bernoulli random variable is equal to $X$.\n\nFor a continuous example, suppose that we consider a sample space $\\mathbb{R}$ with the normal\nprobability density\n$$\nP(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-x^2/2\\sigma^2}dx.\n$$\n\nThe mean of the random variable $x$ is\n$$\nE[x] =\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} xe^{-x^2/2\\sigma^2}dx=0\n$$\n\nsince the function being integrated is odd.  The variance is\n\n$$\nE[x^2] = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} x^2e^{-x^2/2\\sigma^2}dx.\n$$\n\nThe trick to evaluating this integral is to consider the derivative:\n\n$$\n\\frac{d}{d\\sigma}\\left[\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-x^2/(2\\sigma^2)}dx\\right]=0\n$$\n\nwhere the result is zero since the quantity being differentiated is a constant (namely $1$).  Sorting\nthrough the resulting equation leads to the fact that\n\n$$\nE[x^2]=\\sigma^2\n$$\n\nso that the $\\sigma^2$ parameter in the normal distribution really *is* the variance of the associated\nrandom variable.\n\n## Models and Likelihood\n\nA *statistical model* is a mathematical model that accounts for data via a process that\nincorporates random behavior in a structured way. We have\nseen several examples of such models in our discussion so far. For example, the Bernoulli process that\ndescribes the outcome of a series of coin flips as independent choices of heads or tails with probability\n$p$ is a simple statistical model; our more complicated mixture model in which we choose\n one of two coins at random and then flip that is a more complicated model.  \nOur description of the variation in temperature measurements\nas arising from perturbations from the true temperature by a normally distributed amount is\nanother example of a statistical model, this one involving a continuous random variable.\n\nWhen we apply a mathematical model to understand data, we often have a variety of parameters\nin the model that we must adjust to get the model to best \"fit\" the observed data.  For example,\nsuppose that we observe the vibrations of a block attached to a spring.  We know that the motion is\ngoverned by a second order linear differential equation, but the dynamics depend on the mass of the block,\nthe spring constant, and the damping coefficient.  By measuring the dynamics of the block over time,\nwe can try to work backwards to figure out these parameters, after which we will be able to predict\nthe block's motion into the future.\n\n### Maximum Likelihood (Discrete Case){#sec-mlcoin}\n\nTo see this process in a statistical setting, let's return to the simple example of a coin flip.\nThe only parameter in our model is the probability $p$ of getting heads on a particular flip.  Suppose\nthat we flip the coin $100$ times and get $55$ heads and $45$ tails.  What can we say about $p$?\n\nWe will approach this question via the \"likelihood\" function for our data.  We ask: for a particular\nvalue of the parameter $p$, how likely is this outcome?  From @eq-binomial\nwe have\n$$\nP(55H,45T)=\\binom{100}{55}p^{55}(1-p)^{45}.\n$$\n\nThis function is plotted in @fig-beta.  As you can see from that plot, it is extremely unlikely that\nwe would have gotten $55$ heads if $p$ was smaller than $.4$ or greater than $.7$, while the *most likely*\nvalue of $p$ occurs at the maximum value of this function, and a little calculus tells us that this\npoint is where $p=.55$. This *most likely* value of $p$ is called the *maximum likelihood estimate*\nfor the parameter $p$.\n\n![Likelihood Plot](img/beta.png){#fig-beta}\n\n### Maximum Likelihood (Continuous Case)\n\nNow let's look at our temperature measurements where the error is normally distributed with variance\nparameter $\\sigma^2$. As we have seen earlier, the probability density of errors $\\mathbf{x}=(x_1,\\ldots,x_n)$\nof $n$ independent measurements is\n$$\nP(\\mathbf{x}) = \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^{n}e^{-\\|\\mathbf{x}\\|^2/(2\\sigma^2)}d\\mathbf{x}.\n$$\n(see @eq-multivariategaussian).\nWhat should we use as the parameter $\\sigma$?  We can ask which choice of $\\sigma$ makes\nour data *most likely*.  To calculate this, we think of the probability of a function of $\\sigma$\nand use Calculus to find the maximum.  It's easier to do this with the logarithm.\n\n$$\n\\log P(\\mathbf{x})=\\frac{-\\|\\mathbf{x}\\|^2}{2\\sigma^2}-n\\log{\\sigma}+C\n$$\nwhere $C$ is a constant that we'll ignore.  Taking the derivative and setting it to zero, we obtain\n$$\n-\\|\\mathbf{x}\\|^2\\sigma^{-3}-n\\sigma^{-1}=0\n$$\nwhich gives the formula\n$$\n\\sigma^2=\\frac{\\|\\mathbf{x}\\|^2}{n}\n$$\n\nThis should look familiar! The maximum likelihood estimate of the variance\nis the  *mean-squared-error*.\n\n### Linear Regression and likelihood {#sec-LRLike}\n\nIn our earlier lectures we discussed linear regression at length.  Our introduction of ideas\nfrom probability give us new insight into this fundamental tool.  Consider a statistical model\nin which certain measured values $y$ depend linearly on $x$ up to a normally distributed error:\n$$\ny=mx+b+\\epsilon\n$$\nwhere $\\epsilon$ is drawn from the normal distribution with variance $\\sigma^2$.  \n\nThe classic regression setting has us measuring a collection of $N$ points $(x_i,y_i)$ and then\nasking for the \"best\" $m$, $b$, and $\\sigma^2$ to explain these measurements.  Using the\nlikelihood perspective, each value $y_i-mx_i-b$ is an independent draw from the normal distribution\nwith variance $\\sigma^2$, exactly like our temperature measurements in the one variable case.\n\nThe likelihood (density) of those draws is therefore\n$$\nP = \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^Ne^{-\\sum_{i}(y_i-mx_i-b)^2/(2\\sigma^2)}.\n$$\nWhat is the maximum likelihood estimate of the parameters $m$, $b$, and $\\sigma^2$?\n\nTo find this we look at the logarithm of $P$ and take derivatives.\n$$\n\\log(P) = -N\\log(\\sigma) -\\frac{1}{2\\sigma^2}\\sum_{i}(y_i-mx_i-b)^2.\n$$\n\nAs far as $m$ and $b$ are concerned, the minimum comes from the derivatives with respect to $m$ and $b$\nof\n$$\n\\sum_{i}(y_i-mx_i-b)^2.\n$$\nIn other words, the maximum likelihood estimate $m_*$ and $b_*$ for $m$ and $b$ are *exactly the ordinary least squares estimates.*\n\nAs far as $\\sigma^2$ is concerned, we find just as above that the maximum likelihood estimate $\\sigma^2_*$ is\nthe mean squared error\n$$\n\\sigma^2_*=\\frac{1}{N}\\sum_{i}(y_i-m_*x_i-b_*)^2.\n$$\n\nThe multivariate case of regression proposes a model of the form\n$$\nY=X\\beta+\\epsilon\n$$\nand a similar calculation again shows that the least squares estimates for $\\beta$ are the maximum\nlikelihood values for this model.\n\n## Bayesian Inference\n\nWe conclude our review of ideas from probability by examining the Bayesian perspective on data.\n\nSuppose that we wish to conduct an experiment to determine the temperature outside our house.\nWe begin our experiment with a statistical model that is supposed to explain the variability in the\nresults.  The model depends on some parameters that we wish to estimate.  For example,\nthe parameters of our experiment might be the 'true'  temperature $t_*$ and the variance\n$\\sigma^2$ of the error.\n\nFrom the Bayesian point of view, at the beginning of this experiment we have an initial\nsense of what the temperature is likely to be, expressed in the form of a probability distribution.\nThis initial information is called the *prior* distribution.\n\nFor example, if we know that it's December in Connecticut, our prior distribution might say that the\ntemperature is more likely to be between 20 and 40 degrees Fahrenheit and is quite unlikely to be higher than\n60 or lower than 0.  So our prior distribution might look like @fig-tempprior.\n\n![Prior Distribution on Temperature](img/prior.png){#fig-tempprior}\n\nIf we really have no opinion about the temperature other than its between say, $-20$ and $100$ degrees,\nour prior distribution might be uniform over that range, as in @fig-uniformprior.\n\n![Uniform Prior](img/uniform.png){#fig-uniformprior}\n\nThe choice of a prior will guide the interpretation of our experiments in ways that we will see shortly.\n\nThe next step in our experiment is the collection of data.  Suppose we let\n$\\mathbf{t}=(t_1,t_2,\\ldots, t_n)$ be a random variable representing $n$ independent measurements\nof the temperature.  We consider the *joint distribution*\nof the parameters $t_*$ and $\\sigma^2$ and the possible measurements $\\mathbf{t}$:\n$$\nP(\\mathbf{t},t_*,\\sigma^2)=\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^{n}e^{-\\|\\mathbf{t}-t_*\\mathbf{e}\\|^2/(2\\sigma^2)}\n$$\nwhere $\\mathbf{e}=(1,1,\\ldots, 1)$.  \n\nThe conditional probability $P(t_{*},\\sigma^2|\\mathbf{t})$ is the distribution of the values of $t_*$ and\n$\\sigma^2$*given*a value of the $\\mathbf{t}$.  This is what we hope to learn by our experiment -- namely,\nif we make a particular measurement, what does it tell us about $t_*$ and $\\sigma^2$?\n\nNow suppose that we actually make some measurements, and so we obtain a specific set of values $\\mathbf{t}_0$\nfor $\\mathbf{t}$.\n\nBy Bayes Theorem,\n$$\nP(t_{*},\\sigma^2|\\mathbf{t}=\\mathbf{t}_0) = \\frac{P(\\mathbf{t}=\\mathbf{t}_0|t_{*},\\sigma^2)P(t_{*},\\sigma^2)}{P(\\mathbf{t}=\\mathbf{t}_0)}\n$$\nWe interpret this as follows:\n\n- the left hand side $P(t_{*},\\sigma^2|\\mathbf{t}=\\mathbf{t}_0)$ is called the *posterior distribution* and is\nthe distribution of $t_{*}$ and $\\sigma^2$ obtained by *updating our prior knowledge with the results of our experiment.*\n- The probability $P(\\mathbf{t}=\\mathbf{t}_{0}|t_{*},\\sigma^2)$ is the probability of obtaining the\nmeasurements we found for a particular value of the parameters $t_{*}$ and $\\sigma^2$.\n- The probability $P(t_{*},\\sigma^2)$ is the *prior distribution* on the parameters that reflects our\ninitial impression of the distribution of these parameters.\n- The denominator $P(\\mathbf{t}=\\mathbf{t}_{0})$ is the total probability of the results that we obtained,\nand is the integral over the distribution of the parameters weighted by their prior probability:\n$$\nP(\\mathbf{t}=\\mathbf{t}_{0})=\\int_{t_{*},\\sigma^2}P(\\mathbf{t}=\\mathbf{t}_{0}|t_{*},\\sigma^2)P(t_{*},\\sigma^2)\n$$\n\n### Bayesian experiments with the normal distribution\n\nTo illustrate these Bayesian ideas, we'll consider the problem of measuring the temperature, but for simplicity\nlet's assume that we fix the variance in our error measurements at $1$ degree.  Let's use\nthe prior distribution on the true temperature that I proposed in @fig-tempprior, which is a normal\ndistribution with variance $15$ \"shifted\" to be centered at $30$:\n$$\nP(t_*)=\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)e^{-(t_*-30)^2/30}.\n$$\nThe expected value $E[t]$ -- the mean of the this distribution -- is $30$.\n\nSince the error in our measurements is normally distributed with variance $1$, we have\n$$\nP(t-t_{*})=\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)e^{-(t-t_{*})^2/2}\n$$\nor as a function of the absolute temperature, we have\n$$\nP(t,t_{*}) = \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)e^{-(t-t_*)^2/2}.\n$$\n\nNow we make a bunch of measurements to obtain $\\mathbf{t}_0=(t_1,\\ldots, t_n)$. We have\n$$\nP(\\mathbf{t}=\\mathbf{t}_0|t_{*}) = \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^ne^{-\\|\\mathbf{t}-t_*\\mathbf{e}\\|^2/2}.\n$$\n\nThe total probability $T=P(\\mathbf{t}=\\mathbf{t_0})$ is hard to calculate, so let's table that for a while.\nThe posterior probability is\n$$\nP(t_{*}|\\mathbf{t}=\\mathbf{t}_{0}) = \\frac{1}{T}\n\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^ne^{-\\|\\mathbf{t}-t_*\\mathbf{e}\\|^2/2}\n\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)e^{-(t_*-30)^2/30}.\n$$\n\nLeaving aside the multiplicative constants for the moment, consider the exponential\n$$\ne^{-(\\|\\mathbf{t}-t_{*}\\mathbf{e}\\|^2/2+(t_{*}-30)^2)/30}.\n$$\nSince $\\mathbf{t}$ is a vector of constants -- it is a vector of our particular measurements --\nthe exponent\n$$\n\\|\\mathbf{t}-t_{*}\\mathbf{e}\\|^2/2+(t_{*}-30)^2/30 = (t_{*}-30)^2/30+\\sum_{i} (t_{i}-t_{*})^2/2\n$$\nis a quadratic polynomial in $t_{*}$ that simplifies:\n$$\n(t_{*}-30)^2/30+\\sum_{i} (t_{i}-t_{*})^2/2 = At_{*}^2+Bt_{*}+C.\n$$\nHere\n$$\nA=(\\frac{1}{30}+\\frac{n}{2}),\n$$\n$$\nB=-2-\\sum_{i} t_{i}\n$$\n$$\nC=30+\\frac{1}{2}\\sum_{i} t_{i}^2.\n$$\n\nWe can complete the square to write\n$$\nAt_{*}^2+Bt_{*}+C = (t_{*}-U)^2/2V +K\n$$\nwhere\n$$\nU=\\frac{2+\\sum_{i}t_{i}}{\\frac{1}{15}+n}\n$$\nand\n$$\nV=\\frac{1}{\\frac{1}{15}+n}.\n$$\nSo up to constants that don't involve $t_{*}$, the posterior density is\nof the form\n$$\ne^{(t_{*}-U)^2/2V}\n$$\nand since it is a probability density, the constants must work out to give total integral of $1$.\nTherefore the posterior density is a normal distribution centered at $U$ and with variance $V$.\nHere $U$ is called the*posterior mean*and $V$ the*posterior variance*.\n\nTo make this explicit, suppose $n=5$ and we measured the following temperatures:\n$$\n40, 41,39, 37, 44\n$$\nThe mean of these observations is $40.2$ and the variance is $5.4$.\n\nA calculation shows that the posterior mean is $40.1$ and the posterior variance is\n$0.2$.  Comparing the prior with the posterior, we obtain the plot in @fig-comparison.\nThe posterior has a sharp peak at $40.1$ degrees.  This value is just a bit\nsmaller than the\nmean of the observed temperatures which is $40.2$ degrees.  This difference is caused by the prior --\nour prior distribution said the temperature was likely to be around $30$ degrees, and so the prior pulls\nthe observed mean a bit towards the prior mean taking into account past experience.  Because the\nvariance of the prior is large, it has a relatively small influence on the posterior.\n\nThe general version of the calculation above is summarized in this proposition.\n\n**Proposition:** Suppose that our statistical model for an experiment proposes that\nthe measurements are normally distributed around an (unknown)  mean value of $\\mu$ with a (fixed)\nvariance $\\sigma^2$.  Suppose further that our prior distribution on the unknown mean $\\mu$\nis normal with mean $\\mu_0$ and variance $\\tau^2$.  Suppose we make measurements\n$$\ny_1,\\ldots, y_n\n$$\nwith mean $\\overline{y}$.  Then the posterior distribution of $\\mu$ is again normal,\nwith posterior variance\n$$\n\\tau'^2 = \\frac{1}{\\frac{1}{\\tau^2}+\\frac{n}{\\sigma^2}}\n$$\nand posterior mean\n$$\n\\mu' = \\frac{\\frac{\\mu_0}{\\tau^2}+\\frac{n}{\\sigma^2}\\overline{y}}{\\frac{1}{\\frac{1}{\\tau^2}+\\frac{n}{\\sigma^2}}}\n$$\n\nSo the posterior mean is a sort of weighted average of the sample mean and the prior mean; and as $n\\to\\infty$,\nthe posterior mean approaches the sample mean -- in other words, as you get more data, the prior has\nless and less influence on the results of the experiment.\n\n![Prior and Posterior](img/priorposterior.png){#fig-comparison}\n\n### Bayesian coin flipping\n\nFor our final example in this fast overview of ideas from probability, we consider the problem of deciding\nwhether a coin is fair.  Our experiment consists of $N$ flips of  a coin with unknown probability $p$ of heads,\nso the data consists of the number $h$ of heads out of the $N$ flips.  To apply Bayesian reasoning,\nwe need a prior distribution on $p$.  Let's first assume that we have no reason to prefer one value of $p$\nover another, and so we choose for our prior the uniform distribution on $p$ between $0$ and $1$.\n\nWe wish to analyze $P(p|h)$, the probability distribution of $p$ given $h$ heads out of $N$ flips.\nBayes Theorem gives us:\n$$\nP(p|h) = \\frac{P(h|p)P(p)}{P(h)}\n$$\nwhere\n$$\nP(h|p) = \\binom{N}{h}p^{h}(1-p)^{N-h}\n$$\nand\n$$\nP(h)=\\int_{p=0}^{1} P(h|p)P(p) dp = \\binom{N}{h}\\int_{p=0}^{1} p^{h}(1-p)^{N-h}dp\n$$\nis a constant which insures that $$\\int_{p}P(p|h)dp=1.$$\n\nWe see that the posterior distribution $P(p|h)$ is proportional to the polynomial function\n$$\nP(p|h)\\propto p^{h}(1-p)^{N-h}.\n$$\nAs in @sec-mlcoin, we see that this function peaks at $h/N$.  This is called the maximum *a posteriori estimate*\nfor $p$.  \n\nAnother way to summarize the posterior distribution $P(p|h)$ is to look at the expected value of $p$.\nThis is called the *posterior mean* of $p$.  To compute it, we need to know the normalization\nconstant in the expression for $P(p|h)$, and for that we can take advantage of the properties of a special\nfunction $B(a,b)$ called the Beta-function:\n$$\nB(a,b) = \\int_{p=0}^{1} p^{a-1}(1-p)^{b-1} dp.\n$$\n\n**Proposition:** If $a$ and $b$ are integers, then $B(a,b)=\\frac{a+b}{ab}\\frac{1}{\\binom{a+b}{a}}$.\n\n**Proof:**   Using integration by parts, one can show that\n$$\nB(a,b)=\\frac{a-1}{b}B(a-1,b+1)\n$$\nand a simple calculation shows that\n$$\nB(1,b) = \\frac{1}{b}.\n$$\nLet\n$$\nH(a,b)=\\frac{a+b}{ab}\\frac{1}{\\binom{a+b}{a}} = \\frac{(a-1)!(b-1)!}{(a+b-1)!}\n$$\nThen it's easy to check that $H$ satsifies the same recurrences as $B(a,b)$, and that\n$H(1,b)=1/b$.  So the two functions agree by induction.\n\nUsing this Proposition, we see that\n$$\nP(p|h) = \\frac{p^{h}(1-p)^{N-h}}{B(h+1,N-h+1)}\n$$\nand\n$$\nE[p] = \\frac{\\int_{p=0}^{1} p^{h+1}(1-p)^{N-h}dp}{B(h+1,N-h+1)}=\\frac{B(h+2,N-h+1)}{B(h+1,N-h+1)}.\n$$\nSorting through this using the formula for $B(a,b)$ we obtain\n$$\nE[p]=\\frac{h+1}{N+2}.\n$$\n\nSo if we obtained $55$ heads  out of $100$ flips, the maximum a posteriori estimate for $p$ is $.55$,\nwhile the posterior mean is $56/102=.549$ -- just a bit less.\n\nNow suppose that we had some reason to believe that our coin was fair.  Then we can choose a prior\nprobability distribution that expresses this.  For example, we can choose\n$$\nP(p) = \\frac{1}{B(5,5)}p^{4}(1-p)^{4}.\n$$\nHere we use the Beta function to guarantee that $\\int_{0}^{1}P(p)dp=1$.  We show this prior distribution\nin @fig-betaprior.  \n\n![Beta(5,5) Prior](img/betaprior.png){#fig-betaprior}\n\nIf we redo our Bayes theorem calculation, we find that our posterior distribution is\n$$\nP(p|h) \\propto p^{h+4}(1-p)^{N-h+4}\n$$\nand relying again on the Beta function for normalization we have\n$$\nP(p|h) = \\frac{1}{B(h+5,N-h+5)}p^{h+4}(1-p)^{N-h+4}\n$$\nHere the maximum a posterior estimate for $p$ is $h+4/N+8$ while our posterior mean is\n$$\n\\frac{B(h+6,N-h+5)}{B(h+5,N-h+5)} = \\frac{h+5}{N+10}.\n$$\n\nIn the situation of $55$ heads out of $100$, the maximum a posteriori estimate is $.546$ and\nthe posterior mean is $.545$.  These numbers have been pulled just a bit towards $.5$ because our\nprior knowledge makes us a little bit biased towards $p=.5$.\n\n### Bayesian Regression (or Ridge Regression)\n\nIn this chapter we return to our discussion of linear regression and introduce some Bayesian ideas.\nThe combination will lead us to the notion of \"ridge\" regression, which is a type of linear regression that\nincludes a prior distribution on the coefficients that indicates our preference for smaller rather than larger\ncoefficients. Introduction of this prior leads to a form of linear regression that is more resilient in situations\nwhere the independent variables are less independent than we would hope.\n\nBefore introducing these Bayesian ideas, let us recall from @sec-LRLike that ordinary least squares yields the\nparameters that give the \"most likely\" set of parameters for a model of the form\n$$\nY=XM + \\epsilon\n$$\nwhere the error $\\epsilon$ is normally distributed with mean $0$ and variance $\\sigma^2$, and the mean squared error becomes the maximum likelihood estimate of the variance $\\sigma^2$.\n\nTo put this into a Bayesian perspective, we notice that the linear regression model views $Y-XM$\nas normally distributed given $M$.  That is, we see the probability $P(Y-XM|M)$ as normal with variance\n$\\sigma^2$.  \n\nThen we introduce a prior distribution on the coefficients $M$, assuming that they, too, are normally\ndistributed around zero with variance $\\tau^2$.  This means that *ab initio* we think that the coefficients\nare likely to be small.\n\nFrom Bayes Theorem, we then have\n$$\nP(M|Y,X) = \\frac{P(Y,X|M)P(M)}{P(Y,X)}\n$$\nand in distribution terms we have\n$$\nP(M|Y,X) = Ae^{\\|Y-XM\\|^2/\\sigma^2}e^{-\\|M\\|^2/\\tau^{2}}\n$$\nwhere $A$ is a normalizing constant.\n\nThe first thing to note from this expression is that the posterior distribution for the $M$ parameters for regression are \nthemselves normally distributed. \n\nThe maximum likelihood estimate $M_{r}$ for the parameters $M$ occurs when $P(M|Y,X)$ is maximum,\nwhich we find by taking the derivatives.  Using the matrix algebra developed in our linear regression\nchapter, we obtain the equation\n$$\n(X^{\\intercal}Y-(X^{\\intercal}X)M_r)/\\sigma^2-M_r/\\tau^{2}=0\n$$\nor\n$$\n(X^{\\intercal}X+s)M_r=X^{\\intercal}Y\n$${#eq-ridgeformula}\nwhere $s=\\sigma^2/\\tau^2$.\n\nTherefore the ridge coefficients are given by the equation\n$$\nM_{r}=(X^{\\intercal}X+s)^{-1}X^{\\intercal}Y\n$${#eq-ridgecoeffs}\n\n#### Practical aspects of ridge regression\n\nUsing ridge regression leads to a solution to the least squares problem in which the regression coefficients\nare biased towards being smaller.  Beyond this, there are a number of implications of the technique which\naffect its use in practice.\n\nFirst, we can put the Bayesian derivation of the ridge regression formulae in the background and focus our attention\non @eq-ridgecoeffs.  We can treat the  parameter $s$ (which must be non-negative) as \"adjustable\".  \n\nOne important consideration when using ridge regression is that @eq-ridgecoeffs is not invariant if we scale $X$ and $Y$ by a constant.  This is different from \"plain\"\nregression where we consider the equation $Y=XM$.  In that case, rescaling $X$ and $Y$ by the same factor leaves the coefficients $M$ alone.  For this reason, ridge regression\nis typically used on centered, standardized coordinates.  In other words, we replace each feature $x_i$ by $(x_i-\\mu_i)/\\sigma_i$ where $\\mu_i$ and $\\sigma_i$ are the sample mean and standard\ndeviation of the $i^{th}$ feature, and we replace our response variables $y_i$ similarly by $(y-\\mu)/\\sigma$ where $\\mu$ and $\\sigma$ are the mean and standard deviation of the $y$-values.\nThen we find $M$ using @eq-ridgecoeffs, perhaps experimenting with different values of $s$, using our centered and standardized variables. \n\nTo emphasize that we are using centered coordinates, we write $X_{0}$ for our data matrix instead\nof $X$. Recall that the matrix $X_0^{\\intercal}X_0$\nthat enters into @eq-ridgeformula is $ND_{0}$ where $D_{0}$ is the covariance matrix. Therefore in ridge regression we have replaced $ND_0$ by $ND_0+s$.\nSince $D_0$ is a real symmetric matrix, as we've seen in Chapter 2 it is diagonalizable so that $AD_0A^{-1}$ is diagonal for\nan orthogonal matrix $A$ and has eigenvalues $\\lambda_1\\ge \\ldots\\ge \\lambda_k$ which are the variances of the data\nalong the principal directions.\n\nOne effect of using ridge regression is that the eigenvalues of $ND_{0}+s$ are always at least\n$s>0$, so the use of ridge regression avoids the possibility that $D_{0}$ might not be invertible. \nIn fact, a bit more is true.  Numerical analysis tells us that when considering the problem\nof computing the inverse of  a matrix, we should look at its\n*condition number*, which is the ratio $\\lambda_1/\\lambda_k$ of the largest to the smallest eigenvalue.\n\nIf the condition number of a matrix  is large, then results from numerical analysis show that it is *almost singular* and its inverse becomes very sensitive\nto small changes in the entries of the matrix.  However, the eigenvalues of $ND_0+s$ are $N\\lambda_{i}+s$ and so the condition number becomes $(N\\lambda_1+s)/(N\\lambda_k+s)$.\nFor larger values of $\\lambda$, this condition number shrinks, and so the inverse of the matrix $ND_0+s$ becomes better behaved\nthan $ND_{0}$.  In this way, ridge regression helps to improve the numerical stability of the linear regression algorithm.\n\nA second way to look at Ridge regression is to go back to the discussion of the singular value decomposition of the matrix $X_{0}$ in section @sec-svd.  There we showed that the SVD of\n$X_{0}$ yields an expression\n$$\nX_{0}=U\\tilde{\\Lambda}P^{\\intercal}\n$$\nwhere $U$ and $P$ are orthogonal matrices and $\\Lambda$ is an $N\\times k$ matrix whose upper block\nis diagonal with eigenvalues $\\sqrt{N\\lambda_{i}}$.  The rows of $U$ gave us an orthonormal basis\nthat allowed us to write the predicted vector $\\hat{Y}$ as a projection:\n$$\n\\hat{Y}=\\sum_{i=1}^{k} (u_j\\cdot Y)u_{j}^{\\intercal}.\n$$\n\nIf we repeat this calculation, but using the ridge regression formula, we obtain\n$$\n\\hat{Y}_{r}=X_{0}M_r = U\\tilde{\\Lambda}P^{\\intercal}(P\\tilde{\\Lambda}^{\\intercal}U^{\\intercal}U\\tilde{\\Lambda}P^{\\intercal}+s)^{-1}P\\tilde{\\Lambda}^{\\intercal}U^{\\intercal}Y.\n$$\nSince $P$ is orthogonal, $P^{\\intercal}=P^{-1}$, so\n$$\nP^{\\intercal}(P\\tilde{\\Lambda}^2P^{\\intercal}+s)^{-1}P=P^{-1}(P(\\Lambda+s)P^{-1})P=(\\Lambda+s)^{-1}\n$$\nand $\\Lambda+s$ is a $k\\times k$ diagonal matrix with entries $N\\lambda_{i}+s$.\n\nPutting the pieces together we see that\n$$\n\\hat{Y}_{r}=U\\tilde{\\Lambda}(\\Lambda+s)^{-1}\\tilde{\\Lambda}U^{\\intercal}Y.\n$$\n\nIn the language of orthogonal projection, this means that\n$$\n\\hat{Y}_{r} = \\sum_{i=1}^{k} \\frac{N\\lambda_{i}}{N\\lambda_{i}+s}(u_j\\cdot Y)u_{j}^{\\intercal}.\n$$\n\nIn other words, the predicted value computed by ridge regression is obtained by projecting\n$Y$ into the space spanned by the feature vectors, but weighting the different principal components \nby $N\\lambda_{i}/(N\\lambda_{i}+s)$.  With this weighting, the  principal components with smaller variances are weighted less than those with larger variances.  For this reason,\nridge regression is sometimes called a *shrinkage* method.\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"03-probability.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":[{"text":"\\usepackage{listings}\n"},{"file":"../chapters/macros.tex"}],"output-file":"03-probability.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","documentclass":"scrreprt"},"extensions":{"book":{}}}}}