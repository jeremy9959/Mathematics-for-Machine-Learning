{"title":"Principal Component Analysis","markdown":{"headingText":"Principal Component Analysis","containsRefs":false,"markdown":"\n\n## Introduction\n\nSuppose that, as usual,  we begin with a\ncollection of measurements of different features for a group of samples.  Some of these measurements\nwill tell us quite a bit about the difference among our samples, while others may contain relatively little\ninformation.  For example, if we are analyzing the effect of a certain weight loss regimen on a group of people,\nthe age and weight of the subjects may have a great deal of influence on how successful the regimen is, while\ntheir blood pressure might not.  One way to help identify which features are more significant is\nto ask whether or not the feature varies a lot among the different samples.  If nearly all the measurements\nof a feature are the same, it can't have much power in distinguishing the samples, while if the measurements\nvary a great deal then that feature has a chance to contain useful information.\n\nIn this section we will discuss a way to measure the variability of measurements and then introduce principal\ncomponent analysis (PCA).  PCA is a method for finding which linear combinations of measurements have the\ngreatest variability and therefore might contain the most information.  It also allows us to identify\ncombinations of measurements that don't vary much at all.  Combining this information, we can sometimes\nreplace our original system of features with a smaller set that still captures most of the interesting\ninformation in our data, and thereby find hidden characteristics of the data and simplify our analysis\na great deal.\n\n## Variance and Covariance\n\n### Variance\n\nSuppose that we have a collection of measurements $(x_1,\\ldots, x_n)$ of a particular feature $X$.\nFor example,  $x_i$ might be the initial weight of the $ith$ participant in our weight loss study.\nThe mean of the values $(x_1,\\ldots, x_n)$ is\n\n$$\n\\mu_{X} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}.\n$$\n\nThe simplest measure of the variability of the data is called its *variance.*\n\n**Definition:** The (sample) variance of the data $x_1,\\ldots, x_n$ is\n\n$$\n\\sigma_{X}^2 = \\frac{1}{n}\\sum_{i=1}^{n} \\left(x_{i}-\\mu_{X}\\right)^2 = \\frac{1}{n}\\left(\\sum_{i=1}^{n} x_{i}^2\\right)- \\mu_{X}^2\n$${#eq-variance}\n\nThe square root of the variance is called the *standard deviation.*\n\nAs we see from the formula, the variance is a measure of how 'spread out' the data is from the mean.\n\nRecall that in our discussion of linear regression we thought of our set of measurements $x_1,\\ldots, x_n$\nas a vector -- it's one of the columns of our data matrix.  From that point of view, the variance\nhas a geometric interpretation -- it is $\\frac{1}{N}$ times the square of the\ndistance from the point $X=(x_1,\\ldots, x_n)$ to the point $\\mu_{X}(1,1,\\ldots,1)=\\mu_{X}E$:\n\n$$\n\\sigma_{X}^2 = \\frac{1}{n}(X-\\mu_{X}E)\\cdot(X-\\mu_{X}E)  = \\frac{1}{n}\\|X-\\mu_{X}E\\|^2.\n$${#eq-variancedot}\n\n### Covariance\n\nThe variance measures the dispersion of measures of a single feature.  Often, we have measurements\nof multiple features and we might want to know something about how two features are related.  The\n*covariance* is a measure of whether two features tend to be related, in the sense that when one\nincreases, the other one increases; or when one increases, the other one decreases.  \n\n**Definition:**\nGiven measurements $(x_1,\\ldots, x_n)$ and $(y_1,\\ldots, y_n)$ of two features $X$ and $Y$, the covariance\nof $X$ and $Y$ is\n\n$$\n\\sigma_{XY} = \\frac{1}{N}\\sum_{i=1}^{N} (x_i-\\mu_{X})(y_i-\\mu_{Y})\n$${#eq-covariancedot}\n\nThere is a nice geometric interpretation of this, as well, in terms of the dot product.  If\n$X=(x_1,\\ldots, x_n)$ and $Y=(y_1\\ldots,y_n)$ then\n\n$$\n\\sigma_{XY} = \\frac{1}{N} ((X-\\mu_{X}E)\\cdot (Y-\\mu_{Y}E)).\n$$\n\nFrom this point of view, we can see that $\\sigma_{XY}$ is positive if the $X-\\mu_{X}E$ and $Y-\\mu_{Y}E$ vectors\n\"point roughly\nin the same direction\" and its negative if they \"point roughly in the opposite direction.\"\n\n### Correlation\n\nOne problem with interpreting the variance and covariance is that we don't have a scale --\nfor example, if $\\sigma_{XY}$ is large and positive, then we'd like to say that $X$ and $Y$ are closely related,\nbut it could be just that the entries of $X-\\mu_{X}E$ and $Y-\\mu_{Y}E$ are large.   Here, though,\nwe can really take advantage of the geometric interpretation.  Recall that the dot product of\ntwo vectors satisfies the formula\n\n$$\na \\cdot b = \\|a\\|\\|b\\|\\cos(\\theta)\n$$\n\nwhere $\\theta$ is the angle between $a$ and $b$.  So\n\n$$\n\\cos(\\theta) = \\frac{a\\cdot b}{\\|a\\|\\|b\\|}.\n$$\n\nLet's apply this to the variance and covariance, by noticing that\n\n$$\n\\frac{(X-\\mu_{X}E)\\cdot (Y-\\mu_{Y}E)}{\\|(X-\\mu_{X}E)\\|\\|(Y-\\mu_{Y}E)\\|} = \\frac{\\sigma_{XY}}{\\sigma_{XX}\\sigma_{YY}}\n$$\n\nso the quantity\n\n$$\nr_{XY} = \\frac{\\sigma_{XY}}{\\sigma_{X}\\sigma_{Y}}\n$${#eq-rxy}\n\nmeasures the cosine of the angle between the vectors $X-\\mu_{X}E$ and $Y-\\mu_{Y}E$.\n\n**Definition:** The quantity $r_{XY}$ defined in  @eq-rxy is called the (sample)\n*correlation coefficient* between $X$ and $Y$.  We have $0\\le |r_{XY}|\\le 1$ with $r_{XY}=\\pm 1$\nif and only if the two vectors $X-\\mu_{X}$ and $Y-\\mu_{Y}$ are collinear in $\\mathbf{R}^{n}$.\n\n*@fig-corrfig illustrates data with different values of the correlation coefficient.\n\n![Correlation](img/correlation.png){#fig-corrfig width=50%}\n\n### The covariance matrix {#sec-covarmat}\n\nIn a typical situation we have many features for each of our (many)\nsamples, that we organize into a data matrix $X$.  To recall, each\ncolumn of $X$ corresponds to a feature that we measure, and each row\ncorresponds to a sample.  For example, each row of our matrix might\ncorrespond to a person enrolled in a study, and the columns correspond\nto height (cm), weight (kg), systolic blood pressure, and age (in\nyears):\n\n|sample | Ht | Wgt |Bp | Age|\n|----|-------:|-------:|--:|---:|\n| A   |  180   |  75    |110| 35 |\n| B   |  193   |  80    |130| 40 |\n| ... | ...    | ...    |...|... |\n| U   | 150    |  92    |105| 55 |\n\n: A sample data matrix $X$\n\nIf we have multiple features, as in this example, we might be interested in the variance of each\nfeature and all of their mutual covariances.  This \"package\" of information can be obtained \"all at once\"\nby taking advantage of some matrix algebra.\n\n**Definition:** Let $X$ be a $N\\times k$ data matrix, where the $k$ columns of $X$ correspond to different features and\nthe $N$ rows to different samples.  Let $X_{0}$ be the centered version of this data matrix, obtained by subtracting\nthe mean $\\mu_{i}$ of column  $i$ from all the entries $x_{si}$ in that column.  Then the $k\\times k$ symmetric matrix\n\n$$\nD_{0} = \\frac{1}{N}X_{0}^{\\intercal}X_{0}\n$$\n\nis called the (sample) covariance matrix for the data.\n\n**Proposition:** The diagonal entries  $d_{ii}$ of $D_{0}$ are the variances\nof the columns of $X$:\n\n$$\nd_{ii} = \\sigma_{i}^2 = \\frac{1}{N}\\sum_{s=1}^{N}(x_{si}-\\mu_i)^2\n$$\n\nand the off-diagonal entries $d_{ij} = d_{ji}$ are the covariances of the $i^{th}$ and $j^{th}$ columns of $X$:\n\n$$\nd_{ij} = \\sigma_{ij} = \\frac{1}{N}\\sum_{s=1}^{N}(x_{si}-\\mu_{i})(x_{sj}-\\mu_{j})\n$$\n\nThe sum of the diagonal entries, the trace of $D_{0}$ is the **total** variance of the data.\n\n**Proof:** This follows from the definitions, but it's worth checking the details, which we leave as an exercise.\n\n### Visualizing the covariance matrix {#sec-visualizecovar}\n\nIf the number of features in the data is not too large, a density matrix plot provides a tool\nfor visualizing the covariance matrix of the data.  A density matrix plot is an $k\\times k$ grid\nof plots (where $k$ is the number of features).  The  entry with $(i,j)$ coordinates in the grid is a scatter plot of the $i^{th}$\nfeature against the $j^{th}$ one if $i\\not=j$, and is a histogram of the $i^{th}$ variable if $i=j$.\n\n*@fig-density0 is an example of a density matrix plot for a dataset with $50$ samples and $2$ features.\nThis data has been centered, so it can be represented\nin a $50\\times 2$ data matrix $X_{0}$.  The upper left and lower right graphs are scatter plots\nof the two columns, while the lower left and upper right are the histograms of the columns.\n\n![Density Matrix Plot](img/density2x2.png){#fig-density0 width=50%}\n\n### Linear Combinations of Features (Scores)\n\nSometimes useful information about our data can be revealed if we combine different measurements\ntogether to obtain a \"hybrid\" measure that captures something interesting.  For example, in the Auto MPG\ndataset that we studied in the section on Linear Regression, we looked at the influence of both vehicle\nweight $w$ and engine displacement $e$ on gas mileage; perhaps their is some value in considering a hybrid\n\"score\" defined as\n$$\nS = aw + be\n$$\nfor some constants $a$ and $b$ -- maybe by choosing a good combination we could find a better predictor\nof gas mileage than using one or the other of the features individually.\n\nAs another example, suppose we are interested in the impact of the nutritional content of food on\nweight gain in a study.  We know that both calorie content and the level\ndietary fiber contribute to the weight gain of participants eating this particular food; maybe there is some\nkind of combined \"calorie/fiber\" score we could introduce that captures the impact of that food better.\n\n**Definition:** Let $X_{0}$ be a (centered) $N\\times k$ data matrix  giving information about $k$ features for\neach of $N$ samples.  A linear synthetic feature, or a linear score, is a linear combination of the $k$ features.\nThe linear score is defined by constants $a_{1},\\ldots, a_{k}$ so that\nIf $y_{1},\\ldots, y_{k}$ are the values of the features for a particular sample, then the linear score\nfor that sample is\n\n$$\nS = a_{1}y_{1}+a_{2}y_{2}+\\cdots+a_{k}y_{k}\n$$\n\n**Lemma:**\nThe values of the linear score for each of the $N$ samples can be calculated as\n\n$$\n\\left[\\begin{matrix} S_{1} \\\\ \\vdots \\\\ S_{N}\\\\ \\end{matrix}\\right] =\nX_{0}\\left[\n\\begin{matrix} a_{1} \\\\ \\vdots \\\\ a_{k}\\end{matrix}\\right].\n$${#eq-linearscore}\n\n**Proof:** Multiplying a matrix by a column vector computes a linear combination of the columns --\nthat's what this lemma says. Exercise 3 asks you to write out the indices and make sure you believe this.\n\n### Mean and variance of scores\n\nWhen we combine features to make a hybrid score, we assume that the features were centered to begin with,\nso that each features has mean zero.  As a result, the mean of the hybrid features is again zero.\n\n**Lemma:** A linear combination of features with mean zero again has mean zero.\n\n**Proof:** Let $S_{i}$ be the score for the $i^{th}$ sample, so\n$$\nS_{i} = \\sum_{j=1}^{k} x_{ij}a_{j}.\n$$\nwhere $X_{0}$ has entries $x_{ij}$.  Then the mean value of the score is\n$$\n\\mu_{S} = \\frac{1}{k}\\sum_{i=1}^{N} S_{i} = \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{k} x_{ij}a_{j}.\n$$\nReversing the order of the sum yields\n$$\n\\mu_{S} = \\frac{1}{N}\\sum_{j=1}^{k}\\sum_{i=1}^{N} x_{ij}a_{j} = \\sum_{j=1}^{k} a_{j}\\frac{1}{N}(\\sum_{i=1}^{N} x_{ij})=\n\\sum_{j=1}^{k}a_{j}\\mu_{j}=0\n$$\nwhere $\\mu_{j}=0$ is the mean of the $j^{th}$ feature (column) of $X_{0}$.\n\nThe variance is more interesting, and gives us an opportunity to put the covariance matrix to work.\nRemember from @eq-variancedot that, since a score $S$ has mean zero, it's variance is\n$\\sigma_{S}^2=\\frac{1}{N}S\\cdot S$ -- where here the score $S$ is represented by the column vector with entries\n$S_{1},\\ldots S_{k}$ as in @eq-linearscore.\n\n**Lemma:** The variance of the score $S$ with weights $a_1,\\ldots a_k$ is\n$$\n\\sigma_{S}^2 = a^{\\intercal}D_{0}a = \\left[\\begin{matrix}a_{1} & \\cdots & a_{k}\\end{matrix}\\right]D_{0}\n\\left[\\begin{matrix} a_{1} \\\\ \\vdots \\\\ a_{k}\\end{matrix}\\right]\n$${#eq-ada}\nMore generally, if $S_{1}$ and $S_{2}$ are scores with weights $a_1,\\ldots, a_k$ and\n$b_1,\\ldots, b_k$ respectively, then the covariance $\\sigma_{S_{1}S_{2}}$\nis\n$$\n\\sigma_{S_{1}S_{2}} = a^{\\intercal}D_{0}b.\n$$\n\n**Proof:** From @eq-variancedot and @eq-linearscore we know that\n$$\n\\sigma_{S}^2 = \\frac{1}{N}S\\cdot S\n$$\nand\n$$\nS = X_{0}a.\n$$\nSince $\\frac{1}{N}S\\cdot S = \\frac{1}{N}S^{\\intercal}S$, this gives us\n$$\n\\frac{1}{N}\\sigma_{S}^2 = \\frac{1}{N}(X_{0}a)^{\\intercal}(X_{0}a) = \\frac{1}{N}a^{\\intercal}X_{0}^{\\intercal}X_{0}a = a^{\\intercal}D_{0}a\n$$\nas claimed.\n\nFor the covariance, use a similar argument with @eq-covariancedot and @eq-linearscore.\nwriting $\\sigma_{S_{1}S_{2}}=\\frac{1}{N}S_{1}\\cdot S_{2}$ and\nthe fact that $S_{1}$ and $S_{2}$ can be written as $X_{0}a$ and $X_{0}b$.\n\nThe point of this lemma is that the covariance matrix contains not just the variances and covariances\nof the original features, but also enough information to construct the variances and covariances\nfor *any linear combination of features.*\n\nIn the next section we will see how to exploit this\nidea to reveal hidden structure in our data.\n\n### Geometry of Scores\n\nLet's return to the dataset that we looked at in @sec-visualizecovar.  We simplify the density\nmatrix plot in @fig-pcasimfig, which shows one of the scatter plots\nand the two histograms.\n\nThe scatter plot shows that the data points are arranged\nin a more or less elliptical cloud oriented at an angle to the $xy$-axes which represent the two given\nfeatures.  The two individual histograms show the distribution of the two features -- each has mean zero,\nwith the $x$-features distributed between $-2$ and $2$ and the $y$ feature between $-4$ and $4$.\nLooking just at the two features individually, meaning only at the two histograms, we can't see the\noverall elliptical structure.\n\n![Simulated Data with Two Features](img/PCAsimulated-1.png){#fig-pcasimfig width=50%}\n\nHow can we get a better grip on our data in this situation? We can try to find a \"direction\"\nin our data that better illuminates the variation of the data.   For example,\nsuppose that we pick a unit vector at the origin pointing in a particular direction\nin our data.  See @fig-pcasimfig-1.\n\n![A direction in the data](img/PCAsimulated-2.png){#fig-pcasimfig-1 width=50%}\n\nNow we can orthogonally project the datapoints onto the line defined by this vector,\nas shown in @fig-pcasimfig-2.\n\n![Projecting the datapoints](img/PCAsimulated-3.png){#fig-pcasimfig-2 width=50%}\n\nRecall that if the unit vector is defined by coordinates $u=[u_0,u_1]$, then the\northogonal projection of the point $x$ with coordinates $(x_0,x_1)$ is $(x\\cdot u)u$.\nNow\n$$\nx\\cdot u = u_0 x_0 + u_1 x_1\n$$\nso the coordinates of the points along the line defined by $u$ are the values of the score $Z$\ndefined by $u=[u_0,u_1]$.  Using our work in the previous section, we see that we can\nfind all of these coordinates by matrix multiplication:\n$$\nZ = X_0 u\n$$\nwhere $X_0$ is our data matrix.  Now let's add a histogram of the values of $Z$\nto our picture:\n\n![Distribution of Z](img/PCAsimulated-4.png){#fig-pcasimfig-3 width=50%}\n\nThis histogram shows the distribution of the values of $Z$ along the tilted line defined\nby the unit vector $u$.\n\nFinally, using our work on the covariance matrix, we see that the\nvariance of $Z$ is given by\n$$\n\\sigma_{Z}^2 = \\frac{1}{50}u^{\\intercal}X_{0}^{\\intercal}X_{0}u = u^{\\intercal}D_{0}u\n$$\nwhere $D_{0}$ is the covariance matrix of the data $X_{0}$.\n\n**Lemma:** Let $X_{0}$ be a $N\\times k$ centered data matrix, and let $D_{0}=\\frac{1}{N}X_{0}^{\\intercal}X_{0}$\nbe the associated covariance matrix.  Let $u$ be a unit vector in \"feature space\" $\\mathbf{R}^{k}$.  Then\nthe score $S=X_{0}u$ can be interpreted as the coordinates of the points of $X_{0}$ projected onto the line\ngenerated by $u$. The variance of this score is\n$$\n\\sigma^{2}_{S} = u^{\\intercal}D_{0}u = \\sum_{i=1}^{N} s_{i}^2\n$$\nwhere $s_{i} = X_{0}[i,:]u$ is the dot product of the $i^{th}$ row $X_{0}[i,:]$ with $u$. It measures\nthe variability in the data \"in the direction of the unit vector $u$\".\n\n## Principal Components\n\n### Change of variance with direction\n\nAs we've seen in the previous section, if we choose a unit vector $u$ in the feature space and\nfind the projection $X_{0}u$ of our data onto the line through $u$, we get a \"score\" that we can use\nto measure the variance of the data in the direction of $u$.  What happens as we vary $u$?\n\nTo study this question, let's continue with our simulated data from the previous section,\nand introduce a unit vector\n$$\nu(\\theta) = \\left[\\begin{matrix} \\cos(\\theta) & \\sin(\\theta)\\end{matrix}\\right].\n$$\nThis is in fact a unit vector, since $\\sin^2(\\theta)+\\cos^2(\\theta)=1$, and it is oriented\nat an angle $\\theta$ from the $x$-axis.\n\nThe variance of the data in the direction of $u(\\theta)$ is given by\n$$\n\\sigma_{\\theta}^2 = u(\\theta)^{\\intercal}D_{0}u(\\theta).\n$$\n\nA plot of this function for the data we have been considering is in @fig-pcatheta.\nAs you can see, the variance goes through two full periods with the angle, and it reaches a maximum and\nminimum value at intervals of $\\pi/2$ -- so the two angles where the variance are maximum and minimum\nare orthogonal to one another.\n\n![Change of variance with angle theta](img/PCAtheta.png){#fig-pcatheta width=25%}\n\nThe two directions where the variance is maximum and minimum are drawn on the original\ndata scatter plot in @fig-pcaprincipal .\n\n![Data with principal directions](img/PCAprincipal.png){#fig-pcaprincipal width=25%}\n\nLet's try to understand why this is happening.\n\n### Directions of extremal variance {#sec-extremalvariance}\n\nGiven our centered, $N\\times i$ data matrix $X_{0}$, with its associated covariance matrix\n$D_{0}=\\frac{1}{N}X_{0}^{\\intercal}X_{0}$, we would like to find unit vectors $u$ in $\\mathbf{R}^{k}$\nso that\n$$\n\\sigma_{u}^{2} = u^{\\intercal}D_{0}u\n$$\nreaches its maximum and its minimum.  Here $\\sigma_{u}^2$ is the variance of the \"linear score\"\n$X_{0}u$ and it represents how dispersed the data is in the \"u direction\" in $\\mathbf{R}^{k}$.\n\nIn this problem, remember that the coordinates of $u=(u_1,\\ldots, u_{k})$ are the variables\nand the symmetric matrix $D_{0}$ is given.  As usual, we to find the maximum and minimum\nvalues of $\\sigma_{u}^{2}$, we should look at the partial derivatives of $\\sigma_{u}^{2}$ with\nrespect to the variables $u_{i}$ and set them to zero.  Here, however, there is a catch -- we\nwant to restrict $u$ to being a unit vector, with $u\\cdot u =\\sum u_{i}^2=1$.\n\nSo this is a *constrained optimization problem*:\n\n- Find extreme values of the function\n$$\n\\sigma_{u}^{2} = u^{\\intercal}D_{0}u\n$$\n- Subject to the constraint $\\|u\\|^2 = u\\cdot u=1$ (or $u\\cdot u-1=0$)\n\nWe will use the technique of *Lagrange Multipliers* to solve\nsuch a problem.  \n\nTo apply this method, we introduce the function\n\n$$\nS(u, \\lambda) = u^{\\intercal}D_{0}u - \\lambda(u\\cdot u -1)\n$${#eq-lagrange}\n\nThen we compute the gradient\n\n$$\n\\nabla S = \\left[\\begin{matrix} \\frac{\\partial S}{\\partial u_{1}} \\\\ \\vdots \\\\ \\frac{\\partial S}{\\partial u_{k}} \\\\ \\frac{\\partial S}{\\partial \\lambda}\\end{matrix}\\right]\n$${#eq-lagrangegradient}\n\nand solve the system of equations $\\nabla S=0$.  Here we have written the gradient as a column vector for reasons that will become\nclearer shortly.\n\nComputing all of these partial derivatives looks messy, but actually if we take advantage of matrix algebra it's not too bad.\nThe following two lemmas explain how to do this.\n\n**Lemma**: Let $M$ be a $N\\times k$ matrix with constant coefficients and let $u$ be a $k\\times 1$ column vector whose entries are $u_1,\\ldots u_{k}$.\nThe function $F(u) = Mu$ is a linear map from $\\mathbf{R}^{k}\\to\\mathbf{R}^{N}$.  Its (total) derivative is a linear map between the\nsame vector spaces, and satisfies\n$$\nD(F)(v) = Mv\n$$\nfor any $k\\times 1$ vector $v$.   If $u$ is a $1\\times N$ matrix, and $G(u) = uM$, then\n$$\nD(G)(v) = vM\n$$\n\nfor any $1\\times N$ vector $v$. (This is the matrix version of the derivative rule that $\\frac{d}{dx}(ax)=a$ for a constant $a$.)  \n\n**Proof:**  Since $F:\\mathbf{R}^{k}\\to\\mathbf{R}^{N}$, we can write out $F$ in more traditional function notation\nas\n$$\nF(u) = (F_{1}(u_1,\\ldots, u_k), \\ldots, F_{N}(u_1,\\ldots, u_{k})\n$$\nwhere\n$$\nF_{i}(u_1,\\ldots u_k) = \\sum_{j=1}^{k} m_{ij}u_{j}.\n$$\nThus $\\frac{\\partial F_{i}}{\\partial u_{j}} = m_{ij}$.  The total derivative $D(F)$ is the linear map with matrix\n$$\nD(F)_{ij} = \\frac{\\partial F_{i}}{\\partial u_{j}} = m_{ij}\n$$\nand so $D(F)=M$.\n\nThe other result is proved the same way.\n\n**Lemma**:  Let $D$ be a symmetric $k\\times k$ matrix with constant entries and let $u$ be an $k\\times 1$ column vector of variables\n$u_{1},\\ldots, u_{k}$.  Let $F:\\mathbf{R}^{k}\\to R$ be the function $F(u) = u^{\\intercal}Du$.  Then the  gradient $\\nabla_{u} F$ is a\nvector field -- that is, a vector-valued function of $u$, and is given by the formula\n$$\n\\nabla_{u} F = 2Du\n$$\n\n**Proof:** Let $d_{ij}$ be the $i,j$ entry of $D$.  We can write out the function $F$ to obtain\n$$\nF(u_1,\\ldots, u_{k}) = \\sum_{i=1}^{k} \\sum_{j=1}^{k} u_i d_{ij} u_j.\n$$\nNow $\\frac{\\partial F}{\\partial u_{i}}$ is going to pick out only terms where $u_{i}$ appears, yielding:\n$$\n\\frac{\\partial F}{\\partial u_{i}} = \\sum_{j=1}^{k} d_{ij}u_{j} + \\sum_{j=1}^{k} u_{j}d_{ji}\n$$\nHere the first sum catches all of the terms where the first \"u\" is $u_{i}$; and the second sum catches all\nthe terms where the second \"u\" is $u_{i}$.  The diagonal terms $u_{i}^2d_{ii}$ contribute once to each sum,\nwhich is consistent with the rule that the derivative of $u_{i}^2d_{ii} = 2u_{i}d_{ii}$.\nTo finish the proof, notice that\n$$\n\\sum_{j=1}^{k} u_{j}d_{ji} = \\sum_{j=1}^{k} d_{ij}u_{j}\n$$\nsince $D$ is symmetric, so in fact the two terms are the same\nThus\n$$\n\\df{u_{i}}F = 2\\sum_{j=1}^{k} d_{ij}u_{j}\n$$\nBut the right hand side of this equation is twice the $i^{th}$ entry of $Du$, so\nputting the results together we get\n$$\n\\nabla_{u}F = \\left[\\begin{matrix} \\frac{\\partial F}{\\partial u_{1}} \\\\ \\vdots \\\\ \\frac{\\partial F}{\\partial u_{k}}\\end{matrix}\\right] = 2Du.\n$$\n\nThe following theorem puts all of this work together to reduce our questions about how variance changes with direction.\n\n### Critical values of the variance {#sec-critvals}\n\n**Theorem:** The critical values of the variance $\\sigma_{u}^2$, as $u$ varies over unit vectors in $\\mathbf{R}^{N}$, are the eigenvalues\n$\\lambda_{1},\\ldots,\\lambda_{k}$ of the covariance matrix $D$, and if $e_{i}$ is a unit eigenvector corresponding to $\\lambda_{i}$,\nthen $\\sigma_{e_{i}}^2 = \\lambda_{i}$.  \n\n**Proof:**\nRecall that we introduced the Lagrange function $S(u,\\lambda)$, whose critical points give us the solutions to our constrained optimization problem.\nAs we said in @eq-lagrange:\n$$\nS(u,\\lambda) = u^{\\intercal}D_{0}u - \\lambda(u\\cdot u - 1) = u^{\\intercal}D_{0}u -\\lambda(u\\cdot u) + \\lambda\n$$\nNow apply our Matrix calculus lemmas.  First, let's treat $\\lambda$ as a constant and focus on the $u$ variables.  We can write\n$u\\cdot u = u^{\\intercal} I_{N} u$ where $I_{N}$ is the identity matrix to compute:\n$$\n\\nabla_{u} S = 2D_{0}u -2\\lambda u\n$$\nFor $\\lambda$ we have\n$$\n\\df{\\lambda}S = -u\\cdot u +1.\n$$\nThe critical points occur when\n$$\n\\nabla_{u} S = 2(D_{0}-\\lambda)u = 0\n$$\nand\n$$\n\\df{\\lambda}S = 1-u\\cdot u = 0\n$$\nThe first equation says that $\\lambda$ must be an eigenvalue, and $u$ an eigenvector:\n$$\nD_{0}u = \\lambda u\n$$\nwhile the second says $u$ must be a unit vector $u\\cdot u=\\|u\\|^2=1$.    The second part of the result follows from the fact that\nif $e_{i}$ is a unit eigenvector with eigenvalue $\\lambda_{i}$ then\n$$\n\\sigma_{e_{i}}^2 = e_{i}^{\\intercal}D_{0}e_{i} = \\lambda_{i}\\|e_{i}\\|^2=\\lambda_{i}.\n$$\n\nTo really make this result pay off, we need to recall some key facts about the eigenvalues and\neigenvectors of symmetric matrices.  Because these facts are so central to this result, and\nto other applications throughout machine learning and mathematics generally, we provide\nproofs in @sec-spectraltheorem.\n\n\\newpage\n\n-----------------------------------------------------------------------\nSummary                                                                \n-----------------------------------------------------------------------\n1. All of the eigenvalues $\\lambda_{1},\\ldots, \\lambda_{l}$ of  $D$ are real.\nIf $u^{\\intercal}Du\\ge 0$ for all $u\\in\\mathbf{R}^{k}$, then all eigenvalues $\\lambda_{i}$ are non-negative.  In the latter case we say that $D$ is *positive semi-definite.*\n\n2. If $v$ is an eigenvector for $D$ with eigenvalue $\\lambda$, and $w$ is an eigenvector with a different eigenvalue  $\\lambda'$, then $v$ and $w$ are orthogonal: $v\\cdot w = 0$.\n\n3. There is an orthonormal basis $u_{1},\\ldots, u_{k}$  of $\\mathbf{R}^{k}$ made up of eigenvectors of \n$D$ corresponding to the eigenvalues $\\lambda_{i}$.\n\n4. Let $\\Lambda$ be the diagonal matrix with entries $\\lambda_{1},\\ldots, \\lambda_{N}$ and let $P$ \nbe the matrix whose columns are made up of the vectors $u_{i}$.  Then $D = P\\Lambda P^{\\intercal}.$\n------------------------------------------------------------------------\n\n: Properties of Eigenvalues of Real Symmetric Matrices {#tbl-symmmat}\n\nIf we combine our theorem on the critical values with the spectral theorem we get a complete picture.  Let $D_{0}$ be the covariance matrix of our data.\nSince\n$$\n\\sigma_{u}^2 = u^{\\intercal}D_{0}u\\ge 0 \\hbox{(it's a sum of squares)}\n$$\nwe know that the eigenvalues $\\lambda_{1}\\ge\\lambda_{2}\\ge \\cdots \\ge \\lambda_{k}\\ge 0$ are all nonnegative.  Choose a corresponding sequence\n$u_{1},\\ldots u_{k}$ of orthogonal eigenvectors where all $\\|u_{i}\\|^2=1$. Since the $u_{i}$ form a basis of $\\mathbf{R}^{N}$, any score is a\nlinear combination of the $u_{i}$:\n$$\nS = \\sum_{i=1}^{k} a_{i}u_{i}.\n$$\nSince $u_{i}^{\\intercal}D_{0}u_{j} = \\lambda_{j}u_{i}^{\\intercal}u_{j} = 0$ unless $i=j$, in which case it is $\\lambda_{i}$, we can compute\n$$\n\\sigma_{S}^2 = \\sum_{i=1}^{k} \\lambda_{i}a_{i}^2,\n$$\nand $\\|S\\|^2=\\sum_{i=1}^{k} a_{i}^2$ since the $u_{i}$ are an orthonormal set. So in these coordinates, our optimization problem is:\n\n- maximize $\\sum \\lambda_{i}a_{i}^2$\n- subject to the constraint $\\sum a_{i}^2 = 1$.\n\nWe don't need any fancy math to see that the maximum happens when $a_{1}=1$ and the other $a_{j}=0$, and in that case, the maximum is $\\lambda_{1}$. (If $\\lambda_{1}$\noccurs more than once, there may be a whole subspace of directions where the variance is maximal).  Similarly, the minimum value is $\\lambda_{k}$ and occurs when $a_{k}=1$ and the others are zero.  \n\n### Subspaces of extremal variance {#sec-subspaces}\n\nWe can generalize the idea of the variance of our data in a particular direction to a higher dimensional version of *total variance* in a subspace.\nSuppose that  $E$ is a subspace of $\\mathbf{R}^{k}$ and $U$ is a matrix whose columns span $E$ -- the columns of $U$ are\nthe weights of a family of scores that span $E$.  The values of these scores are $XU$ and the covariance matrix of this projected data\nis\n$$\\frac{1}{N}U^{\\intercal}X^{\\intercal}XU=U^{\\intercal}D_{0}U.$$.\n\nFinally, the *total variance* $\\sigma_{E}^2$ of the data projected into $E$ is the sum of the diagonal entries of the matrix\n\n$$\n\\sigma^2_{E} = \\mathop{trace}(U^{\\intercal}D_{0}U)\n$$\n\nJust as the variance in a given direction $u$ depends on the scaling of $u$, the variance in a subspace depends on the scaling of the columns\nof $U$.  To normalize this scaling, we assume that the columns of $U$ are an orthonormal basis of the subspace $E$.  \n\nNow we can generalize the question asked in @sec-extremalvariance by seeking, not just a vector $u$ pointing in the direction\nof the extremal variance, but instead the *subspace* $U_{s}$ of dimension $s$ with the property that the total variance of\nthe projection of the data into $U_{s}$ is maximal compared to its projection into other subspaces of that dimension.\nThis is called a *subspace of extremal variance.*\n\nTo make this concrete, suppose we consider a subspace $E$ of $\\mathbf{R}^{k}$ of dimension $t$ with basis $w_{1},\\ldots, w_{t}$.\nComplete this to a basis $w_{1},\\ldots, w_{t},w_{t+1},\\ldots, w_{k}$ of $\\mathbf{R}^{k}$ and then apply the Gram Schmidt Process\n(see @sec-gsprocess) to find an orthonormal basis $w'_{1},\\ldots,w'_{s},w'_{s+1},\\ldots, w'_{k}$ where the $w'_{1},\\ldots, w'_{t}$\nare an orthonormal basis for $E$.  Let $W$ be the $k\\times t$ matrix whose columns are the $w'_{i}$ for $i=1,\\ldots,t$.  The\nrows of the matrix $X_{0}W$ given the coordinates of the projection of each sample into the subspace $E$ expressed in terms\nof the scores corresponding to these vectors $w'_{i}$.  The total variance of these projections is\n\n$$\n\\sigma_{E}^2 = \\sum_{i=1}^{t} \\|X_{0}w'_{i}\\|^2 = \\sum_{i=1}^{t} (w'_{i})^{\\intercal}X_{0}^{\\intercal}X_{0}w'_{i}  = \\sum_{i=1}^{t} (w'_{i})^{\\intercal}D_{0}w'_{i}\n$$\n\nIf we want to maximize this, we have the constrained optimization problem of finding $w'_{1},\\ldots, w'_{t}$ so that\n\n- $\\sum_{i=1}^{t} (w'_{i})^{\\intercal}D_{0}w'_{i}$ is maximal\n- subject to the constraint that each $w_{i}$ has $\\|w'_{i}\\|^2=1$,\n- and that the $w'_{i}$ are orthogonal, meaning $w'_{i}\\cdot w'_{j}=0$ for $i\\not=j$,\n- and that the $w'_{i}$ are linearly independent.\n\nThen the span $E$ of these $w'_{i}$ is subspace of extremal variance.\n\n**Theorem:** A $t$-dimensional subspace $E$ is a subspace of extremal variance if and only if it is spanned by $t$ orthonormal eigenvectors of the matrix $D_{0}$ corresponding\nto the $t$ largest eigenvalues for $D_{0}$.\n\n**Proof:** We can approach this problem using Lagrange multipliers and matrix calculus if we are careful.  Our unknown is  $k\\times t$ matrix $W$ whose columns\nare the  $t$ (unknown) vectors $w'_{i}$.  The objective function that we are seeking to maximize is\n$$\nF = \\mathop{trace}(W^{\\intercal}D_{0}W) = \\sum_{i=1}^{t} (w'_{i})^{\\intercal}D_{0}w_{i}.\n$$\nThe constraints are the requirements that $\\|w'_{i}\\|^2=1$ and $w'_{i}\\cdot w'_{j}=0$ if $i\\not=j$.  If we introduction a matrix of lagrange multipliers\n$\\Lambda=(\\lambda_{ij})$, where $\\lambda_{ij}$ is the multiplier that goes with the the first of these constraints when $i=j$, and the second when $i\\not=j$,\nwe can express our Lagrange function as:\n$$\nS(W,\\Lambda) = \\mathop{trace}(W^{\\intercal}D_{0}W) - (W^{\\intercal}W-I)\\Lambda\n$$\nwhere $I$ is the $t\\times t$ identity matrix.\n\nTaking the derivatives with respect to the entries of $W$ and of $\\Lambda$ yields the following two equations:\n\\begin{align*}\nD_{0}W &= W\\Lambda \\\\\nW^{\\intercal}W &= I \\\\\n\\end{align*}\n\nThe first of these equations says that the space $E$ spanned by the columns of $W$ is *invariant* under $D_{0}$, while the second says that the\ncolumns of $W$ form an orthonormal basis.  \n\nLet's assume for the moment that we have a matrix $W$ that satisfies these conditions.  \nThen it must be the case that $\\Lambda$ is a symmetric, real valued $t\\times t$ matrix, since\n$$\nW^{\\intercal}D_{0}W = W^{\\intercal}W\\Lambda = \\Lambda.\n$$\nand the matrix on the left is symmetric.\n\nBy the properties of real symmetric matrices (the spectral theorem), there are orthonormal\nvectors $q_{1},\\ldots q_{t}$ that are eigenvectors of $\\Lambda$ with corresponding eigenvalues $\\tau_{i}$.\nIf we let $Q$ be the matrix whose columns are the vectors $q_{i}$ and let $T$ be the diagonal $t\\times t$ matrix whose entries are the $\\tau_{i}$, we have\n$$\n\\Lambda Q = QT.\n$$\n\nIf we go back to our original equations, we see that if $W$ exists such that $DW=W\\Lambda$,\nthen there is a matrix $Q$ with orthonormal columns and a diagonal matrix $T$ such that\n$$\nD_{0}WQ = W\\Lambda Q = W Q T.\n$$\nIn other words,\n$WQ$ is a matrix whose columns are eigenvectors of $D_{0}$ with eigenvalues $\\tau_{i}$ for $i=1,\\ldots, t$.\n\nThus we see how to construct an invariant subspace $E$ and a solution matrix $W$.\nSuch an  $E$ is spanned by $t$ orthonormal eigenvectors $q_{i}$\nwith eigenvalues $\\tau_{i}$\nof $D_{0}$; and $W$ is is the matrix  whose columns are the $q_{i}$. Further, in that case,\nthe total variance associated to $E$ is the sum of the eigenvalues $\\tau_{i}$; to make this as large as possible, we should choose our eigenvectors\nto correspond to $t$ of the largest eigenvalues of $D_{0}$. This concludes the proof.\n\n### Definition of Principal Components\n\n**Definition:** The orthonormal unit eigenvectors $u_{i}$ for $D_{0}$ are the *principal directions* or *principal components* for the data $X_{0}$.  \n\n**Theorem:** The maximum\nvariance occurs in the principal direction(s) associated to the largest eigenvalue, and the minimum variance in the principal direction(s) associated with the smallest one. The covariance between scores in  principal directions associatedwith different eigenvalues is zero.\n\nAt this point, the picture in @fig-pcaprincipal makes sense -- the red and green dashed lines are the principal directions,\nthey are orthogonal to one another, and the point in the directions where the data is most (and least) \"spread out.\"\n\n**Proof:** The statement about the largest and smallest eigenvalues is proved at the very end of the last section.\nThe covariance of two scores corresponding to different eigenvectors $u_{i}$ and $u_{j}$ is\n$$u_{i}^{\\intercal}D_{0}u_{j} = \\lambda_{j}(u_{i}\\cdot u_{j}) = 0$$ since the $u_{i}$ and $u_{j}$ are\northogonal.\n\nSometimes the results above are presented in a slightly different form, and may be referred to, in part, as Rayleigh's theorem.\n\n**Corollary:** (Rayleigh's Theorem)  Let $D$ be a real symmetric matrix and let\n$$\nH(v) = \\max_{v\\not = 0}\\frac{v^{\\intercal}Dv}{v^{\\intercal}v}.\n$$\nThen $H(v)$ is the largest eigenvalue of $D$. (Similarly, if we replace $\\max$ by $\\min$, then the minimum\nis the least eigenvalue).\n\n**Proof:** The maximum of the function $H(v)$ is the solution to the same optimization problem that we considered above.\n\n**Exercises.**\n\n1. Prove that the two expressions for $\\sigma_{X}^2$ given in @eq-variance are the same.\n\n2. Prove that the covariance matrix is as described in the proposition in  @sec-covarmat.\n\n3. Let $X_{0}$  be a $k\\times N$ matrix with entries $x_{ij}$ for $1\\le i\\le k$ and $1\\le j\\le N$.\nIf a linear score is defined by the constants $a_{1},\\ldots a_{N}$, check that equation\n@eq-linearscore holds as claimed.\n\n4. Why is it important to use a unit vector when computing the variance of $X_{0}$ in the direction\nof $u$?  Suppose $v=\\lambda u$ where $u$ is a unit vector and $\\lambda>0$ is a constant.  Let\n$S'$ be the score $X_{0}v$.  How is the variance of $S'$ related to that of $S=X_{0}u$?\n\n## Dimensionality Reduction via Principal Components\n\nThe principal components associated with a dataset separate out directions in the feature space\nin which the data is most (or least) variable.  One of the main applications of this information\nis to enable us to take data with a great many features -- a set of points in a high dimensional\nspace -- and, by focusing  our attention on the scores corresponding to the principal directions,\ncapture most of the information in the data in a much lower dimensional setting.\n\nTo illustrate how this is done, let $X$ be a $N\\times k$ data matrix, let $X_{0}$ be its centered\nversion, and let $D_{0} = \\frac{1}{N}X_{0}^{\\intercal}X$ be the associated covariance matrix.\n\nApply the spectral theorem  (proved in @sec-spectraltheorem)\nto the covariance matrix to obtain eigenvalues $\\lambda_{1}\\ge \\lambda_{2}\\ge\\cdots \\lambda_{k}\\ge 0$\nand associated eigenvectors $u_{1},\\ldots, u_{k}$.  The scores $S_{i}=X_{0}u_{i}$\ngive the values of the data in the principal  directions.  The variance of $S_{i}$ is $\\lambda_{i}$.\n\nNow choose a number $t<k$ and consider the vectors $S_{1},\\ldots, S_{t}$.  The $j^{th}$ entry in $S_{i}$ is\nthe value of the score $S_{i}$ for the $j^{th}$ data point.  Because $S_{1},\\ldots, S_{t}$ capture\nthe most significant variability in the original data, we can learn a lot about our data by\nconsidering just these $t$ features of the data, instead of needing all $N$.  \n\nTo illustrate, let's look at an example.  We begin with a synthetic dataset $X_{0}$ which has\n$200$ samples and $15$ features.  The data (some of it) for some of the samples is shown in\n@tbl-rawdata.\n\n\n\n          f-0      f-1       f-2      f-3      f-4      \\... f-10     f-11     f-12     f-13     f-14\n  ------- -------- --------- -------- -------- -------- ---- -------- -------- -------- -------- --------\n  s-0         1.18     -0.41     2.02     0.44     2.24  \\...    0.32     0.95     0.88     1.10     0.89\n  s-1         0.74      0.58     1.54     0.23     2.05  \\...    0.99     1.14     1.56     0.99     0.59\n  \\...    \\...     \\...      \\...     \\...     \\...      \\...\\...     \\...     \\...     \\...     \\...\n  s-198       1.04      2.02     1.44   0.40       1.33  \\...    0.62     0.62     0.54     1.96     0.04\n  s-199       0.92      2.09     1.58   1.19       1.17  \\...    0.42     0.85     0.83     2.22     0.90\n\n: Simulated Data for PCA Analysis {#tbl-rawdata}\n\n\n\nThe full dataset is a  $200\\times 15$ matrix;\nit has $3000$ numbers in it and we're not really equipped to make sense of it.  We could try\nsome graphing -- for example, @fig-features shows a scatter plot of two of the features plotted against each other.\n\n![Scatter Plot of Two Features](img/features.png){#fig-features width=50%}\n\nUnfortunately there's not much to see in  @fig-features -- just a blob -- because the individual features of the data\ndon't tell us much in isolation, whatever structure there is in this data arises out of the relationship\nbetween different features.\n\nIn @fig-densitygrid we show a \"density grid\" plot of the data.  The graph in position $i,j$\nshows a scatter plot of the $i^{th}$ and $j^{th}$ columns of the data, except in the diagonal\npositions, where in position $i,i$ we plot a histogram of column $i$.  There's not much structure\nvisible; it is a lot of blobs.\n\n![Density Grid Plot of All Features](img/density.png){#fig-densitygrid width=50%}\n\nSo let's apply the theory of principal components.  We use a software package to compute the eigenvalues\nand eigenvectors of the matrix $D_{0}$.  The $15$  eigenvalues $\\lambda_{1}\\ge \\cdots \\ge \\lambda_{15}$\nare plotted, in descending order, in @fig-eigenvalues .\n\n![Eigenvalues of the Covariance Matrix](img/eigenvalues.png){#fig-eigenvalues width=50%}\n\nThis plot shows that the first $4$ eigenvalues are relatively large, while the remaining $11$\nare smaller and not much different from each other.  We interpret this as saying that *most of the\nvariation in the data is accounted for by the first four principal components.*  We can even\nmake this quantitative.  The *total variance* of the data is the sum of the eigenvalues of the covariance\nmatrix -- the trace of $D_{0}$ -- and in this example that sum is around $5$.  The sum of the first\n$4$ eigenvalues is about $4$, so the first four eignvalues account for about $4/5$ of the total variance,\nor about $80\\%$ of the variation of the data.\n\nNow let's focus in on the two largest eigenvalues $\\lambda_{1}$ and $\\lambda_{2}$\nand their corresponding eigenvectors $u_{1}$ and $u_{2}$.  The $200\\times 1$ column vectors\n$S_{1}=X_{0}u_{1}$ and $S_{2}=X_{0}u_{2}$ are the values of the scores associated with these\ntwo eigenvectors.  So for each data point (each row of $X_{0}$) we have two values (the corresponding\nentries of $S_{1}$ and $S_{2}$.)  In @fig-principalvalues we show a scatter plot of these scores.\n\n![Scatter Plot of Scores in the First Two Principal Directions](img/pcadimred.png){#fig-principalvalues width=50%}\n\nNotice that suddenly some structure emerges in our data!  We can see that the 200 points are separated into\nfive clusters, distinguished by the values of their scores!  This ability to find hidden structure in complicated\ndata, is one of the most important applications of principal components.\n\nIf we were dealing with real data, we would now want to investigate the different groups of points to see\nif we can understand what characteristics the principal components have identified.\n\n### Loadings\n\nThere's one last piece of the PCA puzzle that we are going to investigate.  In @fig-principalvalues, we plotted\nour data points in the coordinates given by the first two principal components.  In geometric terms,\nwe took the cloud of $200$ points in $\\mathbf{R}^{15}$ given by the rows of $X_{0}$ and projected those\npoints into the two dimensional plane spanned by the eigenvectors $u_{1}$ and $u_{2}$, and then plotted the\ndistribution of the points in that plane.  \n\nMore generally, suppose we take our dataset $X_{0}$ and consider the first $t$ principal components corresponding\nto the eigenvectors $u_{1},\\ldots, u_{t}$.  The projection of the data into the space spanned by these eigenvectors\nis the represented by the $S = k\\times t$ matrix $X_{0}U$ where $U$ is the $k\\times t$ matrix whose columns are the eigenvectors $u_{i}$.\nEach row of $S$ gives the values of the score arising from $u_{i}$ in the $i^{th}$ column for $i=1,\\ldots, t$.\n\nThe remaining question that we wish to consider is: how can we see some evidence of the original features in subspace?\nWe can answer this by imagining that we had an artificial sample $x$ that has a measurement of $1$ for the $i^{th}$\nfeature and a measurement of zero for all the other features.  The corresponding point is represented by a $1\\times k$\nrow vector with a $1$ in position $i$.  The projection of this synthetic sample into the span of the first\n$t$ principal components is the $1\\times t$ vector $xU$.  Notice, however, that $xU$ is just the $i^{th}$ row\nof the matrix $U$.   This vector in the space spanned by the $u_{i}$ is called the \"loading\" of the $i^{th}$ feature\nin the principal components.\n\nThis is illustrated in @fig-loadings, which shows a line along the direction of the  loading corresponding to the each feature\nadded to the scatter plot of the data in the plane spanned by the first two principal components.  One observation one can make\nis that some of the features are more \"left to right\", like features $7$ and $8$, while others are more \"top to bottom\", like $6$.\nSo points that lie on the left side of the plot have smaller values of features $7$ and $8$, while those at the top of the\nplot have larger values of feature $6$.\n\n![Loadings in the Principal Component Plane](img/loading.png){#fig-loadings width=50%}\n\n\n### The singular value decomposition {#sec-svd}\n\nThe singular value decomposition is a slightly more detailed way of looking at principal components.  Let\n$\\Lambda$ be the diagonal matrix of eigenvalues of $D_{0}$ and let $P$ be the $k\\times k$ orthogonal\nmatrix whose columns are the principal components.  Then we have\n$$\nD_{0} =\\frac{1}{N}X_{0}^{\\intercal}X_{0}= P\\Lambda P^{\\intercal}.\n$$\nConsider the $N\\times k$ matrix\n$$\nX_{0}P = A.\n$$\n\nAs we saw in the previous section, the columns of $A$ give the projection of the data into\nthe $k$ principal directions. \nThen\n\n$$A^{\\intercal}A=P^{\\intercal}X_{0}^{\\intercal}X_{0}P=N\\Lambda.$$\n\nIn other words,\nthe columns of $A$ are orthogonal and the diagonal entries of $A^{\\intercal}A$\nare $N$ times the variance of the data in the various principal directions.\n\nNow we are going to tinker with the matrix $A$ in order to make an $N\\times N$ orthogonal matrix.\nThe first modification we make is to normalize the columns of $A$ so that they have length $1$.\nWe do this by setting\n$$\nA_{1} = A(N\\Lambda)^{-1/2}.\n$$\nThen $A_{1}^{\\intercal}A_{1}$ is the identity, so the columns of $A_{1}$ are orthonormal. Here we\nare assuming that the eigenvalues of $D_{0}$ are nonzero -- this isn't strictly necessary, and we could\nwork around this, but for simplicity we will assume it's true.  It amounts to the assumption that the\nindependent variables are not linearly related, as we've seen before.\n\nThe second modification is to extend $A_{1}$ to an $N\\times N$ matrix.\nThe $k$ columns of $A_1$ span only a $k$-dimensional subspace of the $N$-dimensional space where the feature\nvectors lie.  \nComplete the subspace by finding an orthogonal complement to it -- that is, find $N-k$ mutually orthogonal\nunit vectors all orthogonal to the column space of $A_1$.  By adding these vectors to $A$ as columns,\ncreate an extended $N\\times N$ matrix $\\tilde{A}_1$ which is orthogonal.\n\nNotice that\n$\\tilde{A}_{1}^{\\intercal}A$ is an $N\\times k$ matrix whose upper $k\\times k$ block is $(N\\Lambda)^{1/2}$\nand whose final $N-k$ rows are all zero.  We call this matrix $\\tilde{\\Lambda}$.\n\nTo maintain consistency with the traditional formulation, we let $U=\\tilde{A}_{1}^{\\intercal}$ and\nthen we have the following proposition. \n\n**Proposition:** We have a factorization\n$$\nX_{0} = U\\tilde{\\Lambda}P^{\\intercal}\n$$ {#eq-svd}\nwhere $U$ and $P$ are orthogonal matrices of size $N\\times N$ and $k\\times k$ respectively, and $\\tilde{\\Lambda}$\nis an $N\\times k$ diagonal matrix. This is called the \"singular value decomposition\" of $X_{0}$,\nand the entries of $\\tilde{\\Lambda}$ are called the singular values.  If we let $u_1,\\ldots, u_k$\nbe the first $k$ rows of $U$, then the $k$ column vectors $u_{i}^{\\intercal}$ are an orthonormal\nbasis for the feature space spanned by the columns of $X_{0}$, and they point in the \"principal directions\"\nfor the data matrix $X_{0}$. \n\nIn this section we take a slight detour and apply what we've learned about the covariance matrix,\nprincipal components, and the singular value decomposition to the original problem of linear regression that\nwe studied in Chapter 1.  \n\nIn this setting, in addition to our centered data matrix $X_{0}$, we have a vector $Y$ of target\nvalues and we find the \"best\" approximation\n$$\n\\hat{Y} = X_{0}M\n$$\nusing the least squares method.  As we showed in Chapter 1, the optimum $M$ is found as\n$$\nM = (X_{0}^{\\intercal}X_{0})^{-1}X^{\\intercal}Y = ND_{0}^{-1}X_0^{\\intercal}Y\n$$\nand the predicted values $\\hat{Y}$ are\n$$\n\\hat{Y} = NX_{0}D_{0}^{-1}X_0^{\\intercal}Y.\n$$\n\nGeometrically, we understood this process as defining $\\hat{Y}$ to be the orthogonal projection of $Y$ into the subspace spanned by the columns of $X_{0}$.\n\nLet's use the decomposition (see @eq-svd ) $X_{0}=U\\tilde{\\Lambda}P^{\\intercal}$ in this formula. First, notice that \n$$\nX_{0}^{\\intercal}X_{0}= P\\tilde{\\Lambda}^{\\intercal}U^{\\intercal}U\\tilde{\\Lambda}P^{\\intercal} = P\\tilde{\\Lambda}^{\\intercal}\\tilde{\\Lambda}P^{\\intercal}.\n$$\n\nThe middle term $\\tilde{\\Lambda}^{\\intercal}\\tilde{\\Lambda}$ is the $k\\times k$ matrix $\\Lambda$ whose diagonal entries\nare $N\\lambda_{i}$ where $\\lambda_{i}$ are the eigenvalues of the covariance matrix $D_{0}$. Assuming\nthese are all nonzero (which is tantamount to the assumption that the covariance matrix is invertible),\nwe obtain\n$$\n\\hat{Y} = NU\\tilde{\\Lambda}P^{\\intercal}P\\Lambda^{-1}P^{\\intercal}P\\tilde{\\Lambda}^{\\intercal}U^{\\intercal}Y.\n$$\nThere is a lot of cancellation here, and in the end what's left is\n$$\n\\hat{Y}=UEU^{\\intercal}Y\n$$\nwhere $E$ is and $N\\times N$ matrix whose upper $k\\times k$ block is the identity and whose remaining\nentries are zero.  Rearranging a bit more we have\n$$\nU^{\\intercal}\\hat{Y} = EU^{\\intercal}Y.\n$$\n\nTo unpack this equation, let $u_{1},\\ldots, u_{N}$ be the rows of the matrix $U$.  Since $U$\nis an orthogonal matrix, the column vectors $u_{i}^{\\intercal}$ are an orthonormal basis for\nthe $N$ dimensional space where the columns of $X_{0}$ lie.  We can write the target vector $Y$\n$$\nY = \\sum_{j=1}^{N} (u_{j}\\cdot Y)u_{j}^{\\intercal}.\n$$\n\nThen the projection $\\hat{Y}$ of $Y$ into the subspace spanned by the data is obtained by dropping the last $N-k$ terms in the sum:\n\n$$\n\\hat{Y}=\\sum_{j=1}^{k} (u_{j}\\cdot Y)u_{j}^{\\intercal}\n$$\n\n## Eigenvalues and Eigenvectors of Real Symmetric Matrices (The Spectral Theorem) {#sec-spectraltheorem}\n\nNow that we've shown how to apply the theory of eigenvalues and eigenvectors of symmetric matrices\nto extract principal directions from data, and to use those principal directions to find structure,\nwe will give a proof of the properties that we summarized in @tbl-symmmat.\n\nA key tool in the proof is the Gram-Schmidt\northogonalization process.\n\n### Gram-Schmidt {#sec-gsprocess}\n\n**Proposition (Gram-Schmidt Process):** Let $w_{1},\\ldots, w_{k}$ be a collection of linearly independent vectors\nin $\\mathbf{R}^{N}$ and let $W$ be the span of the $w_{i}$.  Let $u_{1} = w_{1}$ and let\n$$\nu_{i} = w_{i} - \\sum_{j=1}^{i-1} \\frac{w_{i}\\cdot u_{j}}{u_{j}\\cdot u_{j}}u_{j}\n$$\nfor $i=2,\\ldots, k$.  Then\n\n- The vectors $u_{i}$ are orthogonal: $u_{i}\\cdot u_{j}=0$ unless $i=j$.\n- The vectors $u_{i}$ span $W$.\n- Each $u_{i}$ is orthogonal to the all of $w_{1},\\ldots, w_{i-1}$.\n- The vectors $u'_{i} = u_{i}/\\|u_{i}\\|$ are orthonormal.\n\n**Proof:** This is an inductive exercise, and we leave it to you to work out the details.\n\n### The spectral theorem\n\n**Theorem:** Let $D$ be a real symmetric $N\\times N$ matrix.  Then:\n\n1. All of the $N$ eigenvalues $\\lambda_1\\ge \\lambda_2\\ge \\cdots \\ge \\lambda_{N}$ are real.  If\n$u^{\\intercal}Du\\ge 0$ for all $u\\in\\mathbf{R}^{N}$, then all eigenvalues $\\lambda_{i}\\ge 0$.\n2. The matrix $D$ is diagonalizable -- that is, it has $N$ linearly independent eigenvectors.\n3. If $v$ and $w$ are eigenvectors corresponding to eigenvalues $\\lambda$ and $\\lambda'$, with $\\lambda\\not=\\lambda'$,\nthen $v$ and $w$ are orthogonal: $v\\cdot w=0$.\n4. There is an orthonormal basis $u_{1},\\ldots, u_{N}$ of $\\mathbf{R}^{N}$ made up of eigenvectors for the eigenvalues\n$\\lambda_{i}$.\n5. Let $\\Lambda$ be the diagonal matrix with entries $\\lambda_{1},\\ldots, \\lambda_{N}$ and let $P$ be the matrix\nwhose columns are made up of the eigenvectors $u_{i}$.  Then $D=P\\Lambda P^{\\intercal}$.\n\n**Proof:** Start with $1$.\nSuppose that $\\lambda$ is an eigenvalue of\n$D$.  Let $u$ be a corresponding nonzero eigenvector.  Then\n$Du=\\lambda u$ and $D\\overline{u}=\\overline{\\lambda}\\overline{u}$, where $\\overline{u}$ is the\nvector whose entries are the conjugates of the entries of $u$ (and $\\overline{D}=D$ since $D$ is real).\nNow we have\n$$\n\\overline{u}^{\\intercal}Du = \\lambda \\overline{u}\\cdot u = \\lambda\\|u\\|^2\n$$\nand\n$$\nu^{\\intercal}D\\overline{u} = \\overline{\\lambda}u\\cdot \\overline{u} = \\overline{\\lambda}\\|u\\|^2.\n$$\nBut the left hand side of both of these equations are the same (take the transpose and use the symmetry of $D$)\nso we must have $\\lambda\\|u\\|^2 = \\overline{\\lambda}\\|u\\|^2$ so $\\lambda=\\overline{\\lambda}$, meaning $\\lambda$\nis real.  \n\nIf we have the additional property that $u^{\\intercal}Du\\ge 0$ for all $u$, then in particular\n$u_{i}^{\\intercal}Du_{i} = \\lambda\\|u\\|^2\\ge 0$, and since $\\|u\\|^2> 0$ we must have $\\lambda\\ge 0$.\n\nProperty $2$ is in some ways the most critical fact. We know from the\ngeneral theory of the characteristic polynomial, and the fundamental\ntheorem of algebra, that $D$ has $N$ complex eigenvalues, although\nsome may be repeated.  However, it may not be the case that $D$ has $N$ linearly\nindependent eigenvectors -- it may not be *diagonalizable*.  So we will establish that now.\n\nA one-by-one matrix is automatically symmetric and diagonalizable.  In the $N$-dimensional case,\nwe know, at least, that $D$ has at least one eigenvector, and real one at that by part $1$,\nand this gives us a place to begin an inductive argument.  \n\nLet $v_{N}\\not=0$ be an eigenvector with eigenvalue $\\lambda$ and normalized so that $\\|v_{N}\\|^2=1$,  \nand extend this to a basis $v_{1},\\ldots v_{N}$ of $\\mathbf{R}^{N}$.\nApply the Gram-Schmidt process  to construct an orthonormal basis of $\\mathbf{R}^{N}$\n$u_{1},\\ldots, u_{N}$ so that $u_{N}=v_{N}$.  \n\nAny vector $v\\in\\mathbf{R}^{N}$ is a linear combination\n$$\nv = \\sum_{i=1}^{N} a_{i}u_{i}\n$$\nand, since the $u_{i}$ are orthonormal, the coefficients can be calculated as $a_{i}=(u_{i}\\cdot v)$.\n\nUsing this, we can find the matrix $D'$ of the linear map defined by our original matrix $D$\nin this new basis.  By definition, if $d'_{ij}$ are the entries of $D'$, then\n\n$$\nDu_{i} = \\sum_{j=1}^{N} d'_{ij} u_{j}\n$$\n\nand so\n\n$$\nd'_{ij} = u_{j}\\cdot Du_{i} = u_{j}^{\\intercal}Du_{i}.\n$$\n\nSince $D$ is symmetric, $u_{j}^{\\intercal}Du_{i} =u_{i}^{\\intercal}Du_{j}$ and so $d'_{ij}=d'_{ji}$.\nIn other words, the matrix $D'$ is still symmetric.  Furthermore,\n\n$$\nd'_{Ni} = u_{i}\\cdot Du_{N} = u_{i}\\cdot \\lambda u_{N} = \\lambda (u_{i}\\cdot u_{N})\n$$\n\nsince $u_{N}=v_{N}$.  Since the $u_{i}$ are an orthonormal basis, we see that\n$d'_{iN}=0$ unless $i=N$, and $d'_{NN}=\\lambda$.\n\nIn other words, the matrix $D'$ has a block form:\n$$\nD' = \\left(\\begin{matrix} *&* & \\cdots &*  & 0 \\\\ \\vdots & \\vdots & \\ddots   & \\vdots & \\vdots \\\\\n* & *& \\cdots &*  & 0 \\\\\n0 & 0 & \\cdots &0 &\\lambda \\end{matrix}\\right)\n$$\nand the block denoted by $*$'s is symmetric.  If we call that block $D_{*}$,\nthe inductive hypothesis tells us that the symmetric matrix $D_{*}$ is diagonalizable, so it has a basis of\neigenvectors $u'_{1},\\ldots, u'_{N-1}$ with eigenvalues $\\lambda_{1},\\ldots, \\lambda_{N-1}$; this gives\nus a basis for the subspace of $\\mathbf{R}^{N}$ spanned by $u_{1},\\ldots, u_{N-1}$ which, together\nwith $u_{N}$ gives us a basis of $\\mathbf{R}^{N}$ consisting of eigenvectors of $D$.\n\nThis finishes the proof of Property $2$.\n\nFor property $3$, compute\n$$\nv^{\\intercal}Dw = \\lambda'(v\\cdot w)=w^{\\intercal}Dv = \\lambda (w\\cdot v).\n$$\nSince $\\lambda\\not=\\lambda'$, we must have $v\\cdot w=0$.\n\nFor property $4$, if the eigenvalues are all distinct, this is a consequence of property $2$ -- you have\n$N$ eigenvectors, scaled to length $1$, for different eigenvalues, and by $2$ they are orthogonal.  So the\nonly complication is the case where some eigenvalues are repeated.  If $\\lambda$ occurs $r$ times, then\nyou have $r$ linearly independent vectors $u_{1},\\ldots, u_{r}$ that span the $\\lambda$ eigenspace.\nThe Gram-Schmidt process allows you to construct an orthonormal set that spans this eigenspace, and while\nthis orthonormal set isn't unique, any one of them will do.\n\nFor property $5$, let $e_{i}$ be the column vector that is zero except for a $1$ in position $i$.\nThe product $e_{j}^{\\intercal}De_{i}=d_{ij}$.  Let's write $e_{i}$ and $e_{j}$ in terms of the orthonormal\nbasis $u_{1},\\ldots u_{N}$:\n$$\ne_{i} = \\sum_{k=1}^{N} (e_{i}\\cdot u_{k})u_k \\hbox{ and } e_{j} = \\sum_{k=1}^{N}(e_{j}\\cdot u_{k})u_{k}.\n$$\nUsing this expansion, we compute $e_{j}^{\\intercal}De_{i}$ in a more complicated way:\n$$\ne_{j}^{\\intercal}De_{i} = \\sum_{r=1}^{N}\\sum_{s=1}^{N} (e_{j}\\cdot u_{r})(e_{i}\\cdot u_{s})(u_{r}^{\\intercal}Du_{s}).\n$$\nBut $u_{r}^{\\intercal}Du_{s}=\\lambda_{s}(u_{r}\\cdot u_{s})=0$ unless $r=s$, in which case it equals $\\lambda_{r}$, so\n$$\ne_{j}^{\\intercal}De_{i} = \\sum_{r=1}^{N} \\lambda_{r}(e_{j}\\cdot u_{r})(e_{i}\\cdot u_{r}).\n$$\nOn the other hand,\n$$\nP^{\\intercal}e_{i} = \\left[\\begin{matrix} (e_{i}\\cdot u_{1})\\\\ (e_{i}\\cdot u_{2})\\\\ \\vdots \\\\(e_{i}\\cdot u_{N})\\end{matrix}\\right]\n$$\nand\n$$\n\\Lambda P^{\\intercal}e_{i} = \\left[\\begin{matrix} \\lambda_{1}(e_{i}\\cdot u_{i})\\\\ \\lambda_{2}(e_{i}\\cdot u_{2})\\\\ \\vdots \\\\ \\lambda_{N}(e_{i}\\cdot u_{N})\\end{matrix}\\right]\n$$\nTherefore the $i,j$ entry of $P\\Lambda P^{\\intercal}$ is\n$$\n(e_{j}^{\\intercal}P)\\Lambda (P^{\\intercal}e_{j}) = \\sum_{r=1}^{N} \\lambda_{r}(e_{i}\\cdot u_{r})(e_{j}\\cdot u_{r}) = d_{ij}\n$$\nso the two matrices $D$ and $P\\Lambda P^{\\intercal}$ are in fact equal.\n\n**Exercises:**\n\n1. Prove the rest of the first lemma in @sec-svd.\n\n2. Prove the Gram-Schmidt Process has the claimed properties in @sec-gsprocess.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"02-pca.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":[{"text":"\\usepackage{listings}\n"},{"file":"../chapters/macros.tex"}],"output-file":"02-pca.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","documentclass":"scrreprt"},"extensions":{"book":{}}}}}