{"title":"Linear Regression","markdown":{"yaml":{"format":{"pdf":{"include-in-header":["macros.tex"]}}},"headingText":"Linear Regression","containsRefs":false,"markdown":"\n\n\n## Introduction {#sec-Intro}\n\nSuppose that we are trying to study two quantities $x$ and $y$ that we\nsuspect are related -- at least approximately -- by a linear equation\n$y=ax+b$.  Sometimes this linear relationship is predicted by\ntheoretical considerations, and sometimes it is\njust an empirical hypothesis.  \n\nFor example, if we are trying to determine the velocity of an object\ntravelling towards us at constant speed, and we measure measure the\ndistances $d_1, d_2, \\ldots, d_n$ between us and the object at a\nseries of times $t_1, t_2, \\ldots, t_n$, then since \"distance equals\nrate times time\" we have a theoretical foundation for the assumption\nthat $d=rt+b$ for some constants $r$ and $b$.  On the other hand,\nbecause of unavoidable experimental errors, we can't expect that this\nrelationship will hold exactly for the observed data; instead, we\nlikely get a graph like that shown in @fig-dvt.  We've drawn a line on\nthe plot that seems to capture the true slope (and hence velocity) of\nthe object.\n\n![Physics Experiment](img/distance-vs-time.png){#fig-dvt width=50%}\n\nOn the other hand, we might look at a graph such as\n@fig-mpg-vs-displacement, which plots the gas mileage of various car\nmodels against their engine size (displacement), and observe a general\ntrend in which bigger engines get lower mileage.  In this situation we\ncould ask for the best line of the form $y=mx+b$ that captures this\nrelationship and use that to make general conclusions without\nnecessarily having an underlying theory.\n\n![MPG vs Displacement ( @irvine )](img/mpg-vs-displacement.png){#fig-mpg-vs-displacement width=50%}\n\n## Least Squares (via Calculus) {#sec-Calculus}\n\nIn either of the two cases above, the question we face is to determine\nthe line $y=mx+b$ that \"best fits\" the data $\\{(x_i,y_i)_{i=1}^{N}\\}$.\nThe classic approach is to determine the equation of a line $y=mx+b$\nthat minimizes the \"mean squared error\":\n\n$$ MSE(m,b) = \\frac{1}{N}\\sum_{i=1}^{n} (y_i-mx_i-b)^2 $$\n\nIt's worth emphasizing that the $MSE$ is a function of two variables\n-- the slope $m$ and the intercept $b$ -- and that the data points\n$\\{(x_i,y_i)\\}$ are constants for these purposes.  Furthermore, it's a\nquadratic function in those two variables.  Since our goal is to find\n$m$ and $b$ that minimize the $MSE$, we have a Calculus problem that\nwe can solve by taking partial derivatives and setting them to zero.\n\nTo simplify the notation, let's abbreviate $MSE$ by $E$.\n\n$$\n\\begin{aligned} \\frac{\\partial E}{\\partial m} &=\n\\frac{1}{N}\\sum_{1}^{N}-2x_i(y_i-mx_i-b) \\\\ \\frac{\\partial E}{\\partial\nb} &= \\frac{1}{N}\\sum_{1}^{N}-2(y_i-mx_i-b) \\\\ \n\\end{aligned} \n$$\n\nWe set these two partial derivatives to zero, so we can drop the $-2$\nand regroup the sums to obtain two equations in two unknowns (we keep\nthe $\\frac{1}{N}$ because it is illuminating in the final result):\n\n$$\n\\begin{aligned} \\frac{1}{N}(\\sum_{i=1}^{N} x_i^2)m &+&\n\\frac{1}{N}(\\sum_{i=1}^{N} x_i)b &=& \\frac{1}{N}\\sum_{i=1}^{N} x_i y_i\n\\\\ \\frac{1}{N}(\\sum_{i=1}^{N} x_i)m &+& b &=&\n\\frac{1}{N}\\sum_{i=1}^{N} y_{i} \\\\ \\end{aligned}\n$$ {#eq-LS}\n\nIn these equations, notice that $\\frac{1}{N}\\sum_{i=1}^{N} x_i$ is the\naverage (or mean) value of the $x_i$.  Let's call this $\\overline{x}$.\nSimilarly, $\\frac{1}{N}\\sum_{i=1}^{N} y_{i}$ is the mean of the $y_i$,\nand we'll call it $\\overline{y}$.  If we further simplify the notation\nand write $S_{xx}$ for $\\frac{1}{N}\\sum_{i=1}^{N} x_i^2$ and $S_{xy}$\nfor $\\frac{1}{N}\\sum_{i=1}^{N}x_iy_i$ then we can write down a\nsolution to this system using Cramer's rule:\n\n$$ \\begin{aligned} m &=\n\\frac{S_{xy}-\\overline{x}\\overline{y}}{S_{xx}-\\overline{x}^2} \\\\ b &=\n\\frac{S_{xx}\\overline{y}-S_{xy}\\overline{x}}{S_{xx}-\\overline{x}^2} \\\\\n\\end{aligned}\n$$ {#eq-LSAnswer}\n\nwhere we must have $S_{xx}-\\overline{x}^2\\not=0$.\n\n### Exercises {#sec-CalcExercises}\n\n1. Verify that @eq-LSAnswer is in fact the solution to the system in\n@eq-LS .\n\n2. Suppose that $S_{xx}-\\overline{x}^2=0$.  What does that mean about\nthe $x_i$?  Does it make sense that the problem of finding the \"line\nof best fit\" fails in this case?\n\n## Least Squares (via Geometry) {#sec-LinAlg}\n\nIn our discussion above, we thought about our data as consisting of\n$N$ pairs $(x_i,y_i)$ corresponding to $n$ points in the $xy$-plane\n$\\mathbf{R}^2$.  Now let's turn that picture \"on its side\", and\ninstead think of our data as consisting of *two* points in\n$\\mathbf{R}^{n}$:\n\n$$ X=\\left[\\begin{matrix} x_1\\cr x_2\\cr \\vdots\\cr\nx_n\\end{matrix}\\right] \\mathrm{\\ and\\ } Y = \\left[\\begin{matrix}\ny_1\\cr y_2\\cr \\vdots\\cr y_n\\end{matrix}\\right]\n$$\n\nLet's also introduce one other vector\n\n$$ E = \\left[\\begin{matrix} 1 \\cr 1 \\cr \\vdots \\cr\n1\\end{matrix}\\right].  \n$$\n\nFirst, let's assume that $E$ and $X$ are linearly independent.  If\nnot, then $X$ is a constant vector (why?) which we already know is a\nproblem from @sec-Calculus, Exercise 2.  Therefore $E$ and $X$ span a\nplane in $\\mathbf{R}^{n}$.\n\n![Distance to A Plane](img/distance-to-plane.png){#fig-perp\nwidth=50%}\n\nNow if our data points $(x_i,y_i)$ all *did* lie on a line $y=mx+b$,\nthen the three vectors $X$, $Y$, and $E$ would be linearly dependent:\n\n$$ Y = mX + bE.  $$\n\nSince our data is only approximately linear, that's not the case.  So\ninstead we look for an approximate solution.  One way to phrase that\nis to ask:\n\n*What is the point $\\hat{Y}$ in the plane $H$ spanned by $X$ and $E$\nin $\\mathbf{R}^{n}$ which is closest to $Y$?*\n\nIf we knew this point $\\hat{Y}$, then since it lies in $H$ we would\nhave $\\hat{Y}=mX+bE$ and the coefficients $m$ and $b$ would be a\ncandidate for defining a line of best fit $y=mx+b$.  Finding the point\nin a plane closest to another point in $\\mathbf{R}^{n}$ is a geometry\nproblem that we can solve.\n\n**Proposition:** The point $\\hat{Y}$ in the plane spanned by $X$ and\n$E$ is the point such that the vector $Y-\\hat{Y}$ is perpendicular to\n$H$.\n\n**Proof:** See @fig-perp for an illustration -- perhaps you are\nalready convinced by this, but let's be careful.  $\\hat{Y}=mX+bE$ such\nthat $$ D = \\|Y-\\hat{Y}\\|^2 = \\|Y-mX-bE\\|^2 $$ is minimal.  Using some\nvector calculus, we have $$ \\frac{\\partial D}{\\partial m} =\n\\frac{\\partial}{\\partial m} (Y-mX-bE)\\cdot (Y-mX-bE) =\n-2(Y-mX-bE)\\cdot X $$ and $$ \\frac{\\partial D}{\\partial b} =\n\\frac{\\partial}{\\partial b} (Y-mX-bE)\\cdot (Y-mX-bE) =\n-2(Y-mX-bE)\\cdot E.  $$\n\nSo both derivatives are zero exactly when $\\hat{Y}=(Y-mX-bE)$ is\northogonal to both $X$ and $E$, and therefore every vector in $H$.\n\nWe also obtain equations for $m$ and $b$ just as in our first look at\nthis problem.\n\n$$ \\begin{aligned} m(X\\cdot E) &+ b(E\\cdot E) &= (Y\\cdot E) \\cr\nm(X\\cdot X) &+ b(E\\cdot X) &= (Y\\cdot X) \\cr \\end{aligned}\n$$ {#eq-LSAnswer2}\n\nWe leave it is an exercise below to check that these are the same\nequations that we obtained in @eq-LSAnswer.\n\n### Exercises\n\n1. Verify that @eq-LSAnswer and @eq-LSAnswer2 are equivalent.\n\n## The Multivariate Case (Calculus) {#sec-Multivariate-calculus}\n\nHaving worked through the problem of finding a \"line of best fit\" from\ntwo points of view, let's look at a more general problem.  We looked\nabove at a scatterplot showing the relationship between gas mileage\nand engine size (displacement).  There are other factors that might\ncontribute to gas mileage that we want to consider as well -- for\nexample:\n\n- a car that is heavy compared to its engine size may get worse\n  mileage\n- a sports car with a drive train that gives fast acceleration as\ncompared to a car with a transmission designed for long trips may have\ndifferent mileage for the same engine size.\n\nSuppose we wish to use engine displacement, vehicle weight, and\nacceleration all together to predict mileage.  Instead of looking\npoints $(x_i,y_i)$ where $x_i$ is the displacement of the $i^{th}$ car\nmodel and we try to predict a value $y$ from a corresponding $x$ as\n$y=mx+b$ -- let's look at a situation in which our measured value $y$\ndepends on multiple variables -- say displacement $d$, weight $w$, and\nacceleration $a$ with $k=3$ -- and we are trying to find the best\nlinear equation\n\n$$ \ny=m_1 d + m_2 w + m_3 a +b \n$${#eq-multivariate}\n\nBut to handle this situation more generally we need to adopt a\nconvention that will allow us to use indexed variables instead of $d$,\n$w$, and $a$.  We will use the *tidy* data convention.\n\n**Tidy Data:** A dataset is tidy if it consists of values $x_{ij}$ for\n$i=1,\\ldots,N$ and $j=1,\\ldots, k$ so that:\n\n- the row index corresponds to a *sample* -- a set of measurements\n   from a single event or item;\n- the column index corresponds to a *feature* -- a particular\n   property measured for all of the events or items.\n\nIn our case,\n\n- the *samples* are the different types of car models,\n- the *features* are the properties of those car models.  \n\nFor us, $N$ is the number of different types of cars, and $k$ is the\nnumber of properties we are considering.  Since we are looking at\ndisplacement, weight, and acceleration, we have $k=3$.\n\nSo the \"independent variables\" for a set of data that consists of $N$\nsamples, and $k$ measurements for each sample, can be represented by a\n$N\\times k$ matrix\n\n$$ X = \\left(\\begin{matrix} x_{11} & x_{12} & \\cdots & x_{1k} \\\\\nx_{21} & x_{22} & \\cdots & x_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\n\\\\ x_{N1} & x_{k2} & \\cdots & x_{Nk} \\\\ \\end{matrix}\\right)\n$$\n\nand the measured dependent variables $Y$ are a column vector \n$$ Y =\n\\left[\\begin{matrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\\end{matrix}\\right].\n$$\n\nIf $m_1,\\ldots, m_k$ are \"slopes\" associated with these properties in\n@eq-multivariate, and $b$ is the \"intercept\", then the predicted\nvalue $\\hat{Y}$ is given by a matrix equation\n\n$$ \n\\hat{Y} = X\\left[\\begin{matrix} m_1 \\\\ m_2 \\\\ \\cdots \\\\\nm_k\\end{matrix}\\right]+\\left[\\begin{matrix} 1 \\\\ 1 \\\\ \\cdots \\\\\n1\\end{matrix}\\right]b \n$$\n\nand our goal is to choose these parameters $m_i$ and $b$ to make the\nmean squared error:\n\n$$ MSE(m_1,\\ldots, m_k,b) = \\|Y-\\hat{Y}\\|^2 = \\sum_{i=1}^{N} (y_i -\n\\sum_{j=1}^{k} x_{ij}m_j -b )^2.\n$$\n\nHere we are summing over the $N$ different car models, and for each\nmodel taking the squared difference between the true mileage $y_i$ and\nthe \"predicted\" mileage $\\sum_{j=1}^{k} x_{ij}m_j +b$. We wish to\nminimize this MSE.\n\nLet's make one more simplification.  The intercept variable $b$ is\nannoying because it requires separate treatment from the $m_i$.  But\nwe can use a trick to eliminate the need for special treatment.  Let's\nadd a new feature to our data matrix (a new column) that has the\nconstant value $1$.\n\n$$ X = \\left(\\begin{matrix} x_{11} & x_{12} & \\cdots & x_{1k} & 1\\\\\nx_{21} & x_{22} & \\cdots & x_{2k} & 1\\\\ \\vdots & \\vdots & \\ddots &\n\\vdots & 1\\\\ x_{N1} & x_{k2} & \\cdots & x_{Nk} & 1\\\\\n\\end{matrix}\\right)\n$$\n\nNow our data matrix $X$ is $N\\times(k+1)$ and we can put our\n\"intercept\" $b=m_{k+1}$ into our vector of \"slopes\" $m_1, \\ldots,\nm_k,m_{k+1}$:\n\n$$ \\hat{Y} = X\\left[\\begin{matrix} m_1 \\\\ m_2 \\\\ \\cdots \\\\ m_k \\\\\nm_{k+1}\\end{matrix}\\right]\n$$\n\nand our MSE becomes\n\n$$ \nMSE(M) = \\|Y - XM\\|^2\n$$\n\nwhere\n\n$$ M=\\left[\\begin{matrix} m_1 \\\\ m_2 \\\\ \\cdots \\\\ m_k \\\\\nm_{k+1}\\end{matrix}\\right].\n$$\n\n**Remark:** Later on (see {@sec-centered}) we will see that if we\n\"center\" our features about their mean, by subtracting the average\nvalue of each column of $X$ from that column; and we also subtract the\naverage value of $Y$ from the entries of $Y$, then the $b$ that\nemerges from the least squares fit is zero.  As a result, instead of\nadding a column of $1$'s, you can change coordinates to center each\nfeature about its mean, and keep your $X$ matrix $N\\times k$.\n\nThe Calculus approach to minimizing the $MSE$ is to take its partial\nderivatives with respect to the $m_{i}$ and set them to zero.  Let's\nfirst work out the derivatives in a nice form for later.\n\n**Proposition:** The gradient of $MSE(M)=E$ is given by\n\n$$ \nnabla E = \\left[\\begin{matrix} \\df{M_1}E \\\\ \\df{M_2}E \\\\ \\vdots \\\\\n\\df{m_{M+1}}E\\end{matrix}\\right] = -2 X^{\\intercal}Y + 2\nX^{\\intercal}XM \n$${#eq-gradient}\n\nwhere $X^{\\intercal}$ is the transpose of $X$.\n\n**Proof:** First, remember that the $ij$ entry of $X^{\\intercal}$ is\nthe $ji$ entry of $X$. Also, we will use the notation $X[j,:]$ to mean\nthe $j^{th}$ row of $X$ and $X[:,i]$ to mean the $i^{th}$ column of\n$X$.  (This is copied from the Python programming language; the ':'\nmeans that index runs over all possibilities).\n\nSince $$ E = \\sum_{j=1}^{N} (Y_j-\\sum_{s=1}^{k+1} X_{js}M_{s})^2 $$ we\ncompute: $$\\begin{aligned} \\df{M_t}E &= -2\\sum_{j=1}^{N}\nX_{jt}(Y_{j}-\\sum_{s=1}^{k+1} X_{js}M_{s}) \\\\ &= -2(\\sum_{j=1}^{N}\nY_{j}X_{jt} - \\sum_{j=1}^{N}\\sum_{s=1}^{k+1} X_{jt}X_{js}M_{s}) \\\\ &=\n-2(\\sum_{j=1}^{N} X^{\\intercal}_{tj}Y_{j}\n-\\sum_{j=1}^{N}\\sum_{s=1}^{k+1} X^{\\intercal}_{tj}X_{js}M_{s}) \\\\ &=\n-2(X^{\\intercal}[t,:]Y - \\sum_{s=1}^{k+1}\\sum_{j=1}^{N}\nX^{\\intercal}_{tj}X_{js}M_{s}) \\\\ &= -2(X^{\\intercal}[t,:]Y -\n\\sum_{s=1}^{k+1} (X^{\\intercal}X)_{ts}M_{s}) \\\\ &=\n-2[X^{\\intercal}[t,:]Y - (X^{\\intercal}X](t,:)M)\\\\\n\\end{aligned}$${#eq-gradient2}\n\nStacking up the different rows to make $E$ yields the desired formula.\n\n**Proposition:** Assume that $D=X^{\\intercal}X$ is invertible (notice\nthat it is a $(k+1)\\times(k+1)$ square matrix so this makes sense).\nThe solution $M$ to the multivariate least squares problem is $$ M =\nD^{-1}X^{\\intercal}Y $${#eq-Msolution} and the \"predicted value\"\n$\\hat{Y}$ for $Y$ is \n$$ \n\\hat{Y} = XD^{-1}X^{\\intercal}Y.\n$${#eq-projection}\n\n## The Multivariate Case (Geometry)\n\nLet's look more closely at the equation obtained by setting the\ngradient of the error, @eq-gradient, to zero. Remember that $M$ is\nthe unknown vector in this equation, everything else is known:\n\n$$ X^{\\intercal}Y = X^{\\intercal}XM $$\n\nHere is how to think about this:\n\n   1. As $M$ varies, the $N\\times 1$ matrix $XM$ varies over the space\n   spanned by the columns of the matrix $X$.\n   So as $M$ varies $XM$ is a general element of the subspace $H$ of $R^{N}$ spanned by the $k+1$ columns of $X$.  \n\n   2. The product $X^{\\intercal}XM$ is a $(k+1)\\times 1$ matrix.  Each\n   entry is the dot product of the general element\n   of $H$ with one of the $k+1$ basis vectors of $H$.  \n\n   3. The product $X^{\\intercal}Y$ is a $(k+1)\\times 1$ matrix whose\n   entries are the dot product of the basis vectors of $H$ with $Y$.\n\nTherefore, this equation asks for us to find $M$ so that the vector\n$XM$ in $H$ has the same dot products with the basis vectors of $H$ as\n$Y$ does.  The condition\n\n$$ X^{\\intercal}\\cdot (Y-XM)=0 $$\n\nsays that $Y-XM$ is orthogonal to $H$.  This argument establishes the\nfollowing proposition.\n\n**Proposition:** Just as in the simple one-dimensional case, the\npredicted value $\\hat{Y}$ of the least squares problem is the point in\n$H$ closest to $Y$ -- or in other words the point $\\hat{Y}$ in $H$\nsuch that $Y-\\hat{Y}$ is perpendicular to $H$.\n\n### Orthogonal Projection\n\nRecall that we introduced the notation $D=X^{\\intercal}X$, and let's\nassume, for now, that $D$ is an invertible matrix.  We have the\nformula (see @eq-projection): $$ \\hat{Y} = XD^{-1}X^{\\intercal}Y.  $$\n**Proposition:** The matrix $P=XD^{-1}X^{\\intercal}$ is an $N\\times N$\nmatrix called the orthogonal projection operator onto the subspace $H$\nspanned by the columns of $X$. It has the following properties:\n\n- $PY$ belongs to the subspace $H$ for any $Y\\in\\mathbf{R}^{N}$.\n- $(Y-PY)$ is orthogonal to $H$.\n- $P*P = P$.\n\n**Proof:** First of all, $PY=XD^{-1}X^{\\intercal}Y$ so $PY$ is a\nlinear combination of the columns of $X$ and is therefore an element\nof $H$.  Next, we can compute the dot product of $PY$ against a basis\nof $H$ by computing\n\n$$ X^{\\intercal}PY = X^{\\intercal}XD^{-1}X^{\\intercal}Y =\nX^{\\intercal}Y \n$$\n\nsince $X^{\\intercal}X=D$.  This equation means that\n$X^{\\intercal}(Y-PY)=0$ which tells us that $Y-PY$ has dot product\nzero with a basis for $H$.  Finally,\n\n$$ PP = XD^{-1}X^{\\intercal}XD^{-1}X^{\\intercal} =\nXD^{-1}X^{\\intercal}=P.  \n$$\n\nIt should be clear from the above discussion that the matrix\n$D=X^{\\intercal}X$ plays an important role in the study of this\nproblem.  In particular it must be invertible or our analysis above\nbreaks down.  In the next section we will look more closely at this\nmatrix and what information it encodes about our data.\n\n## Centered coordinates {#sec-centered}\n\nRecall from last section that the matrix $D=X^{\\intercal}X$ is of central importance to the study of the multivariate least squares problem. Let's look at it more closely.\n\n**Lemma:** The $i,j$ entry of $D$ is the dot product $$\nD_{ij}=X[:,i]\\cdot X[:,j] $$ of the $i^{th}$ and $j^{th}$ columns of\n$X$.\n\n**Proof:** In the matrix multiplication $X^{\\intercal}X$, the $i^{th}$\nrow of $X^{\\intercal}$ gets \"dotted\" with the $j^{th}$ column of $X$\nto product the $i,j$ entry.  But the $i^{th}$ row of $X^{\\intercal}$\nis the $i^{th}$ column of $X$, as asserted in the statement of the\nlemma.\n\nA crucial point in our construction above relied on the matrix $D$\nbeing invertible.  The following Lemma shows that $D$ fails to be\ninvertible only when the different features (the columns of $X$) are\nlinearly dependent.\n\n**Lemma:** $D$ is not invertible if and only if the columns of $X$ are\nlinearly dependent.\n\n**Proof:** If the columns of $X$ are linearly dependent, then there is\na nonzero vector $m$ so that $Xm=0$.  In that case clearly\n$Dm=X^{\\intercal}Xm=0$ so $D$ is not invertible.  Suppose $D$ is not\ninvertible.  Then there is a nonzero vector $m$ with\n$Dm=X^{\\intercal}Xm=0$.  This means that the vector $Xm$ is orthogonal\nto all of the columns of $X$.  Since $Xm$ belongs to the span $H$ of\nthe columns of $X$, if it is orthogonal to $H$ it must be zero.\n\nIn fact, the matrix $D$ captures some important statistical measures\nof our data, but to see this clearly we need to make a slight change\nof basis.  First recall that $X[:,k+1]$ is our column of all $1$,\nadded to handle the intercept.  As a result, the dot product\n$X[:,i]\\cdot X[:,k+1]$ is the sum of the entries in the $i^{th}$\ncolumn, and so if we let $\\mu_{i}$ denote the average value of the\nentries in column $i$, we have $$ \\mu_{i} = \\frac{1}{N}(X[:,i]\\cdot\nX[:,k+1]) $$\n\nNow change the matrix $X$ by elementary column operations to obtain a\nnew data matrix $X_{0}$ by setting $$ X_{0}[:,i] =\nX[:,i]-\\frac{1}{N}(X[:,i]\\cdot X[:,k+1])X[:,k+1] =\nX[:,i]-\\mu_{i}X[:,k+1] $$ for $i=1,\\ldots, k$.\n\nIn terms of the original data, we are changing the measurement scale\nof the data so that each feature has average value zero, and the\nsubspace $H$ spanned by the columns of $X_{0}$ is the same as that\nspanned by the columns of $X$.  Using $X_{0}$ instead of $X$ for our\nleast squares problem, we get\n\n$$ \\hat{Y} = X_{0}D_{0}^{-1}X_{0}^{\\intercal}Y $$\n\nand\n\n$$ M_{0} = D_{0}^{-1}X_{0}^{\\intercal}Y $$\n\nwhere $D_{0}=X_{0}^{\\intercal}X_{0}.$\n\n**Proposition:** The matrix $D_{0}$ has a block form. Its upper left\nblock is a $k\\times k$ symmetric block with entries $$ (D_{0})_{ij} =\n(X[:,i]-\\mu_{i}X[:,k+1])\\cdot(X[:,j]-\\mu_{j}X[:,k+1]) $$ Its\n$(k+1)^{st}$ row and column are all zero, except for the $(k+1),(k+1)$\nentry, which is $N$.\n\n**Proof:** This follows from the fact that the last row and column\nentries are (for $i\\not=k+1$): $$ (X[:,i]-\\mu_{i}X[:,k+1])\\cdot\nX[:,k+1] = (X[:,i]\\cdot X[:,k+1])-N\\mu_{i} = 0 $$ and for $i=k+1$ we\nhave $X[:,k+1]\\cdot X[:,k+1]=N$ since that column is just $N$ $1$'s.\n\n**Proposition:** If the $x$ coordinates (the features) are centered so\nthat they have mean zero, then the intercept $b$ is $$ \\overline{Y} =\n\\frac{1}{N}\\sum y_{i}.  $$\n\n**Proof:** By centering the coordinates, we replace the matrix $X$ by\n$X_{0}$ and $D$ by $D_{0}$.  and we are trying to minimize\n$\\|Y-X_{0}M_{0}\\|^2$.  Use the formula from @eq-Msolution to see that\n$$ M_{0} = D_{0}^{-1}X_{0}^{\\intercal}Y.  $$ \nThe $b$ value we are\ninterested in is the last entry $m_{k+1}$ in $M_{0}$. From the block\nform of $D_{0}$, we know that $D_{0}^{-1}$ has bottom row and last\ncolumn zero except for $1/N$ in position $(k+1)\\times(k+1)$.  Also\n$X_{0}^{\\intercal}$ has last row consisting entirely of $1$.  So the\nbottom entry of $X_{0}^{\\intercal}Y$ is $\\sum_{i=1}^{N} y_{i}$, and\nthe bottom entry $b$ of $D_{0}^{-1}X_{0}^{\\intercal}Y$ is $$ \\mu_{Y} =\n\\frac{1}{N}\\sum_{i=1}^{N} y_{i}.  $$ as claimed.\n\n**Corollary:** If we make a further change of coordinates to define $$\nY_{0} = Y - \\mu_{Y}\\left[\\begin{matrix} 1 \\\\ 1 \\\\ \\vdots \\\\\n1\\end{matrix}\\right] $$ then the associated $b$ is zero.  As a result\nwe can forget about the extra column of $1's$ that we added to $X$ to\naccount for it and reduce the dimension of our entire problem by $1$.\n\nJust to recap, if we center our data so that $\\mu_{Y}=0$ and\n$\\mu_{i}=0$ for $i=1,\\ldots, k$, then the least squares problem\nreduces to minimizing $$ E(M) = \\|Y-XM\\|^2 $$ where $X$ is the\n$N\\times k$ matrix with $j^{th}$ row $(x_{j1},x_{j2},\\ldots, x_{jk})$\nfor $j=1,\\ldots, N$ and the solutions are as given in @eq-Msolution\nand @eq-projection.\n\n## Caveats about Linear Regression\n\n### Basic considerations\n\nReflecting on our long discussion up to this point, we should take\nnote of some of the potential pitfalls that lurk in the use of linear\nregression.\n\n1. When we apply linear regression, we are explicitly assuming that\n    the variable $Y$ is associated to $X$ via linear\nequations.  This is a big assumption!\n\n2. When we use multilinear regression, we are assuming that changes\nin the different features have independent effects on the target\nvariable $y$.  In other words, suppose that $y=ax_1+bx_2$.  Then an\nincrease of $x_1$ by $1$ increases $y$ by $a$, and an increase of\n$x_2$ by $1$ increases $y$ by $b$.  These effects are independent of\none another and combine to yield an increase of $a+b$.\n\n3. We showed in our discussion above that linear regression problem\nhas a solution when the matrix $D=X^{\\intercal}X$ is invertible, and\nthis happens when the columns of $D$ are linearly independent.  When\nworking with real data, which is messy, we could have a situation in\nwhich the features we are studying are, in fact, dependent -- but\nbecause of measurement error, the samples that we collected aren't.\nIn this case, the matrix $D$ will be \"close\" to being non-invertible,\nalthough formally still invertible.  In this case, computing $D^{-1}$\nleads to numerical instability and the solution we obtain is very\nunreliable.\n\n### Simpson's Effect\n\nSimpson's effect is a famous phenomenon that illustrates that linear\nregression can be very misleading in some circumstances.  It is often\na product of \"pooling\" results from multiple experiments.  Suppose,\nfor example, that we are studying the relationship between a certain\nmeasure of blood chemistry and an individual's weight gain or less on\na particular diet.  We do our experiments in three labs, the blue,\ngreen, and red labs.  Each lab obtains similar results -- higher\nlevels of the blood marker correspond to greater weight gain, with a\nregression line of slope around 1.  However, because of differences in\nthe population that each lab is studying, some populations are more\nsusceptible to weight gain and so the red lab sees a mean increase of\nalmost 9 lbs while the blue lab sees a weight gain of only 3 lbs on\naverage.\n\nThe three groups of scientists pool their results to get a larger\nsample size and do a new regression.  Surprise!  Now the regression\nline has slope $-1.6$ and increasing amounts of the marker seem to\nlead to *less* weight gain!\n\nThis is called Simpson's effect, or Simpson's paradox, and it shows\nthat unknown factors (confounding factors) may cause linear regression\nto yield misleading results.  This is particularly true when data from\nexperiments conducted under different conditions is combined; in this\ncase, the differences in experimental setting, called *batch effects*,\ncan throw off the analysis very dramatically.  See @fig-simpsons .\n\n![Simpson's Effect](img/SimpsonsEffect.png){#fig-simpsons\nwidth=50%}\n\n### Exercises\n\n1. When proving that $D$ is invertible if and only if the columns of\n$X$ are linearly independent, we argued that if $X^{\\intercal}Xm=0$\nfor a nonzero vector $m$, then $Xm$ is orthogonal to the span of the\ncolumns of $X$, and is also an element of that span, and is therefore\nzero.  Provide the details: show that if $H$ is a subspace of\n$\\mathbf{R}^{N}$, and $x$ is a vector in $H$ such that $x\\cdot h=0$\nfor all $h\\in H$, then $x=0$.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"01-linear-regression.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":[{"text":"\\usepackage{listings}\n"},{"file":"../chapters/macros.tex"},"macros.tex"],"output-file":"01-linear-regression.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","documentclass":"scrreprt"},"extensions":{"book":{}}}}}