{"title":"Support Vector Machines","markdown":{"headingText":"Support Vector Machines","containsRefs":false,"markdown":"\n\n## Introduction\n\nSuppose that we are given a collection of data made up of samples from two different classes,\nand we would like to develop an algorithm that can distinguish between the two classes.  For example,\ngiven a picture that is either a dog or a cat,  we'd like to be able to say which\nof the pictures are dogs, and which are cats.  For another example, we might want to be able to distinguish\n\"real\" emails from \"spam.\"  This type of problem is called a *classification* problem. \n\nTypically, one approaches a classification problem by beginning with a large set of data for which\nyou know the classes, and you use that data to *train* an algorithm to correctly distinguish\nthe classes for the test cases where you already know the answer.  For example, you start with a few thousand\npictures labelled \"dog\" and \"cat\" and you build your algorithm so that it does a good job distinguishing\nthe dogs from the cats in this initial set of *training data*.  Then you apply your algorithm to pictures\nthat aren't labelled and rely on the predictions you get, hoping that whatever let your algorithm distinguish\nbetween the particular examples will generalize to allow it to correctly classify images that aren't\npre-labelled.\n\nBecause classification is such a central problem, there are many approaches to it.  We will see several\nof them through the course of these lectures.  We will begin with \na particular classification algorithm called \"Support Vector Machines\" (SVM)\nthat is based on linear algebra.  The SVM algorithm is widely used in practice and has a beautiful\ngeometric interpretation, so it will serve as a good beginning for later discussion of more complicated\nclassification algorithms.\n\nIncidentally, I'm not sure why this algorithm is called a \"machine\"; the algorithm\nwas introduced in the paper @vapnik92 where it is called the \"Optimal Margin Classifier\" and as we shall\nsee that is a much better name for it.\n\nMy presentation of this material was heavily influenced by the beautiful paper @bennettDuality.\n\n## A simple example\n\nLet us begin our discussion with a very simple dataset (see @penguins and @penguindata).  This data\nconsists of various measurements of physical characteristics of 344 penguins of 3 different species:\nGentoo, Adelie, and Chinstrap.  If we focus our attention for the moment on the Adelie and Gentoo\nspecies, and plot their body mass against their culmen depth, we obtain the following scatterplot.\n\n![Penguin Scatterplot](img/penguins.png){#fig-penguins width=50%}\n\nIncidentally, a bird's *culmen* is the upper ridge of their beak, and the *culmen depth* is a measure\nof the thickness of the beak.  There's a nice picture at @penguindata for the penguin enthusiasts.\n\nA striking feature of this scatter plot is that there is a clear separation between the clusters of\nAdelie and Gentoo penguins.  Adelie penguins have deeper culmens and less body mass than Gentoo penguins.\nThese characteristics seem like they should provide a way to classify a penguin between these two\nspecies based on these two measurements.\n\nOne way to express the separation between these two clusters is to observe that one can draw\na line on the graph with the property that all of the Adelie penguins lie on one side of that\nline and all of the Gentoo penguins lie on the other. In @fig-penguinsline I've drawn in\nsuch a line (which I found by eyeballing the picture in @fig-penguins).  The line has the equation\n$$\nY = 1.25X+2.\n$$\n\n![Penguins with Separating Line](img/penguins_with_line.png){#fig-penguinsline width=50%}\n\nThe fact that all of the Gentoo penguins lie above this line means that, for the Gentoo penguins,\ntheir body mass in grams is at least $400$ more than $250$ times their culmen depth in mm. (Note\nthat the $y$ axis of the graph is scaled by $200$ grams).\n\n$$\n\\mathrm{Gentoo\\ mass}> 250(\\mathrm{Gentoo\\ culmen\\ depth})+400\n$$\n\nwhile\n\n$$\n\\mathrm{Adelie\\ mass}<250(\\mathrm{Adelie\\ culmen\\ depth})+400.\n$$\n\nNow, if we measure a penguin caught in the wild, we can compute $250(\\mathrm{culmen\\ depth})+400$\nfor that penguin and if this number is greater than the penguin's mass, we say it's an Adelie; otherwise, a Gentoo.\nBased on the experimental data we've collected -- the *training* data -- this seems likely to work\npretty well.\n\n## The general case\n\nTo generalize this approach, let's imagine now that we have $n$ samples and $k$ features (or measurements)\nfor each sample. As before, we can represent this data as an $n\\times k$ data matrix $X$.  In the penguin\nexample, our data matrix would be $344\\times 2$, with one row for each penguin and the columns\nrepresenting the mass and the culmen depth.  In addition to this numerical data, we have a classification\nthat assigns each row to one of two classes.  Let's represent the classes by a $n\\times 1$ vector\n$Y$, where $y_{i}=+1$ if the $i^{th}$ sample is in one class, and $y_{i}=-1$ if that $i^{th}$ sample\nis in the other.   Our goal is to predict $Y$ based on $X$ -- but unlike in linear regression,\n$Y$ takes on the values of $\\pm 1$.\n\nIn the penguin case, we were able to find a line that separated the two classes and then classify\npoints by which side of the line the point was on.  We can generalize\nthis notion to higher dimensions. Before attacking that generalization, let's recall\na few facts about the generalization to $\\mathbf{R}^{k}$ of the idea of a line.\n\n### Hyperplanes\n\n\nThe correct generalization of a line given by an equation\n$w_1 x_1+ w_2 w_2+b=0$ in $\\mathbf{R}^{2}$ is an equation $f(x)=0$ where $f(x)$ is a  degree one polynomial\n$$\nf(x) = f(x_1,\\ldots, x_k) = w_1 x_1 + w_2 x_2 +\\cdots + w_k x_k + b \n$${#eq-degreeone}\n\nIt's easier to understand the geometry of an equation like $f(x)=0$ in @eq-degreeone if we think of the coefficients\n$w_i$ as forming a *nonzero* vector $w = (w_1,\\ldots, w_k)$ in $\\mathbf{R}^{k}$ and writing the formula for $f(x)$\nas \n$$\nf(x) = w\\cdot x +b\n$$.\n\n**Lemma:** Let $f(x)=w\\cdot x+b$ with $w\\in\\mathbf{R}^{k}$ a nonzero vector and $b$ a constant in $\\mathbf{R}$.\n\n- The inequalities $f(x)>0$ and $f(x)<0$ divide up $\\mathbf{R}^{k}$ into two disjoint subsets (called half spaces), in the way that a line\nin $\\mathbf{R}^{2}$ divides the plane in half. \n- The vector $w$ is normal vector to the hyperplane $f(x)=0$.  Concretely this means that if $p$ and $q$ are any two\npoints in that hyperplane, then $w\\cdot (p-q)=0$. \n- Let $p=(u_1,\\ldots,u_k)$ be a point in $\\mathbf{R}^{k}$. Then the perpendicular distance $D$ from $p$ to the hyperplane $f(x)=0$ is\n$$\nD = \\frac{f(p)}{\\|w\\|}\n$$\n\n\n**Proof:** The first part is clear since the inequalities are mutually exclusive. For the secon part, \nsuppose that $p$ and $q$ satisfy $f(x)=0$.  Then $w\\cdot p+b = w\\cdot q+b=0$. Subtracting these two\nequations gives $w\\cdot (p-q)=0$, so $p-q$ is orthogonal to $w$.\n\nFor the third part, consider @fig-triangle.  The point $q$ is an arbitrary point on the hyperplane defined by the equation\n$w\\cdot x+b=0$.  The distance from the hyperplane to $p$ is measured along the dotted line perpendicular to the hyperplane.\nThe dot product $w\\cdot (p-q) = \\|w\\|\\|p-q\\|\\cos(\\theta)$ where $\\theta$ is the angle between $p-q$ and $w$ -- which is complementary\nto the angle between $p-q$ and the hyperplane. The\ndistance $D$ is therefore \n$$\nD=\\frac{w\\cdot(p-q)}{\\|w\\|}.\n$$\nHowever, since $q$ lies on the hyperplane, we know that $w\\cdot q+b=0$ so $w\\cdot q = -b$.  Therefore $w\\cdot(p-q)=w\\cdot p+b=f(p)$,\nwhich is the formula we seek.\n\n![Distance to a Hyperplane](img/triangle.png){#fig-triangle width=30%}\n\n### Linear separability and Margins {#sec-linearseparable}\n\nNow we can return to our classification scheme.  The following definition generalizes our two dimensional picture\nfrom the penguin data.\n\n**Definition:** Suppose that we have an $n\\times k$ data matrix $X$ and a set of labels $Y$ that assign the\n$n$ samples to one of two classes.  Then the labelled data is said to be *linearly separable* if there\nis a vector $w$ and a constant $b$ so that, if $f(x)=w\\cdot x+b$, then \n$f(x)>0$ whenever $x=(x_1,\\ldots, x_k)$ is a row of $X$ -- a sample -- belonging to the $+1$ class,\nand $f(x)<0$ whenever $x$ belongs to the $-1$ class.  The solutions to the equation $f(x)=0$ in this situation\nform a hyperplane that is called a *separating hyperplane* for the data.\n\nIn the situation where our data falls into two classes that are linearly separable, \nour classification strategy  is to find a \nseparating hyperplane $f$ for our training data. Then, given a point $x$ whose class we don't know,\nwe can evaluate $f(x)$ and assign $x$ to a class depending on whether $f(x)>0$ or $f(x)<0$.\n\nThis definition begs two questions about a particular dataset:\n\n1.  How do we tell if the two classes are linearly separable?\n2.  If the two sets are linearly separable, there are infinitely many separating hyperplanes. To see this,\nlook back at the penguin example and notice that we can 'wiggle' the red line a little bit and it will\nstill separate the two sets.  Which is the 'best' separating hyperplane?\n\nLet's try to make the first of these two questions concrete.  We have two sets of points $A$ and $B$ in $\\mathbf{R}^{k}$,\nand we want to (try to) find a vector $w$ and a constant $b$ so that $f(x)=w\\cdot x+b$ takes strictly positive values\nfor $x\\in A$ and strictly negative ones for $x\\in B$.  Let's approach the problem by first choosing $w$\nand then asking whether there is a $b$ that will work.  In the two dimensional case, this is equivalent to choosing\nthe slope of our line, and then asking if we can find an intercept so that the line passes between the two classes.\n\nIn algebraic terms, we are trying to solve the following system of inequalities: given $w$, find $b$ so that:\n$$\nw\\cdot x+b>0 \\hbox{ for all $x$ in A}\n$$\nand\n$$\nw\\cdot x+b<0\\hbox{ for all $x$ in B}.\n$$\nThis is only going to be possible if there is a gap between the smallest value of $w\\cdot x$ for $x\\in A$ and the largest\nvalue of $w\\cdot x$ for $x\\in B$.  In other words, given $w$ there is a $b$ so that $f(x)=w\\cdot x+b$ separates $A$ and $B$\nif\n$$\n\\max_{x\\in B}w\\cdot x < \\min_{x\\in A} w\\cdot x.\n$$\nIf this holds, then choose $b$ so that $-b$ lies in this open interval and you will obtain a separating hyperplane.\n\n**Proposition:** The sets $A$ and $B$ are linearly separable if there is a $w$ so that\n$$\n\\max_{x\\in B}w\\cdot x < \\min_{x\\in A} w\\cdot x\n$$\nIf this inequality holds for some $w$, and $-b$ within this open interval, then $f(x)=w\\cdot x+b$ is a separating hyperplane \nfor $A$ and $B$.\n\n*@fig-penguinhwy2 is an illustration of this argument for a subset of the penguin data.  Here, we have fixed $w=(1.25,-1)$\ncoming from the line $y=1.25x+2$ that we eyeballed earlier.  For each Gentoo (green) point $x_{i}$, we computed $-b=w\\cdot x_{i}$ \nand drew the line $f(x) = w\\cdot x - w\\cdot x_{i}$ giving a family of parallel lines through each of the green points. \nSimilarly for each Adelie (blue) point we drew the corresponding line.  The maximum value of $w\\cdot x$ for the blue points\nturned out to be $1.998$ and the minimum value of $w\\cdot x$ for the green points turned out to be $2.003$.  Thus we have\ntwo lines with a gap between them, and any parallel line in that gap will separate the two sets. \n\nFinally, among all the lines *with this particular $w$*, it seems that the **best** separating line is the one running\nright down the middle of the gap between the boundary lines.  Any other line in the gap will be closer to either the blue\nor green set that the midpoint line is.\n\n![Lines in  Penguin Data for $w=(1.25,-1)$](img/penguinhwy2.png){#fig-penguinhwy2 width=50%}\n\nLet's put all of this together and see if we can make sense of it in general.\n\nSuppose that $A^{+}$ and $A^{-}$ are finite point sets in $\\mathbf{R}^{k}$ and \n$w\\in\\mathbf{R}^{k}$\nsuch that\n$$\nB^{-}(w)=\\max_{x\\in A^{-}}w\\cdot x < \\min_{x\\in A^{+}}w\\cdot x=B^{+}(w).\n$$\nLet $x^{-}$ be a point in $A^{-}$ with $w\\cdot x^{-}=B^{-}(w)$ and $x^{+}$ be a point in $A$ with $w\\cdot x^{+}=B^{+}(w)$.\nThe two hyperplanes $f^{\\pm}(x) = w\\cdot x - B^{\\pm}$ have the property that:\n$$\nf^{+}(x)\\ge 0\\hbox{ for }x\\in A^{+}\\hbox{ and }f^{+}(x)<0\\hbox{ for }x\\in A^{-}\n$$\nand\n$$\nf^{-}(x)\\le 0\\hbox{ for }x\\in A^{-}\\hbox{ and }f^{-}(x)>0\\hbox{ for }x\\in A^{+}\n$$\n\nHyperplanes like $f^{+}$ and $f^{-}$, which \"just touch\" a set of points, are called supporting hyperplanes.\n\n**Definition:** Let $A$ be a set of points in $\\mathbf{R}^{k}$. A hyperplane $f(x)=w\\cdot x+b=0$ is called a \n*supporting hyperplane* for $A$ if $f(x)\\ge 0$ for all $x\\in A$ and $f(x)=0$ for at least one point in $A$,\nor if $f(x)\\le 0$ for all $x\\in A$ and $f(x)=0$ for at least one point in $A$.\n\nThe gap between the two supporting hyperplanes $f^{+}$ and $f^{-}$ is called the *margin* between $A$ and $B$\nfor $w$.\n\n**Definition:** Let $f^{+}$ and $f^{-}$ be as in the discussion above for point sets $A^{+}$ and $A^{-}$ and\nvector $w$.  Then the orthogonal distance between the two  hyperplanes $f^{+}$ and $f^{-}$ is called\nthe geometric margin $\\tau_{w}(A^{+},A^{-})$ (along $w$) between $A^{+}$ and $A^{-}$.  We have\n$$\n\\tau_{w}(A^{+},A^{-})=\\frac{B^{+}(w)-B^{-}(w)}{\\|w\\|}.\n$$\n\nNow we can propose an answer to our second question about the best classifying hyperplane.\n\n**Definition:** The *optimal margin* $\\tau(A^{+},A^{-})$ between $A^{+}$ and $A^{-}$ is the largest value of $\\tau_{w}$\nover all possible $w$ for which $B^{-}(w)<B^{+}(w)$:\n$$\n\\tau(A^{+},A^{-}) = \\max_{w} \\tau_{w}(A^{+},A^{-}).\n$$\nIf $w$ is such that $\\tau_{w}=\\tau$, then the hyperplane $f(x)=w\\cdot x - \\frac{(B^{+}+B^{-})}{2}$\nis the  *optimal margin classifying hyperplane*.  \n\nThe optimal classifying hyperplane runs \"down the middle\" of the gap between the two supporting hyperplanes $f^{+}$\nand $f^{-}$ that give the sides of the optimal margin.\n\nWe can make one more observation about the maximal margin.    If we find a vector $w$ so that\n$f^{+}(x) = w\\cdot x -B^{+}$ and $f^{-}(x) = w\\cdot x-B^{-}$ are the two supporting hyperplanes such that\nthe gap between them is the optimal margin, then this gap gives us an estimate on how close together\nthe points in $A^{+}$ and $A^{-}$ can be.  This is visible in @fig-penguinhwy2, where it's clear that\nto get from a blue point to a green one, you have to cross the gap between the two supporting hyperplanes.\n\n**Proposition:** The closest distance between points in $A^{+}$ and $A^{-}$ is greater than or equal\nto the optimal margin:\n$$\n\\min_{p\\in A^{+},q\\in A^{-}} \\|p-q\\|\\ge \\tau(A^{+},A^{-})\n$$.\n\n**Proof:** We have $f^{+}(p) = w\\cdot p - B^{+}\\ge 0$ and $f^{-}(q) = w\\cdot q -B^{-}\\le 0$.\nThese two inequalities imply that \n$$\nw\\cdot (p-q)\\ge B^{+}-B^{-}>0.\n$$\nTherefore\n$$\n\\|p-q\\|\\|w\\|\\ge |w\\cdot (p-q)|\\ge |B^{+}-B^{-}|\n$$\nand so\n$$\n\\|p-q\\| \\ge \\frac{B^{+}-B^{-}}{\\|w\\|} = \\tau(A^{+},A^{-})\n$$\n\nIf this inequality were always *strict* -- that is, if the optimal margin equalled the minimum distance\nbetween points in the two clusters -- then this would give us an approach to finding this optimal margin.\n\nUnfortunately, that isn't the case.  In @fig-nonstrict, we show a very simple case involving only\nsix points in total in which the distance between the closest points in $A^{+}$ and $A^{-}$ is larger than the optimal margin.\n\n![Shortest distance between + and - points can be greater than the optimal margin](img/margindistance2.png){#fig-nonstrict height=3in}\n\nAt least now our problem is clear.  Given our two point sets $A^{+}$ and $A^{-}$, find $w$ so that $\\tau_{w}(A^{+},A^{-})$\nis maximal among all $w$ where $B^{-}(w)<B^{+}(w)$.   This is an optimization problem, but unlike the optimization\nproblems that arose in our discussions of linear regression and principal component analysis, it does not have a closed\nform solution.  We will need to find an algorithm to determine $w$ by successive approximations.  Developing that algorithm\nwill require thinking about a new concept known as *convexity.*\n\n## Convexity, Convex Hulls, and Margins\n\nIn this section we introduce the notion of a *convex set* and the particular case of the *convex hull*\nof a finite set of points.  As we will see, these ideas will give us a different interpretation of the\nmargin between two sets and will eventually lead to an algorithm for finding the optimal margin classifier.\n\n**Definition:**  A subset $U$ of $\\mathbf{R}^{k}$ is *convex* if, for any pair of points $p$ and $q$ in $U$,\nevery point $t$ on the line segment joining $p$ and $q$ also belongs to $U$.  In vector form, for every\n$0\\le s\\le 1$, the point $t(s) = (1-s)p+sq$ belongs to $U$.  (Note that $t(0)=p$, $t(1)=q$, and\nso $t(s)$ traces out the segment joining $p$ to $q$.)\n\n*@fig-convexnotconvex illustrates the difference between convex sets and non-convex ones.\n\n![Convex vs Non-Convex Sets](img/ConvexNotConvex.png){#fig-convexnotconvex height=3in}\n\nThe key idea from convexity that we will need to solve our optimization problem and find the optimal margin is the\nidea of the *convex hull* of a finite set of points in $\\mathbf{R}^{k}$.\n\n**Definition:** Let $S=\\{q_1,\\ldots, q_{N}\\}$ be a finite set of $N$ points in $\\mathbf{R}^{k}$.  The *convex hull* $C(S)$ of $S$\nis the set of points\n$$\np = \\sum_{i=1}^{N} \\lambda_{i}q_{i}\n$$\nas $\\lambda_{1},\\ldots,\\lambda_{N}$ runs over all positive real numbers such that\n$$\n\\sum_{i=1}^{N} \\lambda_{i} = 1.\n$$\n\nThere are a variety of ways to think about the convex hull $C(S)$ of a set of points $S$, but perhaps the most useful is that\nit is the smallest convex set that contains all of the points of $S$.  That is the content of the next lemma.\n\n**Lemma:** $C(S)$ is  convex.  Furthermore, let $U$ be any convex set containing all of the points of $S$.  Then $U$ contains $C(S)$.\n\n**Proof:** To show that $C(S)$ is convex, we apply the definition.  Let $p_1$ and $p_2$ be two points in $C(S)$,\nso that let $p_{j}=\\sum_{i=1}^{N} \\lambda^{(j)}_{i}q_{i}$ where $\\sum_{i=1}^{N}\\lambda^{(j)}_{i} = 1$ for $j=1,2$.  Then\na little algebra shows that\n$$\n(1-s)p_1+sp_{2} = \\sum_{i=1}^{N} (s\\lambda^{(1)}_{i}+(1-s)\\lambda^{(2)}_{i})q_{i}\n$$\nand $\\sum_{i=1}^{N} (s\\lambda^{(1)}_{i}+(1-s)\\lambda^{(2)}_{i}) = 1$.  Therefore all of the points $(1-s)p_{1}+sp_{2}$ belong to $C(S)$,\nand therefore $C(S)$ is convex.\n\nFor the second part, we proceed by induction.  Let $U$ be a convex set containing $S$.  Then by the definition of convexity,\n$U$ contains all sums $\\lambda_{i}q_{i}+\\lambda_{j}q_{j}$ where $\\lambda_i+\\lambda_j=1$.  Now suppose that $U$ contains\nall the sums $\\sum_{i=1}^{N} \\lambda_{i}q_{i}$  where exactly $m-1$ of the $\\lambda_{i}$ are non-zero for some $m<N$.  \nConsider a sum\n$$\nq = \\sum_{i=1}^{N}\\lambda_{i}q_{i}\n$$\nwith exactly $m$ of the $\\lambda_{i}\\not=0$.  For simplicity let's assume that $\\lambda_{i}\\not=0$ for $i=1,\\ldots, m$.\nNow let $T=\\sum_{i=1}^{m-1}\\lambda_{i}$ and set\n$$\nq' = \\sum_{i=1}^{m-1}\\frac{\\lambda_{i}}{T}q_{i}.\n$$\nThis point $q'$ belongs to $U$ by the inductive hypothesis. Also, $(1-T)=\\lambda_{m}$.  Therefore by convexity of $U$, \n$$\nq = (1-T)q_{m}+Tq'\n$$\nalso belongs to $U$.  It follows that all of $C(S)$ belongs to $U$.\n\nIn @fig-convexhull we show our penguin data together with the convex hull of points corresponding\nto the two types of penguins.  Notice that the boundary of each convex hull\nis a finite collection of line segments that join the \"outermost\" points in the point set.  \n\n![The Convex Hull](img/penguinswithhulls.png){#fig-convexhull width=50%}\n\nOne very simple example of a convex set is a half-plane.  More specifically,\nif $f(x)=w\\cdot x+b=0$ is a hyperplane, then the two \"sides\" of the hyperplane, meaning\nthe subsets $\\{x: f(x)\\ge 0\\}$ and $\\{x: f(x)\\le 0\\}$, are both convex. (This is exercise 1 in @sec-exercises ).\n\n\nAs a result of this observation, and the Lemma above, we can conclude that if $f(x)=w\\cdot x+b=0$\nis a supporting hyperplane for the set $S$ -- meaning that either $f(x)\\ge 0$ for all $x\\in S$, or $f(x)\\le 0$ for all\n$x\\in S$, with at least one point $x\\in S$ such that $f(x)=0$ -- then $f(x)=0$ is a supporting hyperplane for the entire\nconvex hull. After all, if $f(x)\\ge 0$ for all points $x\\in S$, then $S$ is contained in the convex set of points where\n$f(x)\\ge 0$, and therefore $C(S)$ is contained in that set as well.\n\nInterestingly, however, the converse is true as well -- the supporting hyperplanes of $C(S)$ are exactly the same\nas those for $S$.\n\n**Lemma:** Let $S$ be a finite set of points in $\\mathbf{R}^{k}$ and \nlet $f(x)=w\\cdot x +b=0$ be a supporting hyperplane for $C(S)$.  Then $f(x)$ is a supporting hyperplane for $S$.\n\n**Proof:** Suppose $f(x)=0$ is a supporting hyperplane for $C(S)$.  Let's assume that  $f(x)\\ge 0$ for all $x\\in C(S)$ and $f(x^{*})=0$ \nfor a point $x^{*}\\in C(S)$, since the case where $f(x)\\le 0$ is identical.  Since $S\\subset C(S)$, we have $f(x)\\ge 0$ for all $x\\in S$.\nTo show that $f(x)=0$ is a supporting hyperplane, we need to know that $f(x)=0$ for at least one point $x\\in S$.  \nLet $x'$ be the point in $S$ where $f(x')$ is minimal among all $x\\in S$.  Note that $f(x')\\ge 0$. Then\nthe hyperplane $g(x) = f(x)-f(x')$ has the property that $g(x)\\ge 0$ on all of $S$, and $g(x')=0$.  Since the halfplane\n$g(x)\\ge 0$ is convex and contains all of $S$, we have $C(S)$ contained in that halfplane.  So, on the one hand we have\n$g(x^{*})=f(x^{*})-f(x')\\ge 0$. On the other hand $f(x^{*})=0$, so $-f(x')\\ge 0$, so $f(x')\\le 0$.  Since $f(x')$ is also\ngreater or equal to zero, we have $f(x')=0$, and so we have found a point of $S$ on the hyperplane $f(x)=0$. Therefore $f(x)=0$\nis also a supporting hyperplane for $S$.\n\nThis argument can be used to give an alternative description of $C(S)$ as the intersection of all halfplanes containing $S$ arising\nfrom supporting hyperplanes for $S$. This is exercise 2 in @sec-exercises.  It also has as a corollary that\n$C(S)$ is a closed set.\n\n**Lemma:**  $C(S)$ is compact.\n\n**Proof:** Exercise 2 in @sec-exercises shows that it is the intersection of closed sets in $\\mathbf{R}^{k}$, so it is closed.\nExercise 3 shows that $C(S)$ is bounded.  Thus it is compact.\n\nNow let's go back to our optimal margin problem, so that we have linearly separable sets of points $A^{+}$ and $A^{-}$.\nRecall that we showed that the optimal margin was at most the minimal distance between points in $A^{+}$ and $A^{-}$,\nbut that there could be a gap between the minimal distance and the optimal margin -- see @fig-nonstrict for a reminder.\n\nIt turns out that by considering the minimal distance between $C(A^{+})$ and $C(A^{-})$, we can \"close this gap.\" The following\nproposition shows that we can change the problem of finding the optimal margin into the problem of finding the closest\ndistance between the convex hulls of $C(A^{+})$ and $C(A^{-})$.   The following proposition generalizes the Proposition \nat the end of @sec-linearseparable.\n\n**Proposition:** Let $A^{+}$ and $A^{-}$ be linearly separable sets in $\\mathbf{R}^{k}$.  Let $p\\in C(A^{+})$\nand $q\\in C(A^{-})$ be any two points.  Then \n$$\n\\|p-q\\|\\ge \\tau(A^{+},A^{-}).\n$$\n\n**Proof:** As in the earlier proof, choose supporting hyperplanes $f^{+}(x)=w\\cdot x-B^{+}=0$ and $f^{-}(x)=w\\cdot x-B^{-}$ for $A^{+}$\nand $A^{-}$. By our discussion above, these are also supporting hyperplanes for $C(A^{+})$ and $C(A^{-})$.  Therefore\nif $p\\in C(A^{+})$ and $q\\in C(A^{-})$, we have $w\\cdot p-B^{+}\\ge 0$ and $w\\cdot q-B^{-}\\le 0$.  As before\n$$\nw\\cdot(p-q)\\ge B^{+}-B^{-}>0\n$$\nand so \n$$\n\\|p-q\\|\\ge\\frac{B^{+}-B^{-}}{\\|w\\|}=\\tau_{w}(A^{+},A^{-})\n$$\nSince this holds for any $w$, we have the result for $\\tau(A^{+},A^{-})$.\n\nThe reason this result is useful is that, as we've seen,  if we restrict $p$ and $q$ to $A^{+}$ and $A^{-}$, then there\ncan be a gap between the minimal distance and the optimal margin.  If we allow $p$ and $q$ to range over the convex hulls\nof these sets, then that gap disappears.\n\nOne other consequence of this is that if $A^{+}$ and $A^{-}$ are linearly separable then their convex hulls are disjoint.\n\n**Corollary:** If $A^{+}$ and $A^{-}$ are linearly separable then $\\|p-q\\|>0$ for all $p\\in C(A^{+})$ and $q\\in C(A^{-})$\n\n**Proof:** The sets are linearly separable precisely when $\\tau>0$. \n\n\nOur strategy now is to show that if $p$ and $q$ are points in $C(A^{+})$ and $C(A^{-})$ respectively that are at minimal distance $D$,\nand if we set $w=p-q$, then we obtain supporting hyperplanes with margin equal to $\\|p-q\\|$.  Since this margin\nis the *largest possible margin*, this $w$ must be the optimal $w$.  This transforms the problem of finding\nthe optimal margin into the problem of finding the closest points in the convex hulls.\n\n**Lemma:**  Let \n$$\nD=\\min_{p\\in C(A^{+}),q\\in C(A^{-})} \\|p-q\\|.\n$$\nThen there are points $p^*\\in C(A^{+})$ and $q^{*}\\in C(A^{-})$ with $\\|p^{*}-q^{*}\\|=D$. If $p_1^{*},q_1^{*}$ and $p_2^{*},q_2^{*}$\nare two pairs of points satisfying this condition, then $p_1^{*}-q_1^{*}=p_2^{*}-q_{2}^{*}$.\n\n**Proof:** Consider the set of differences\n$$\nV = \\{p-q: p\\in C(A^{+}),q\\in C(A^{-})\\}.\n$$\n\n- $V$ is compact.  This is because it is the image of the compact set $C(A^{+})\\times C(A^{-})$ in $\\mathbf{R}^{k}\\times\\mathbf{R}^{k}$\nunder the continuous map $h(x,y)=x-y$.  \n\n- the function $d(v)=\\|v\\|$ is continuous and satisfies $d(v)\\ge D>0$ for all $v\\in V$.\n\nSince $d$ is a continuous function on a compact set, it attains its minimum $D$ and so there is a $v=p^{*}-q^{*}$ with\n$d(v)=D$.  \n\nNow suppose that there are two distinct points $v_1=p_1^*-q_1^*$ and $v_2=p_2^*-q_2^*$ with $d(v_1)=d(v_2)=D$.  Consider the line segment \n$$\nt(s) = (1-s)v_1+sv_2\\hbox{ where }0\\le s\\le 1\n$$\njoining $v_1$  and $v_2$.  \nNow\n$$\nt(s) = ((1-s)p_1^*+sp_2^*)-((1-s)q_1^*+sq_2^*).\n$$\nBoth terms in this difference belong to $C(A^{+})$ and $C(A^{-})$ respectively, regardless of $s$, by convexity,\nand therefore $t(s)$ belongs to $V$ for all $0\\le s\\le 1$.  \n\nThis little argument shows that $V$ is convex.\nIn geometric terms, $v_1$ and $v_2$ are two points in the set $V$ equidistant from the origin and the segment\njoining them is a chord of a circle; as @fig-chord shows,\nin that situation there must be a point on the line segment joining them that's closer to the origin than they are.\nSince all the points on that segment are in $V$ by convexity, this would \ncontradict the assumption that $v_1$ is the closet point in $V$ to the origin.\n\n![Chord of a circle](img/chord2.png){#fig-chord width=3in}\n\nIn algebraic terms, \nsince $D$ is the minimal value of $\\|v\\|$ for all $v\\in V$,\nwe must have $t(s)\\ge D$.  \nOn the other hand\n$$\n\\frac{d}{ds}\\|t(s)\\|^2 = \\frac{d}{ds}(t(s)\\cdot t(s)) =t(s)\\cdot \\frac{dt(s)}{ds} = t(s)\\cdot(v_2-v_1).\n$$\nTherefore\n$$\n\\frac{d}{ds}\\|t(s)\\|^2|_{s=0} = v_{1}\\cdot(v_{2}-v_{1})=v_{1}\\cdot v_{2}-\\|v_{1}\\|^2\\le 0\n$$\nsince $v_{1}\\cdot v_{2}\\le D^{2}$ and $\\|v_{1}\\|^2=D^2$. If $v_{1}\\cdot v_{2}<D^{2}$, then\nthis derivative would be negative, which would mean that there is a value of\n$s$ where $t(s)$ would be less than $D$.  Since that can't happen, we conclude that $v_{1}\\cdot v_{2}=D^{2}$\nwhich means that $v_{1}=v_{2}$ -- the vectors have the same magnitude $D$ and are parallel.\nThis establishes uniqueness.  \n\n**Note:** The essential ideas of this argument show that a compact convex set in $\\mathbf{R}^{k}$ has a unique\npoint closest to the origin.  The convex set in this instance, \n$$\nV=\\{p-q:p\\in C(A^{+}),q\\in C(A^{-})\\},\n$$\nis called the difference $C(A^{+})-C(A^{-})$, and it is generally true that the difference of convex sets is convex.\n\nNow we can conclude this line of argument.\n\n**Theorem:** Let  $p$ and $q$ be points in  $C(A^{+})$ and $C(A^{-})$\nrespectively are such that $\\|p-q\\|$ is minimal among all such pairs.  Let $w=p-q$ and set\n$B^{+}=w\\cdot p$ and $B^{-}=w\\cdot q$.  Then $f^{+}(x)=w\\cdot x-B^{+}=0$ and $f^{-}(x)=w\\cdot x-B^{-}$\nare supporting hyperplanes for $C(A^{+})$ and $C(A^{-})$ respectively and the\nassociated margin \n$$\n\\tau_{w}(A^{+},A^{-})=\\frac{B^{+}-B^{-}}{\\|w\\|} = \\|p-q\\|\n$$\nis optimal.\n\n**Proof:** First we show that $f^{+}(x)=0$ is a supporting hyperplane for $C(A^{+})$.  Suppose not.\nThen there is a point $p'\\in C(A^{+})$ such that $f^{+}(x)<0$.  Consider the line segment\n$t(s) = (1-s)p+sp'$ running from $p$ to $p'$.  By convexity it is entirely contained in \n$C(A^{+})$.  Now look at the distance from points on this segment to $q$:\n$$\nD(s)=\\|t(s)-q\\|^2.\n$$\nWe have\n$$\n\\frac{dD(s)}{ds}|_{s=0} = 2(p-q)\\cdot (p'-p) = 2w\\cdot (p'-p) = 2\\left[(f^{+}(p')+B^{+})-(f^{+}(p)+B^{+})\\right]\n$$\nso \n$$\n\\frac{dD(s)}{ds}|_{s=0} = 2(f^{+}(p')-f^{+}(p))<0\n$$\nsince $f(p)=0$. This means that $D(s)$ is decreasing along $t(s)$ and so \nthere is a point $s'$ along $t(s)$ where $\\|t(s')-q\\|<D$.  This contradicts the fact that $D$ is the minimal\ndistance.  The same argument shows that $f^{-}(x)=0$ is also a supporting hyperplane.\n\nNow the margin for this $w$ is \n$$\n\\tau_{w}(A^{+},A^{-}) = \\frac{w\\cdot (p-q)}{\\|w\\|} = \\|p-q\\|=D\n$$\nand as $w$ varies we know this is the largest possible $\\tau$ that can occur.  Thus this is the maximal margin.\n\n*@fig-strict shows how considering the closest point in the convex hulls \"fixes\" the problem\nthat we saw in @fig-nonstrict.  The closest point occurs at a point on the boundary of the convex hull that\nis not one of the points in $A^{+}$ or $A^{-}$.  \n\n\n\n![Closest distance between convex hulls gives optimal margin](img/ConvexHullWithMargin.png){#fig-strict width=50%}\n\n## Finding the Optimal Margin Classifier\n\nNow that we have translated our problem into geometry, we can attempt to develop an algorithm for solving\nit.  To recap, we have two sets of points \n$$\nA^{+}=\\{x^+_1,\\ldots, x^+_{n_{+}}\\}\n$$ and \n$$\nA^{-}=\\{x^-_1,\\ldots, x^-_{n_{-}}\\}\n$$\nin $\\mathbf{R}^{k}$ that are linearly separable.\n\nWe wish to find points $p\\in C(A^{+})$ and $q\\in C(A^{-})$ such that \n$$\n\\|p-q\\|=\\min_{p'\\in C(A^{+}),q'\\in C(A^{-})} \\|p'-q'\\|.\n$$\n\nUsing the definition of the convex hull we can express this more concretely.  Since\n$p\\in C(A^{+})$, there are coefficients $\\lambda^{+}_{i}\\ge 0$ for $i=1,\\ldots,n_{+}$ and\n$\\lambda^{-}_{i}\\ge 0$ for $i=1,\\ldots, n_{-}$ so that\n$$\n\\begin{aligned}\np&=&\\sum_{i=1}^{n_{+}}\\lambda^{+}_{i} x^{+}_{i} \\\\\nq&=&\\sum_{i=1}^{n_{-}}\\lambda^{-}_{i} x^{-}_{i} \\\\\n\\end{aligned}\n$$\nwhere $\\sum_{i=1}^{n_{\\pm}} \\lambda_{i}^{\\pm}=1$.\n\nWe can summarize this as follows:\n\n**Optimization Problem 1:** Write $\\lambda^{\\pm}=(\\lambda^{\\pm}_{1},\\ldots, \\lambda^{\\pm}_{n_{\\pm}})$\nDefine\n$$\nw(\\lambda^+,\\lambda^-) = \\sum_{i=1}^{n_{+}}\\lambda^{+}_{i}x^{+}_{i} - \\sum_{i=1}^{n_{-}}\\lambda^{-}x^{-}_{i}\n$$\nTo find the supporting hyperplanes that define the optimal margin between $A^{+}$ and $A^{-}$,\nfind $\\lambda^{+}$ and $\\lambda^{-}$ such that $\\|w(\\lambda^{+},\\lambda^{-})\\|^2$ is minimal among\nall such $w$ where all $\\lambda^{\\pm}_{i}\\ge 0$ and $\\sum_{i=1}^{n_{\\pm}} \\lambda^{\\pm}_{i}=1$.\n\nThis is an example of a *constrained optimization problem.*   It's worth observing that\nthe *objective function* $\\|w(\\lambda^{+},\\lambda^{-})\\|^2$ is just a quadratic function in the $\\lambda^{\\pm}.$\nIndeed we can expand\n$$\n\\|w(\\lambda^{+},\\lambda^{-})\\|^2 = (\\sum_{i=1}^{n_{+}}\\lambda^{+}_{i}x_{i}- \\sum_{i=1}^{n_{-}}\\lambda^{-}x^{-}_{i})\\cdot(\\sum_{i=1}^{n_{+}}\\lambda^{+}_{i}x_{i}- \\sum_{i=1}^{n_{-}}\\lambda^{-}x^{-}_{i})\n$$\nto obtain\n$$\n\\|w(\\lambda^{+},\\lambda^{-})\\|^2 = R -2S +T\n$$\nwhere\n$$\n\\begin{aligned}\nR &=& \\sum_{i=1}^{n_{+}}\\sum_{j=1}^{n_{+}}\\lambda^{+}_{i}\\lambda^{+}_{j}(x^{+}_{i}\\cdot x^{+}_{j}) \\\\\nS &=& \\sum_{i=1}^{n_{+}}\\sum_{j=1}^{n_{-}}\\lambda^{+}_{i}\\lambda^{-}_{j}(x^{+}_{i}\\cdot x^{-}_{j}) \\\\\nT &=& \\sum_{i=1}^{n_{-}}\\sum_{j=1}^{n_{-}}\\lambda^{-}_{i}\\lambda^{-}_{j}(x^{-}_{i}\\cdot x^{-}_{j}) \\\\\n\\end{aligned}\n$${#eq-kernel}\nThus the function we are trying to minimize is relatively simple.  \n\nOn the other hand, unlike optimization problems we have seen earlier in these lectures,\nin which we can apply Lagrange multipliers, in this case\nsome of the constraints are inequalities -- namely\nthe requirement that all of the $\\lambda^{\\pm}\\ge 0$ -- rather than equalities.  There is an extensive theory of such problems that derives from the idea of Lagrange multipliers.  However, in these notes, we will not dive into that\ntheory but will instead construct an algorithm for solving the problem directly.\n\n\n### Relaxing the constraints\n\nOur first step in attacking this problem is to adjust our constraints and our objective function\nslightly so that the problem becomes easier to attack.  \n\n**Optimization Problem 2:** This is a slight revision of problem 1 above.   We minimize:\n$$\nQ(\\lambda^{+},\\lambda^{-}) = \\|w(\\lambda^{+},\\lambda^{-})\\|^2-\\sum_{i=1}^{n_{+}}\\lambda^{+}_{i}-\\sum_{i=1}^{n_{-}}\\lambda^{-}_{i}\n$$\nsubject to the constraints that all $\\lambda^{\\pm}_{i}\\ge 0$ and \n$$\n\\alpha = \\sum_{i=1}^{n_{+}}\\lambda^+_{i} = \\sum_{i=1}^{n_{-}}\\lambda^{-}_{i}.\n$$\n  \nProblem 2 is like problem 1, except we don't require the sums of the $\\lambda^{\\pm}_{i}$ to be \none, but only that they be equal to each other; and we modify the objective function slightly.\nIt turns out that the solution to this optimization problem easily yields the solution to our original one.\n\n**Lemma:**  Suppose $\\lambda^{+}$ and $\\lambda^{-}$ satisfy the constraints of problem 2 and\nyield the minimal value for the objective function $Q(\\lambda^{+},\\lambda^{-})$.  Then $\\alpha\\not=0$.\nRescale the\n$\\lambda^{\\pm}$ to have sum equal to one by dividing by $\\alpha$, yielding \n$\\tau^{\\pm}=(1/\\alpha)\\lambda^{\\pm}$.  Then $w(\\tau^{+},\\tau^{-})$ is a solution to optimization problem 1.\n\n**Proof:**  To show that $\\alpha\\not=0$, suppose that $\\lambda^{\\pm}_{i}=0$ for all $i\\not=1$ and\n$\\lambda=\\lambda^{+}_{1}=\\lambda^{-}_{1}$.  The one-variable quadratic function $Q(\\lambda)$ takes\nits minimum value at $\\lambda=1/\\|x_{1}^{+}-x_{1}^{-}\\|^2$ and its value at that point is negative.  Therefore\nthe minimum value of $Q$ is negative, which means $\\alpha\\not=0$ at that minimum point. \n\nFor the equivalence, notice that $\\tau^{\\pm}$ still satisfy the constraints of problem 2.\nTherefore\n$$\nQ(\\lambda^{+},\\lambda^{-}) = \\|w(\\lambda^{+},\\lambda^{-})\\|^2-2\\alpha\\le \\|w(\\tau^{+},\\tau^{-})\\|^2-2.\n$$\nOn the other hand, suppose that $\\sigma^{\\pm}$ are a solution to problem 1.\nThen \n$$\n\\|w(\\sigma^{+},\\sigma^{-})\\|^2\\le \\|w(\\tau^{+},\\tau^{-})\\|^2.\n$$\nTherefore\n$$\n\\alpha^2 \\|w(\\sigma^{+},\\sigma^{-})\\|^2 = \\|w(\\alpha\\sigma^{+},\\alpha\\sigma^{-})\\|^2\\le \\|w(\\lambda^{+},\\lambda^{-})\\|^2\n$$\nand finally\n$$\n\\|w(\\alpha\\sigma^{+},\\alpha\\sigma^{-})\\|^2-2\\alpha\\le Q(\\lambda^{+},\\lambda^{-})=\\|w(\\alpha\\tau^{+},\\alpha\\tau^{-})\\|^2-2\\alpha.\n$$\nSince $Q$ is the minimal value, we have\n$$\n\\alpha^{2}\\|w(\\sigma^{+},\\sigma^{-})\\|^2 = \\alpha^{2}\\|w(\\tau^{+},\\tau^{-})\\|^2\n$$\nso that indeed $w(\\tau^{+},\\tau^{-})$ gives a solution to Problem 1.\n\n\n### Sequential Minimal Optimization\n\nNow we outline an algorithm for solving Problem 2 that is called Sequential Minimal Optimization\nthat was introduced by John Platt in 1998 (See @plattSMO and Chapter 12 of @KernelMethodAdvances).\nThe algorithm is based on the principle of \"gradient ascent\", where we exploit the fact that the\nnegative gradient of a function points in the direction of its most rapid decrease and we take small steps \nin the direction of the negative gradient until we reach the minimum.\n\nHowever, in this case simplify this idea a little.  Recall that the objective function $Q(\\lambda^{+},\\lambda^{-})$\nis a quadratic function in the $\\lambda$'s and that we need to preserve the condition that\n$\\sum \\lambda^{+}_{i}=\\sum\\lambda^{-}_{i}$.  So our approach is going to be to take, one at a time,\na pair $\\lambda^{+}_{i}$ and $\\lambda^{-}_{j}$ and change them *together* so that the equality of the sums\nis preserved and the change reduces the value of the objective function.  Iterating this will take us\nto a minimum.\n\nSo, for example, let's look at $\\lambda^{+}_i$ and $\\lambda^{-}_{j}$ and, for the moment, think\nof all of the other $\\lambda$'s as constants.  Then our objective function reduces to a quadratic function\nof these two variables that looks something like:\n$$\nQ(\\lambda_{i}^{+},\\lambda_{j}^{-}) = a(\\lambda^{+}_i)^2+b\\lambda^{+}_i\\lambda^{-}_j+c(\\lambda^{-}_{i})^2+d\\lambda^{+}_i+e\\lambda^{-}_{j}+f.\n$$\nThe constraints that remain are $\\lambda^{\\pm}\\ge 0$, and we are going to try to minimize $Q$ by changing\n$\\lambda_{i}^{+}$ and $\\lambda_{j}^{-}$ *by the same amount* $\\delta$.    Furthermore, since we still must have\n$\\lambda_{i}^{+}+\\delta\\ge 0$ and $\\lambda_{j}^{-}+\\delta\\ge 0$, we have\n\n$$\n\\delta\\ge M=\\max\\{-\\lambda_{i}^{+},-\\lambda_{j}^{-}\\}\n$${#eq-delta}\n\nIn terms of this single variable\n$\\delta$, our optimization problem becomes the job of finding the minimum of a quadratic polynomial in one\nvariable subject to the constraint in @eq-delta. This is easy!  There are two cases: the critical\npoint of the quadratic is to the left of $M$, in which case the minimum value occurs at $M$; or the critical\npoint of the quadratic is to the right of $M$, in which case the critical point occurs there.\nThis is illustrated in @fig-quadratics.\n\n![Minimizing the 1-variable quadratic objective function](img/quadratic.png){#fig-quadratics width=50%}\n\nComputationally,  let's write \n$$\nw_{\\delta,i,j}(\\lambda^{+},\\lambda^{-}) = w(\\lambda^{+},\\lambda^{-})+\\delta(x^{+}_{i}-x^{-}_{j}).\n$$\nThen\n$$\n\\frac{d}{d\\delta}(\\|w_{\\delta,i,j}(\\lambda^{+},\\lambda^{-})\\|^2-2\\alpha)  = 2w_{\\delta,i,j}(\\lambda^{+},\\lambda^{-})\\cdot(x^{+}_{i}-x^{-}_{j})-2\n$$\nand using the definition of $w_{\\delta,i,j}$ we obtain the following formula for the critical value of \n$\\delta$ by setting this derivative to zero:\n$$\n\\delta_{i,j} = \\frac{(1-w(\\lambda^{+},\\lambda^{-})\\cdot(x_{i}^{+}-x_{j}^{-})}{\\|x^+_{i}-x^{-}_{j}\\|^2}\n$$\n\nUsing this information we can describe the SMO algorithm.\n\n**Algorithm (SMO, see @plattSMO):**\n\n**Given:** Two linearly separable sets of points $A^{+}=\\{x_{1}^{+},\\ldots,x_{n_{+}}^{+}\\}$ and\n$A^{-}=\\{x_{1}^{-},\\ldots, x_{n_{-}}^{-}\\}$ in $\\mathbf{R}^{k}$.\n\n**Find:** Points $p$ and $q$ belonging to $C(A^{+})$ and $C(A^{-})$ respectively such that\n$$\n\\|p-q\\|^2=\\min_{p'\\in C(A^{+}),q'\\in C(A^{-})} \\|p'-q'\\|^2\n$$\n\n**Initialization:** Set $\\lambda_{i}^{+}=\\frac{1}{n_{+}}$ for $i=1,\\ldots, n_{+}$ and\n$\\lambda_{i}^{-}=\\frac{1}{n_{-}}$ for $i=1,\\ldots, n_{-}$.  Set \n$$\np(\\lambda^{+})=\\sum_{i=1}^{n_{+}}\\lambda^{+}_{i}x^{+}_{i}\n$$\nand\n$$\nq(\\lambda^{-})=\\sum_{i=1}^{n_{-}}\\lambda^{-}_{i}x^{-}_{i}\n$$\nNotice that $w(\\lambda^{+},\\lambda^{-})=p(\\lambda^{+})-q(\\lambda^{-})$.\nLet $\\alpha=\\sum_{i=1}^{n_{+}}\\lambda^{+}=\\sum_{i=1}^{n_{-}}\\lambda^{-}$.  These sums\nwill remain equal to each other throughout the operation of the algorithm.\n\nRepeat the following steps until maximum value of  $\\delta^{*}$ computed\nin each iteration is smaller than some tolerance (so that the change in all of the $\\lambda$'s\nis very small):\n\n- For each pair $i,j$ with $1\\le i\\le n_{+}$ and $1\\le j\\le n_{-}$, compute\n$$\nM_{i,j} = \\max\\{-\\lambda_{i}^{+},-\\lambda_{j}^{-}\\}\n$$\nand \n$$\n\\delta_{i,j} = \\frac{1-(p(\\lambda^{+})-q(\\lambda^{-}))\\cdot(x_{i}^{+}-x_{j}^{-})}{\\|x^+_{i}-x^{-}_{j}\\|^2}.\n$$\nIf $\\delta_{i,j}\\ge M$ then set $\\delta^{*}=\\delta_{i,j}$; otherwise set $\\delta^{*}=M$.  Then update\nthe $\\lambda^{\\pm}$ by the equations:\n$$\n\\begin{aligned}\n\\lambda^{+}_{i}&=&\\lambda^{+}_{i}+\\delta_{i,j}^{*} \\\\\n\\lambda^{+}_{j}&=&\\lambda^{-}_{j}+\\delta_{i,j}^{*} \\\\\n\\end{aligned}\n$$\n\n\nWhen this algorithm finishes, $p\\approx p(\\lambda^{+})$ and $q\\approx q(\\lambda^{-})$ will be very good approximations\nto the desired closest points.\n\nRecall that if we set $w=p-q$, then the optimal margin classifier is\n\n$$\nf(x)=w\\cdot x - \\frac{B^{+}+B^{-}}{2}=0\n$$\n\nwhere $B^{+}=w\\cdot p$ and $B^{-}=w\\cdot q$.  Since $w=p-q$ we can simplify this to obtain\n\n$$\nf(x)=(p-q)\\cdot x -\\frac{\\|p\\|^2-\\|q\\|^2}{2}=0.\n$$\n\nIn @fig-penguinsolution, we show the result of applying this algorithm to the penguin data\nand illustrate the closest points as found by an implementation of the SMO algorithm, together\nwith the optimal classifying line.\n\nBearing in mind that the y-axis is scaled by a factor of 200, we obtain the following rule for distinguishing\nbetween Adelie and Gentoo penguins -- if the culmen depth and body mass put you above the red line,\nyou are a Gentoo penguin, otherwise you are an Adelie.\n\n\n![Closest points in convex hulls of penguin data](img/solution.png){#fig-penguinsolution width=50%}\n\n## Inseparable Sets\n\nNot surprisingly, real life is often more complicated than the penguin example we've discussed at length\nin these notes.  In particular, sometimes we have to work with sets that are not linearly separable.\nInstead, we might have two point clouds, the bulk of which are separable, but because of some outliers\nthere is no hyperplane we can draw that separates the two sets into two halfplanes.  \n\nFortunately, all is not lost.  There are two common ways to address this problem, and while we won't take\nthe time to develop the theory behind them, we can at least outline how they work.\n\n### Best Separating Hyperplanes\n\nIf our sets are not linearly separable, then their convex hulls overlap and so our technique\nfor finding the closest points of the convex hulls won't work.  In this case, we can \"shrink\" the\nconvex hull by considering combinations of points $\\sum_{i}\\lambda_{i}x_{i}$ where\n$\\sum\\lambda_{i}=1$ and  $C\\ge\\lambda_{i}\\ge 0$ for some $C\\le 1$.  For $C$ small enough, reduced\nconvex hulls will be linearly separable -- although some outlier points from each class will\nlie outside of them -- and we can find hyperplane that separates the reduced hulls.  \nIn practice, this means we allow a few points to lie on the \"wrong side\" of the hyperplane.\nOur tolerance for these mistakes depends on $C$, but we can include $C$ in the optimization problem to try to\nfind the smallest $C$ that \"works\".\n\n### Nonlinear kernels\n\nThe second option is to look not for separating hyperplanes but instead for separating curves -- perhaps polynomials\nor even more exotic curves.  This can be achieved by taking advantage of the form of @eq-kernel.  As you see\nthere, the only way the points $x_{i}^{\\pm}$ enter in to the function being minimized is through the\ninner products $x_{i}^{\\pm}\\cdot x_{j}^{\\pm}$.  We can adopt a different inner product than the usual\nEuclidean one, and reconsider the problem using this different inner product.  This amounts to embedding\nour points in a higher dimensional space where they are more likely to be linearly separable.  Again,\nwe will not pursue the mathematics of this further in these notes.\n\n\n\n## Exercises{#sec-exercises}\n\n1.  Prove that, if $f(x)=w\\cdot x+b=0$ is a hyperplane in $\\mathbf{R}^{k}$, then the two \"sides\" of this hyperplane, consisting\nof the points where $f(x)\\ge 0$ and $f(x)\\le 0$, are both convex sets.\n\n2.  Prove that $C(S)$ is the intersection of all the halfplanes $f(x)\\ge 0$ as $f(x)=w\\cdot x+b$ runs through all supporting hyperplanes\nfor $S$ where $f(x)\\ge 0$ for all $p\\in S$.  \n\n3.  Prove that $C(S)$ is bounded.  Hint: show that $S$ is contained in a sphere of sufficiently large radius centered\nat zero, and then that $C(S)$ is contained in that sphere as well.\n\n4. Confirm the final formula for the optimal margin classifier at the end of the lecture.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"06-svm.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":[{"text":"\\usepackage{listings}\n"},{"file":"../chapters/macros.tex"}],"output-file":"06-svm.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["../references/references.bib"],"reference-section-title":"References","csl":"../references/stat.csl","documentclass":"scrreprt"},"extensions":{"book":{}}}}}