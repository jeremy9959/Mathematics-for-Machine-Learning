{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import row\n",
    "from bokeh.palettes import Spectral11\n",
    "\n",
    "output_notebook()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The digits dataset\n",
    "\n",
    "In this notebook we will use gradient descent and logistic regression to classify handwritten digits.\n",
    "\n",
    "The digits dataset provided by sklearn consist of just under 1800 samples, each of which is an 8x8 image\n",
    "with the pixels on a 0 to 10 scale. To recover the image, you must convert the 64 entry rows of the data matrix\n",
    "into an 8x8 array. First we load the data (with the labels) and illustrate a few of the entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_digits()[\"data\"]\n",
    "labels = datasets.load_digits()[\"target\"]\n",
    "samples = rng.choice(list(range(data.shape[0])), size=10)\n",
    "\n",
    "\n",
    "def pic(samples):\n",
    "    \"\"\"Return renderings of data rows with indices from array samples\"\"\"\n",
    "    L = []\n",
    "    for i in samples:\n",
    "        p = figure(\n",
    "            height=80,\n",
    "            width=80,\n",
    "            match_aspect=True,\n",
    "            toolbar_location=None,\n",
    "            title=\"Label: {}\".format(labels[i]),\n",
    "        )\n",
    "        p.grid.visible = False\n",
    "        p.axis.visible = False\n",
    "        k0 = np.flip(-data[i, :].reshape(8, 8), axis=0)\n",
    "        p.image(image=[k0], x=0, y=0, dw=8, dh=8)\n",
    "        L.append(p)\n",
    "    x = row(L)\n",
    "    return x\n",
    "\n",
    "\n",
    "show(pic(samples))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The computation\n",
    "\n",
    "We add a column of ones to the data and we scale the data so it lies between 0 and 1 (the pixel values are between 0 and 10). This helps with numerical stability.\n",
    "\n",
    "We use a library routine `OneHotEncoder` to convert the labels into a one-hot array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([data, np.ones((data.shape[0], 1))], axis=1)\n",
    "# scale the data\n",
    "data = data / np.max(data)\n",
    "\n",
    "# Use sklearn to create a \"one-hot\" encoding of the labels\n",
    "\n",
    "E = OneHotEncoder()\n",
    "Y = E.fit_transform(labels.reshape(-1, 1)).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is our main gradient descent routine. It also returns the sequence of computed likelihoods\n",
    "## that we can plot to observe how our algorithm is performing\n",
    "\n",
    "\n",
    "def descent(x, y, max_iter=1000, nu=0.001, M=None):\n",
    "    \"\"\"does gradient descent to maximum likelihood for data=x and labels (one-hot)=y\"\"\"\n",
    "    features = x.shape[1]\n",
    "    classes = y.shape[1]\n",
    "    likes = []\n",
    "    if M is None:\n",
    "        M = rng.normal(loc=0, scale=1, size=(features, classes))\n",
    "    for i in tqdm(range(max_iter)):\n",
    "        P = softmax(x @ M, axis=1)\n",
    "        grad = x.transpose() @ (P - Y)\n",
    "        M = M - nu * grad\n",
    "        likes.append(-np.trace(y.transpose() @ np.log(P)))\n",
    "    return M, likes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the descent algorithm with the data and the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, L = descent(data, Y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how accurate we are, we compute the predicted labels by taking the column of the largest probability in\n",
    "each row of the matrix P=sigma(XM) and compare it to the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.argmax(softmax(data @ M, axis=1), axis=1)\n",
    "correct = (predicted == labels).sum()\n",
    "percentage = correct / len(labels)\n",
    "print(\"Percentage correct on training data = {}\".format(100 * percentage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(title=\"Negative Log Likelihood over iterations\")\n",
    "p.line(x=list(range(len(L))), y=L)\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d760aa701f9c3953c4158a2f5d628550462b9fbb129003fab9d076ff94005fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
