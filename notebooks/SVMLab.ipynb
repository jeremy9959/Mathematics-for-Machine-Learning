{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acute-williams",
   "metadata": {},
   "source": [
    "# SVM Lab\n",
    "In this lab we will work with the sklearn SVM Library to apply support vector machines to the penguin data set.\n",
    "\n",
    "We load the usual libraries:\n",
    "- numpy\n",
    "- bokeh\n",
    "- the sklearn library  which stands for support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import CategoricalColorMapper\n",
    "from sklearn.svm import SVC\n",
    "rng = default_rng(5)\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-packing",
   "metadata": {},
   "source": [
    "The following two functions are utilities that will help us to extract information from the SVC object in a form\n",
    "suitable for plotting.  The key is that the vector $w$ that gives the optimal margin is $\\sum t_{i}\\alpha_{i}x_{i}$\n",
    "where the $x_{i}$ are the \"support vectors\" in ```P.support_vectors_```, the $t_{i}$ are the associated labels, and the $\\alpha_{i}$\n",
    "are the \"dual coefficients\" in ```P.dual_coef_```.  See the documentation of the [mathematical formulation](https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation),\n",
    "section 1.4.7.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperplane(P,x,z=0):\n",
    "    \"\"\"Given an SVC object P and an array of vectors x, computes the hyperplane wx+b=z\"\"\"\n",
    "    alphas = P.dual_coef_\n",
    "    svs = P.support_vectors_\n",
    "    c = P.intercept_[0]-z\n",
    "    a = np.sum(alphas.T*svs,axis=0)[0]\n",
    "    b = np.sum(alphas.T*svs,axis=0)[1]\n",
    "    return (-c-a*x)/b\n",
    "\n",
    "def pts(P):\n",
    "    \"\"\"Given an SVC object P, returns the two closest points in the associated reduced convex hulls.\"\"\"\n",
    "    alphas = P.dual_coef_[0]\n",
    "    svs = P.support_vectors_\n",
    "    plus_indices = np.where(alphas>0)\n",
    "    minus_indices = np.where(alphas<=0)\n",
    "    alphas = alphas.reshape(-1,1)\n",
    "    pluspt = np.sum(alphas[plus_indices]*svs[plus_indices],axis=0)/np.sum(alphas[plus_indices])\n",
    "    minuspt = np.sum(alphas[minus_indices]*svs[minus_indices],axis=0)/np.sum(alphas[minus_indices])\n",
    "    return pluspt, minuspt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-bullet",
   "metadata": {},
   "source": [
    "## Simulated data illustration\n",
    "\n",
    "The command\n",
    "```P=SVC(kernel='linear',C=1000).fit(data,labels)```\n",
    "computes the Support Vector Classifier.  Here C bounds the coefficients $$\\lambda_{i}$$, and assuming the point sets are\n",
    "linearly separable you should you should take C large to find the optimal margin based on the convex hulls.  You can\n",
    "read the \"mathematical formulation\" part of the SVC documentation to see the exact correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=200\n",
    "# Generate two random point sets and plot them\n",
    "A = rng.normal(-1,.3,size=(N,2))\n",
    "B = rng.normal(1,.3,size=(N,2))\n",
    "\n",
    "f=figure(x_range=[-2,2],y_range=[-2,2],height=500,width=500)\n",
    "\n",
    "f.scatter(x=A[:,0],y=A[:,1],color='blue')\n",
    "f.scatter(x=B[:,0],y=B[:,1],color='green')\n",
    "\n",
    "data  = np.concatenate([A,B],axis=0)\n",
    "labels = np.array([0]*N+[1]*N)\n",
    "\n",
    "# compute the svc classifier\n",
    "\n",
    "P = SVC(kernel='linear',C=1000).fit(data,labels)\n",
    "\n",
    "# draw the separating hyperplane, plus the hyperplanes that define the margin.  \n",
    "# here we are extracting information from the SVC object.\n",
    "\n",
    "x=np.linspace(-2,2,100)\n",
    "y = hyperplane(P,x)\n",
    "f.line(x=x,y=y)\n",
    "y = hyperplane(P,x,1)\n",
    "f.line(x=x,y=y)\n",
    "y = hyperplane(P,x,-1)\n",
    "f.line(x=x,y=y)\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P.support_vectors_ gives the points that lie on the marginal hyperplanes.\n",
    "P.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P.dual_coef_ gives the associated lambda's multiplied by +/-1 depending on the label\n",
    "P.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the \"closest points\" use the pts function\n",
    "pts(P)\n",
    "xs = [pts(P)[0][0],pts(P)[1][0]]\n",
    "ys = [pts(P)[0][1],pts(P)[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.line(x=xs,y=ys,color='black',line_width=5)\n",
    "f.scatter(x=xs,y=ys,color='black',size=8)\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-robinson",
   "metadata": {},
   "source": [
    "## Penguins and multiclass classification\n",
    "\n",
    "I've simplified life a little by removing missing data from the penguin data set and by restricting to  numerical features.\n",
    "\n",
    "There are more than two features for the penguin data, but lets start by looking at the two we considered in the theoretical discussion:\n",
    "culmen length and body mass.  These are features 0 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"penguin_data.csv\",delimiter=',',skip_header=1)\n",
    "labels = np.genfromtxt(\"penguin_labels.csv\",delimiter=',',dtype=int,skip_header=1)\n",
    "working_data=data[:,[0,3]]\n",
    "working_data[:,1] = working_data[:,1]/200\n",
    "colors = ['red','blue','green']\n",
    "penguin_colors = np.array([colors[i] for i in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=figure(title='Penguin Data: culmen length vs body mass/200',x_range=[30,60],y_range=[10,35])\n",
    "f.scatter(x=working_data[:,0],y=working_data[:,1],color=penguin_colors)\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-therapist",
   "metadata": {},
   "source": [
    "We can try \"one vs rest\" classification where we consider each group against all of the other points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_points = np.where(labels==0)\n",
    "blue_points =np.where(labels==1)\n",
    "green_points = np.where(labels ==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_vs_others = np.array([0 if x ==1 else 1 for x in labels])\n",
    "red_vs_others = np.array([0 if x==0 else 1 for x in labels])\n",
    "green_vs_others = np.array([0 if x==2 else 1 for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred = SVC(kernel='linear',C=1000).fit(working_data,red_vs_others)\n",
    "Pgreen = SVC(kernel='linear',C=1000).fit(working_data,green_vs_others)\n",
    "Pblue = SVC(kernel='linear',C=1000).fit(working_data,blue_vs_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=figure(title='Penguin Data: culmen length vs body mass/200',x_range=[30,60],y_range=[10,35])\n",
    "f.scatter(x=working_data[:,0],y=working_data[:,1],color=penguin_colors)\n",
    "x=np.linspace(30,60,100)\n",
    "yred=hyperplane(Pred,x)\n",
    "y0 = hyperplane(Pred,x,1)\n",
    "y1 = hyperplane(Pred,x,-1)\n",
    "f.line(x=x,y=yred,line_width=3,color='black',line_dash='dashed',legend_label='red vs others')\n",
    "f.line(x=x,y=y0,line_width=1,alpha=.5,color='black',line_dash='dashed',legend_label='red vs others')\n",
    "f.line(x=x,y=y1,line_width=1,alpha=.5,color='black',line_dash='dashed',legend_label='red vs others')\n",
    "\n",
    "#f.line(x=x,y=yblue,line_width=3,color='black',line_dash='dotted',legend_label='blue vs others')\n",
    "#f.line(x=x,y=ygreen,line_width=3,color='black',line_dash='dotdash',legend_label='green vs others')\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=figure(title='Penguin Data: culmen length vs body mass/200',x_range=[30,60],y_range=[10,35],width=500,height=500)\n",
    "f.scatter(x=working_data[:,0],y=working_data[:,1],color=penguin_colors)\n",
    "x=np.linspace(30,60,100)\n",
    "yred=hyperplane(Pred,x)\n",
    "y0 = hyperplane(Pred,x,1)\n",
    "y1 = hyperplane(Pred,x,-1)\n",
    "f.line(x=x,y=yred,line_width=3,color='black',line_dash='dashed',legend_label='red vs others')\n",
    "f.line(x=x,y=y0,line_width=1,alpha=.5,color='black',line_dash='dashed',legend_label='red vs others')\n",
    "f.line(x=x,y=y1,line_width=1,alpha=.5,color='black',line_dash='dashed',legend_label='red vs others')\n",
    "yblue = hyperplane(Pblue,x)\n",
    "y0 = hyperplane(Pblue,x,1)\n",
    "y1 = hyperplane(Pblue,x,-1)\n",
    "f.line(x=x,y=yblue,line_width=3,color='black',line_dash='dotted',legend_label='blue vs others')\n",
    "f.line(x=x,y=y0,line_width=1,alpha=.5,color='black',line_dash='dotted',legend_label='blue vs others')\n",
    "f.line(x=x,y=y1,line_width=1,alpha=.5,color='black',line_dash='dotted',legend_label='blue vs others')\n",
    "ygreen=hyperplane(Pgreen,x)\n",
    "y0 = hyperplane(Pgreen,x,1)\n",
    "y1 = hyperplane(Pgreen,x,-1)\n",
    "f.line(x=x,y=ygreen,line_width=3,color='black',line_dash='dotdash',legend_label='green vs others')\n",
    "f.line(x=x,y=y0,line_width=1,alpha=.5,color='black',line_dash='dotdash',legend_label='green vs others')\n",
    "f.line(x=x,y=y1,line_width=1,alpha=.5,color='black',line_dash='dotdash',legend_label='green vs others')\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-runner",
   "metadata": {},
   "source": [
    "The SVC classifier computes a decision function by evaluating the different hyperplane functions.  So it looks at\n",
    "fred(p), fblue(p), and fgreen(p).  It assigns the point to the class where the value is largest.  Choose C as large as possible\n",
    "to use the most straightforward version of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pall = SVC(kernel='linear',C=1000).fit(working_data,labels)\n",
    "Pall.predict(working_data)\n",
    "predicted_colors = [colors[i] for i in Pall.predict(working_data)]\n",
    "f=figure(title='predicted classification',x_range=[30,60],y_range=[10,35])\n",
    "f.scatter(x=working_data[:,0],y=working_data[:,1],color=predicted_colors)\n",
    "yred=hyperplane(Pred,x)\n",
    "y0 = hyperplane(Pred,x,1)\n",
    "y1 = hyperplane(Pred,x,-1)\n",
    "f.line(x=x,y=yred,line_width=3,color='black',line_dash='dashed',legend_label='red vs others')\n",
    "f.line(x=x,y=y0,line_width=1,alpha=.5,color='black',line_dash='dashed',legend_label='red vs others')\n",
    "f.line(x=x,y=y1,line_width=1,alpha=.5,color='gray',line_dash='dashed',legend_label='red vs others')\n",
    "yblue = hyperplane(Pblue,x)\n",
    "y0 = hyperplane(Pblue,x,1)\n",
    "y1 = hyperplane(Pblue,x,-1)\n",
    "f.line(x=x,y=yblue,line_width=3,color='black',line_dash='dotted',legend_label='blue vs others')\n",
    "f.line(x=x,y=y0,line_width=1,alpha=.5,color='black',line_dash='dotted',legend_label='blue vs others')\n",
    "f.line(x=x,y=y1,line_width=1,alpha=.5,color='gray',line_dash='dotted',legend_label='blue vs others')\n",
    "ygreen=hyperplane(Pgreen,x)\n",
    "y0 = hyperplane(Pgreen,x,1)\n",
    "y1 = hyperplane(Pgreen,x,-1)\n",
    "f.line(x=x,y=ygreen,line_width=3,color='black',line_dash='dotdash',legend_label='green vs others')\n",
    "f.line(x=x,y=y0,line_width=1,alpha=.5,color='black',line_dash='dotdash',legend_label='green vs others')\n",
    "f.line(x=x,y=y1,line_width=1,alpha=.5,color='gray',line_dash='dotdash',legend_label='green vs others')\n",
    "show(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classifier yields accuracy of {:2f}%'.format(Pall.score(working_data,labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-attraction",
   "metadata": {},
   "source": [
    "## Using all the features\n",
    "\n",
    "We can train an SVC classifier on 25% of the data and still get exceptionally good predictions using all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data,labels,test_size=.75)\n",
    "P = SVC(kernel='linear').fit(data_train,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.score(data_test,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-teddy",
   "metadata": {},
   "source": [
    "# f-MNIST with SVM\n",
    "\n",
    "The two files fmnist_data.csv and fmnist_labels.csv are a selection of 10000 records of the F-MNIST\n",
    "data set, slightly cleaned up, for use in SVC classification.\n",
    "\n",
    "The fmnist_data.csv files contains the 10000x784 records of the images, and the labels contains the 10000 associated labels\n",
    "0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.genfromtxt('fmnist_data.csv',delimiter=',')\n",
    "labels=np.genfromtxt('fmnist_labels.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(data,labels,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P = SVC().fit(data_train,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-hamburg",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
