<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-99.9.9">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Lectures on Machine Learning - 6&nbsp; Support Vector Machines</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/20-references.html" rel="next">
<link href="../chapters/05-logistic-regression.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Lectures on Machine Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-linear-regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-pca.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Principal Component Analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Probability and Bayes Theorem</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-naive-bayes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Naive Bayes classification method</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-logistic-regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Logistic Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-svm.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">6.1</span>  Introduction</a></li>
  <li><a href="#a-simple-example" id="toc-a-simple-example" class="nav-link" data-scroll-target="#a-simple-example"><span class="toc-section-number">6.2</span>  A simple example</a></li>
  <li><a href="#the-general-case" id="toc-the-general-case" class="nav-link" data-scroll-target="#the-general-case"><span class="toc-section-number">6.3</span>  The general case</a>
  <ul class="collapse">
  <li><a href="#hyperplanes" id="toc-hyperplanes" class="nav-link" data-scroll-target="#hyperplanes"><span class="toc-section-number">6.3.1</span>  Hyperplanes</a></li>
  <li><a href="#sec-linearseparable" id="toc-sec-linearseparable" class="nav-link" data-scroll-target="#sec-linearseparable"><span class="toc-section-number">6.3.2</span>  Linear separability and Margins</a></li>
  </ul></li>
  <li><a href="#convexity-convex-hulls-and-margins" id="toc-convexity-convex-hulls-and-margins" class="nav-link" data-scroll-target="#convexity-convex-hulls-and-margins"><span class="toc-section-number">6.4</span>  Convexity, Convex Hulls, and Margins</a></li>
  <li><a href="#finding-the-optimal-margin-classifier" id="toc-finding-the-optimal-margin-classifier" class="nav-link" data-scroll-target="#finding-the-optimal-margin-classifier"><span class="toc-section-number">6.5</span>  Finding the Optimal Margin Classifier</a>
  <ul class="collapse">
  <li><a href="#relaxing-the-constraints" id="toc-relaxing-the-constraints" class="nav-link" data-scroll-target="#relaxing-the-constraints"><span class="toc-section-number">6.5.1</span>  Relaxing the constraints</a></li>
  <li><a href="#sequential-minimal-optimization" id="toc-sequential-minimal-optimization" class="nav-link" data-scroll-target="#sequential-minimal-optimization"><span class="toc-section-number">6.5.2</span>  Sequential Minimal Optimization</a></li>
  </ul></li>
  <li><a href="#inseparable-sets" id="toc-inseparable-sets" class="nav-link" data-scroll-target="#inseparable-sets"><span class="toc-section-number">6.6</span>  Inseparable Sets</a>
  <ul class="collapse">
  <li><a href="#best-separating-hyperplanes" id="toc-best-separating-hyperplanes" class="nav-link" data-scroll-target="#best-separating-hyperplanes"><span class="toc-section-number">6.6.1</span>  Best Separating Hyperplanes</a></li>
  <li><a href="#nonlinear-kernels" id="toc-nonlinear-kernels" class="nav-link" data-scroll-target="#nonlinear-kernels"><span class="toc-section-number">6.6.2</span>  Nonlinear kernels</a></li>
  </ul></li>
  <li><a href="#sec-exercises" id="toc-sec-exercises" class="nav-link" data-scroll-target="#sec-exercises"><span class="toc-section-number">6.7</span>  Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
::: {.hidden} 
$$
    \newcommand{\df}[1]{\frac{\partial}{\partial #1}}
$$
:::

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">6.1</span> Introduction</h2>
<p>Suppose that we are given a collection of data made up of samples from two different classes, and we would like to develop an algorithm that can distinguish between the two classes. For example, given a picture that is either a dog or a cat, we’d like to be able to say which of the pictures are dogs, and which are cats. For another example, we might want to be able to distinguish “real” emails from “spam.” This type of problem is called a <em>classification</em> problem.</p>
<p>Typically, one approaches a classification problem by beginning with a large set of data for which you know the classes, and you use that data to <em>train</em> an algorithm to correctly distinguish the classes for the test cases where you already know the answer. For example, you start with a few thousand pictures labelled “dog” and “cat” and you build your algorithm so that it does a good job distinguishing the dogs from the cats in this initial set of <em>training data</em>. Then you apply your algorithm to pictures that aren’t labelled and rely on the predictions you get, hoping that whatever let your algorithm distinguish between the particular examples will generalize to allow it to correctly classify images that aren’t pre-labelled.</p>
<p>Because classification is such a central problem, there are many approaches to it. We will see several of them through the course of these lectures. We will begin with a particular classification algorithm called “Support Vector Machines” (SVM) that is based on linear algebra. The SVM algorithm is widely used in practice and has a beautiful geometric interpretation, so it will serve as a good beginning for later discussion of more complicated classification algorithms.</p>
<p>Incidentally, I’m not sure why this algorithm is called a “machine”; the algorithm was introduced in the paper <span class="citation" data-cites="vapnik92">[<a href="20-references.html#ref-vapnik92" role="doc-biblioref">1</a>]</span> where it is called the “Optimal Margin Classifier” and as we shall see that is a much better name for it.</p>
<p>My presentation of this material was heavily influenced by the beautiful paper <span class="citation" data-cites="bennettDuality">[<a href="20-references.html#ref-bennettDuality" role="doc-biblioref">2</a>]</span>.</p>
</section>
<section id="a-simple-example" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="a-simple-example"><span class="header-section-number">6.2</span> A simple example</h2>
<p>Let us begin our discussion with a very simple dataset (see <span class="citation" data-cites="penguins">[<a href="20-references.html#ref-penguins" role="doc-biblioref">3</a>]</span> and <span class="citation" data-cites="penguindata">[<a href="20-references.html#ref-penguindata" role="doc-biblioref">4</a>]</span>). This data consists of various measurements of physical characteristics of 344 penguins of 3 different species: Gentoo, Adelie, and Chinstrap. If we focus our attention for the moment on the Adelie and Gentoo species, and plot their body mass against their culmen depth, we obtain the following scatterplot.</p>
<div id="fig-penguins" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/penguins.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.1: Penguin Scatterplot</figcaption><p></p>
</figure>
</div>
<p>Incidentally, a bird’s <em>culmen</em> is the upper ridge of their beak, and the <em>culmen depth</em> is a measure of the thickness of the beak. There’s a nice picture at <span class="citation" data-cites="penguindata">[<a href="20-references.html#ref-penguindata" role="doc-biblioref">4</a>]</span> for the penguin enthusiasts.</p>
<p>A striking feature of this scatter plot is that there is a clear separation between the clusters of Adelie and Gentoo penguins. Adelie penguins have deeper culmens and less body mass than Gentoo penguins. These characteristics seem like they should provide a way to classify a penguin between these two species based on these two measurements.</p>
<p>One way to express the separation between these two clusters is to observe that one can draw a line on the graph with the property that all of the Adelie penguins lie on one side of that line and all of the Gentoo penguins lie on the other. In <a href="#fig-penguinsline">Figure&nbsp;<span>6.2</span></a> I’ve drawn in such a line (which I found by eyeballing the picture in <a href="#fig-penguins">Figure&nbsp;<span>6.1</span></a>). The line has the equation <span class="math display">\[
Y = 1.25X+2.
\]</span></p>
<div id="fig-penguinsline" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/penguins_with_line.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.2: Penguins with Separating Line</figcaption><p></p>
</figure>
</div>
<p>The fact that all of the Gentoo penguins lie above this line means that, for the Gentoo penguins, their body mass in grams is at least <span class="math inline">\(400\)</span> more than <span class="math inline">\(250\)</span> times their culmen depth in mm. (Note that the <span class="math inline">\(y\)</span> axis of the graph is scaled by <span class="math inline">\(200\)</span> grams).</p>
<p><span class="math display">\[
\mathrm{Gentoo\ mass}&gt; 250(\mathrm{Gentoo\ culmen\ depth})+400
\]</span></p>
<p>while</p>
<p><span class="math display">\[
\mathrm{Adelie\ mass}&lt;250(\mathrm{Adelie\ culmen\ depth})+400.
\]</span></p>
<p>Now, if we measure a penguin caught in the wild, we can compute <span class="math inline">\(250(\mathrm{culmen\ depth})+400\)</span> for that penguin and if this number is greater than the penguin’s mass, we say it’s an Adelie; otherwise, a Gentoo. Based on the experimental data we’ve collected – the <em>training</em> data – this seems likely to work pretty well.</p>
</section>
<section id="the-general-case" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="the-general-case"><span class="header-section-number">6.3</span> The general case</h2>
<p>To generalize this approach, let’s imagine now that we have <span class="math inline">\(n\)</span> samples and <span class="math inline">\(k\)</span> features (or measurements) for each sample. As before, we can represent this data as an <span class="math inline">\(n\times k\)</span> data matrix <span class="math inline">\(X\)</span>. In the penguin example, our data matrix would be <span class="math inline">\(344\times 2\)</span>, with one row for each penguin and the columns representing the mass and the culmen depth. In addition to this numerical data, we have a classification that assigns each row to one of two classes. Let’s represent the classes by a <span class="math inline">\(n\times 1\)</span> vector <span class="math inline">\(Y\)</span>, where <span class="math inline">\(y_{i}=+1\)</span> if the <span class="math inline">\(i^{th}\)</span> sample is in one class, and <span class="math inline">\(y_{i}=-1\)</span> if that <span class="math inline">\(i^{th}\)</span> sample is in the other. Our goal is to predict <span class="math inline">\(Y\)</span> based on <span class="math inline">\(X\)</span> – but unlike in linear regression, <span class="math inline">\(Y\)</span> takes on the values of <span class="math inline">\(\pm 1\)</span>.</p>
<p>In the penguin case, we were able to find a line that separated the two classes and then classify points by which side of the line the point was on. We can generalize this notion to higher dimensions. Before attacking that generalization, let’s recall a few facts about the generalization to <span class="math inline">\(\mathbf{R}^{k}\)</span> of the idea of a line.</p>
<section id="hyperplanes" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="hyperplanes"><span class="header-section-number">6.3.1</span> Hyperplanes</h3>
<p>The correct generalization of a line given by an equation <span class="math inline">\(w_1 x_1+ w_2 w_2+b=0\)</span> in <span class="math inline">\(\mathbf{R}^{2}\)</span> is an equation <span class="math inline">\(f(x)=0\)</span> where <span class="math inline">\(f(x)\)</span> is a degree one polynomial <span id="eq-degreeone"><span class="math display">\[
f(x) = f(x_1,\ldots, x_k) = w_1 x_1 + w_2 x_2 +\cdots + w_k x_k + b
\tag{6.1}\]</span></span></p>
<p>It’s easier to understand the geometry of an equation like <span class="math inline">\(f(x)=0\)</span> in <a href="#eq-degreeone">Equation&nbsp;<span>6.1</span></a> if we think of the coefficients <span class="math inline">\(w_i\)</span> as forming a <em>nonzero</em> vector <span class="math inline">\(w = (w_1,\ldots, w_k)\)</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span> and writing the formula for <span class="math inline">\(f(x)\)</span> as <span class="math display">\[
f(x) = w\cdot x +b
\]</span>.</p>
<p><strong>Lemma:</strong> Let <span class="math inline">\(f(x)=w\cdot x+b\)</span> with <span class="math inline">\(w\in\mathbf{R}^{k}\)</span> a nonzero vector and <span class="math inline">\(b\)</span> a constant in <span class="math inline">\(\mathbf{R}\)</span>.</p>
<ul>
<li>The inequalities <span class="math inline">\(f(x)&gt;0\)</span> and <span class="math inline">\(f(x)&lt;0\)</span> divide up <span class="math inline">\(\mathbf{R}^{k}\)</span> into two disjoint subsets (called half spaces), in the way that a line in <span class="math inline">\(\mathbf{R}^{2}\)</span> divides the plane in half.</li>
<li>The vector <span class="math inline">\(w\)</span> is normal vector to the hyperplane <span class="math inline">\(f(x)=0\)</span>. Concretely this means that if <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are any two points in that hyperplane, then <span class="math inline">\(w\cdot (p-q)=0\)</span>.</li>
<li>Let <span class="math inline">\(p=(u_1,\ldots,u_k)\)</span> be a point in <span class="math inline">\(\mathbf{R}^{k}\)</span>. Then the perpendicular distance <span class="math inline">\(D\)</span> from <span class="math inline">\(p\)</span> to the hyperplane <span class="math inline">\(f(x)=0\)</span> is <span class="math display">\[
D = \frac{f(p)}{\|w\|}
\]</span></li>
</ul>
<p><strong>Proof:</strong> The first part is clear since the inequalities are mutually exclusive. For the secon part, suppose that <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> satisfy <span class="math inline">\(f(x)=0\)</span>. Then <span class="math inline">\(w\cdot p+b = w\cdot q+b=0\)</span>. Subtracting these two equations gives <span class="math inline">\(w\cdot (p-q)=0\)</span>, so <span class="math inline">\(p-q\)</span> is orthogonal to <span class="math inline">\(w\)</span>.</p>
<p>For the third part, consider <a href="#fig-triangle">Figure&nbsp;<span>6.3</span></a>. The point <span class="math inline">\(q\)</span> is an arbitrary point on the hyperplane defined by the equation <span class="math inline">\(w\cdot x+b=0\)</span>. The distance from the hyperplane to <span class="math inline">\(p\)</span> is measured along the dotted line perpendicular to the hyperplane. The dot product <span class="math inline">\(w\cdot (p-q) = \|w\|\|p-q\|\cos(\theta)\)</span> where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(p-q\)</span> and <span class="math inline">\(w\)</span> – which is complementary to the angle between <span class="math inline">\(p-q\)</span> and the hyperplane. The distance <span class="math inline">\(D\)</span> is therefore <span class="math display">\[
D=\frac{w\cdot(p-q)}{\|w\|}.
\]</span> However, since <span class="math inline">\(q\)</span> lies on the hyperplane, we know that <span class="math inline">\(w\cdot q+b=0\)</span> so <span class="math inline">\(w\cdot q = -b\)</span>. Therefore <span class="math inline">\(w\cdot(p-q)=w\cdot p+b=f(p)\)</span>, which is the formula we seek.</p>
<div id="fig-triangle" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/triangle.png" class="img-fluid figure-img" style="width:30.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.3: Distance to a Hyperplane</figcaption><p></p>
</figure>
</div>
</section>
<section id="sec-linearseparable" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="sec-linearseparable"><span class="header-section-number">6.3.2</span> Linear separability and Margins</h3>
<p>Now we can return to our classification scheme. The following definition generalizes our two dimensional picture from the penguin data.</p>
<p><strong>Definition:</strong> Suppose that we have an <span class="math inline">\(n\times k\)</span> data matrix <span class="math inline">\(X\)</span> and a set of labels <span class="math inline">\(Y\)</span> that assign the <span class="math inline">\(n\)</span> samples to one of two classes. Then the labelled data is said to be <em>linearly separable</em> if there is a vector <span class="math inline">\(w\)</span> and a constant <span class="math inline">\(b\)</span> so that, if <span class="math inline">\(f(x)=w\cdot x+b\)</span>, then <span class="math inline">\(f(x)&gt;0\)</span> whenever <span class="math inline">\(x=(x_1,\ldots, x_k)\)</span> is a row of <span class="math inline">\(X\)</span> – a sample – belonging to the <span class="math inline">\(+1\)</span> class, and <span class="math inline">\(f(x)&lt;0\)</span> whenever <span class="math inline">\(x\)</span> belongs to the <span class="math inline">\(-1\)</span> class. The solutions to the equation <span class="math inline">\(f(x)=0\)</span> in this situation form a hyperplane that is called a <em>separating hyperplane</em> for the data.</p>
<p>In the situation where our data falls into two classes that are linearly separable, our classification strategy is to find a separating hyperplane <span class="math inline">\(f\)</span> for our training data. Then, given a point <span class="math inline">\(x\)</span> whose class we don’t know, we can evaluate <span class="math inline">\(f(x)\)</span> and assign <span class="math inline">\(x\)</span> to a class depending on whether <span class="math inline">\(f(x)&gt;0\)</span> or <span class="math inline">\(f(x)&lt;0\)</span>.</p>
<p>This definition begs two questions about a particular dataset:</p>
<ol type="1">
<li>How do we tell if the two classes are linearly separable?</li>
<li>If the two sets are linearly separable, there are infinitely many separating hyperplanes. To see this, look back at the penguin example and notice that we can ‘wiggle’ the red line a little bit and it will still separate the two sets. Which is the ‘best’ separating hyperplane?</li>
</ol>
<p>Let’s try to make the first of these two questions concrete. We have two sets of points <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span>, and we want to (try to) find a vector <span class="math inline">\(w\)</span> and a constant <span class="math inline">\(b\)</span> so that <span class="math inline">\(f(x)=w\cdot x+b\)</span> takes strictly positive values for <span class="math inline">\(x\in A\)</span> and strictly negative ones for <span class="math inline">\(x\in B\)</span>. Let’s approach the problem by first choosing <span class="math inline">\(w\)</span> and then asking whether there is a <span class="math inline">\(b\)</span> that will work. In the two dimensional case, this is equivalent to choosing the slope of our line, and then asking if we can find an intercept so that the line passes between the two classes.</p>
<p>In algebraic terms, we are trying to solve the following system of inequalities: given <span class="math inline">\(w\)</span>, find <span class="math inline">\(b\)</span> so that: <span class="math display">\[
w\cdot x+b&gt;0 \hbox{ for all $x$ in A}
\]</span> and <span class="math display">\[
w\cdot x+b&lt;0\hbox{ for all $x$ in B}.
\]</span> This is only going to be possible if there is a gap between the smallest value of <span class="math inline">\(w\cdot x\)</span> for <span class="math inline">\(x\in A\)</span> and the largest value of <span class="math inline">\(w\cdot x\)</span> for <span class="math inline">\(x\in B\)</span>. In other words, given <span class="math inline">\(w\)</span> there is a <span class="math inline">\(b\)</span> so that <span class="math inline">\(f(x)=w\cdot x+b\)</span> separates <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> if <span class="math display">\[
\max_{x\in B}w\cdot x &lt; \min_{x\in A} w\cdot x.
\]</span> If this holds, then choose <span class="math inline">\(b\)</span> so that <span class="math inline">\(-b\)</span> lies in this open interval and you will obtain a separating hyperplane.</p>
<p><strong>Proposition:</strong> The sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are linearly separable if there is a <span class="math inline">\(w\)</span> so that <span class="math display">\[
\max_{x\in B}w\cdot x &lt; \min_{x\in A} w\cdot x
\]</span> If this inequality holds for some <span class="math inline">\(w\)</span>, and <span class="math inline">\(-b\)</span> within this open interval, then <span class="math inline">\(f(x)=w\cdot x+b\)</span> is a separating hyperplane for <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>*<a href="#fig-penguinhwy2">Figure&nbsp;<span>6.4</span></a> is an illustration of this argument for a subset of the penguin data. Here, we have fixed <span class="math inline">\(w=(1.25,-1)\)</span> coming from the line <span class="math inline">\(y=1.25x+2\)</span> that we eyeballed earlier. For each Gentoo (green) point <span class="math inline">\(x_{i}\)</span>, we computed <span class="math inline">\(-b=w\cdot x_{i}\)</span> and drew the line <span class="math inline">\(f(x) = w\cdot x - w\cdot x_{i}\)</span> giving a family of parallel lines through each of the green points. Similarly for each Adelie (blue) point we drew the corresponding line. The maximum value of <span class="math inline">\(w\cdot x\)</span> for the blue points turned out to be <span class="math inline">\(1.998\)</span> and the minimum value of <span class="math inline">\(w\cdot x\)</span> for the green points turned out to be <span class="math inline">\(2.003\)</span>. Thus we have two lines with a gap between them, and any parallel line in that gap will separate the two sets.</p>
<p>Finally, among all the lines <em>with this particular <span class="math inline">\(w\)</span></em>, it seems that the <strong>best</strong> separating line is the one running right down the middle of the gap between the boundary lines. Any other line in the gap will be closer to either the blue or green set that the midpoint line is.</p>
<div id="fig-penguinhwy2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/penguinhwy2.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.4: Lines in Penguin Data for <span class="math inline">\(w=(1.25,-1)\)</span></figcaption><p></p>
</figure>
</div>
<p>Let’s put all of this together and see if we can make sense of it in general.</p>
<p>Suppose that <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> are finite point sets in <span class="math inline">\(\mathbf{R}^{k}\)</span> and <span class="math inline">\(w\in\mathbf{R}^{k}\)</span> such that <span class="math display">\[
B^{-}(w)=\max_{x\in A^{-}}w\cdot x &lt; \min_{x\in A^{+}}w\cdot x=B^{+}(w).
\]</span> Let <span class="math inline">\(x^{-}\)</span> be a point in <span class="math inline">\(A^{-}\)</span> with <span class="math inline">\(w\cdot x^{-}=B^{-}(w)\)</span> and <span class="math inline">\(x^{+}\)</span> be a point in <span class="math inline">\(A\)</span> with <span class="math inline">\(w\cdot x^{+}=B^{+}(w)\)</span>. The two hyperplanes <span class="math inline">\(f^{\pm}(x) = w\cdot x - B^{\pm}\)</span> have the property that: <span class="math display">\[
f^{+}(x)\ge 0\hbox{ for }x\in A^{+}\hbox{ and }f^{+}(x)&lt;0\hbox{ for }x\in A^{-}
\]</span> and <span class="math display">\[
f^{-}(x)\le 0\hbox{ for }x\in A^{-}\hbox{ and }f^{-}(x)&gt;0\hbox{ for }x\in A^{+}
\]</span></p>
<p>Hyperplanes like <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span>, which “just touch” a set of points, are called supporting hyperplanes.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(A\)</span> be a set of points in <span class="math inline">\(\mathbf{R}^{k}\)</span>. A hyperplane <span class="math inline">\(f(x)=w\cdot x+b=0\)</span> is called a <em>supporting hyperplane</em> for <span class="math inline">\(A\)</span> if <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(x\in A\)</span> and <span class="math inline">\(f(x)=0\)</span> for at least one point in <span class="math inline">\(A\)</span>, or if <span class="math inline">\(f(x)\le 0\)</span> for all <span class="math inline">\(x\in A\)</span> and <span class="math inline">\(f(x)=0\)</span> for at least one point in <span class="math inline">\(A\)</span>.</p>
<p>The gap between the two supporting hyperplanes <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> is called the <em>margin</em> between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> for <span class="math inline">\(w\)</span>.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> be as in the discussion above for point sets <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> and vector <span class="math inline">\(w\)</span>. Then the orthogonal distance between the two hyperplanes <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> is called the geometric margin <span class="math inline">\(\tau_{w}(A^{+},A^{-})\)</span> (along <span class="math inline">\(w\)</span>) between <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>. We have <span class="math display">\[
\tau_{w}(A^{+},A^{-})=\frac{B^{+}(w)-B^{-}(w)}{\|w\|}.
\]</span></p>
<p>Now we can propose an answer to our second question about the best classifying hyperplane.</p>
<p><strong>Definition:</strong> The <em>optimal margin</em> <span class="math inline">\(\tau(A^{+},A^{-})\)</span> between <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> is the largest value of <span class="math inline">\(\tau_{w}\)</span> over all possible <span class="math inline">\(w\)</span> for which <span class="math inline">\(B^{-}(w)&lt;B^{+}(w)\)</span>: <span class="math display">\[
\tau(A^{+},A^{-}) = \max_{w} \tau_{w}(A^{+},A^{-}).
\]</span> If <span class="math inline">\(w\)</span> is such that <span class="math inline">\(\tau_{w}=\tau\)</span>, then the hyperplane <span class="math inline">\(f(x)=w\cdot x - \frac{(B^{+}+B^{-})}{2}\)</span> is the <em>optimal margin classifying hyperplane</em>.</p>
<p>The optimal classifying hyperplane runs “down the middle” of the gap between the two supporting hyperplanes <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> that give the sides of the optimal margin.</p>
<p>We can make one more observation about the maximal margin. If we find a vector <span class="math inline">\(w\)</span> so that <span class="math inline">\(f^{+}(x) = w\cdot x -B^{+}\)</span> and <span class="math inline">\(f^{-}(x) = w\cdot x-B^{-}\)</span> are the two supporting hyperplanes such that the gap between them is the optimal margin, then this gap gives us an estimate on how close together the points in <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> can be. This is visible in <a href="#fig-penguinhwy2">Figure&nbsp;<span>6.4</span></a>, where it’s clear that to get from a blue point to a green one, you have to cross the gap between the two supporting hyperplanes.</p>
<p><strong>Proposition:</strong> The closest distance between points in <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> is greater than or equal to the optimal margin: <span class="math display">\[
\min_{p\in A^{+},q\in A^{-}} \|p-q\|\ge \tau(A^{+},A^{-})
\]</span>.</p>
<p><strong>Proof:</strong> We have <span class="math inline">\(f^{+}(p) = w\cdot p - B^{+}\ge 0\)</span> and <span class="math inline">\(f^{-}(q) = w\cdot q -B^{-}\le 0\)</span>. These two inequalities imply that <span class="math display">\[
w\cdot (p-q)\ge B^{+}-B^{-}&gt;0.
\]</span> Therefore <span class="math display">\[
\|p-q\|\|w\|\ge |w\cdot (p-q)|\ge |B^{+}-B^{-}|
\]</span> and so <span class="math display">\[
\|p-q\| \ge \frac{B^{+}-B^{-}}{\|w\|} = \tau(A^{+},A^{-})
\]</span></p>
<p>If this inequality were always <em>strict</em> – that is, if the optimal margin equalled the minimum distance between points in the two clusters – then this would give us an approach to finding this optimal margin.</p>
<p>Unfortunately, that isn’t the case. In <a href="#fig-nonstrict">Figure&nbsp;<span>6.5</span></a>, we show a very simple case involving only six points in total in which the distance between the closest points in <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> is larger than the optimal margin.</p>
<div id="fig-nonstrict" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/margindistance2.png" style="height:3in" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.5: Shortest distance between + and - points can be greater than the optimal margin</figcaption><p></p>
</figure>
</div>
<p>At least now our problem is clear. Given our two point sets <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>, find <span class="math inline">\(w\)</span> so that <span class="math inline">\(\tau_{w}(A^{+},A^{-})\)</span> is maximal among all <span class="math inline">\(w\)</span> where <span class="math inline">\(B^{-}(w)&lt;B^{+}(w)\)</span>. This is an optimization problem, but unlike the optimization problems that arose in our discussions of linear regression and principal component analysis, it does not have a closed form solution. We will need to find an algorithm to determine <span class="math inline">\(w\)</span> by successive approximations. Developing that algorithm will require thinking about a new concept known as <em>convexity.</em></p>
</section>
</section>
<section id="convexity-convex-hulls-and-margins" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="convexity-convex-hulls-and-margins"><span class="header-section-number">6.4</span> Convexity, Convex Hulls, and Margins</h2>
<p>In this section we introduce the notion of a <em>convex set</em> and the particular case of the <em>convex hull</em> of a finite set of points. As we will see, these ideas will give us a different interpretation of the margin between two sets and will eventually lead to an algorithm for finding the optimal margin classifier.</p>
<p><strong>Definition:</strong> A subset <span class="math inline">\(U\)</span> of <span class="math inline">\(\mathbf{R}^{k}\)</span> is <em>convex</em> if, for any pair of points <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> in <span class="math inline">\(U\)</span>, every point <span class="math inline">\(t\)</span> on the line segment joining <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> also belongs to <span class="math inline">\(U\)</span>. In vector form, for every <span class="math inline">\(0\le s\le 1\)</span>, the point <span class="math inline">\(t(s) = (1-s)p+sq\)</span> belongs to <span class="math inline">\(U\)</span>. (Note that <span class="math inline">\(t(0)=p\)</span>, <span class="math inline">\(t(1)=q\)</span>, and so <span class="math inline">\(t(s)\)</span> traces out the segment joining <span class="math inline">\(p\)</span> to <span class="math inline">\(q\)</span>.)</p>
<p>*<a href="#fig-convexnotconvex">Figure&nbsp;<span>6.6</span></a> illustrates the difference between convex sets and non-convex ones.</p>
<div id="fig-convexnotconvex" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ConvexNotConvex.png" style="height:3in" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.6: Convex vs Non-Convex Sets</figcaption><p></p>
</figure>
</div>
<p>The key idea from convexity that we will need to solve our optimization problem and find the optimal margin is the idea of the <em>convex hull</em> of a finite set of points in <span class="math inline">\(\mathbf{R}^{k}\)</span>.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(S=\{q_1,\ldots, q_{N}\}\)</span> be a finite set of <span class="math inline">\(N\)</span> points in <span class="math inline">\(\mathbf{R}^{k}\)</span>. The <em>convex hull</em> <span class="math inline">\(C(S)\)</span> of <span class="math inline">\(S\)</span> is the set of points <span class="math display">\[
p = \sum_{i=1}^{N} \lambda_{i}q_{i}
\]</span> as <span class="math inline">\(\lambda_{1},\ldots,\lambda_{N}\)</span> runs over all positive real numbers such that <span class="math display">\[
\sum_{i=1}^{N} \lambda_{i} = 1.
\]</span></p>
<p>There are a variety of ways to think about the convex hull <span class="math inline">\(C(S)\)</span> of a set of points <span class="math inline">\(S\)</span>, but perhaps the most useful is that it is the smallest convex set that contains all of the points of <span class="math inline">\(S\)</span>. That is the content of the next lemma.</p>
<p><strong>Lemma:</strong> <span class="math inline">\(C(S)\)</span> is convex. Furthermore, let <span class="math inline">\(U\)</span> be any convex set containing all of the points of <span class="math inline">\(S\)</span>. Then <span class="math inline">\(U\)</span> contains <span class="math inline">\(C(S)\)</span>.</p>
<p><strong>Proof:</strong> To show that <span class="math inline">\(C(S)\)</span> is convex, we apply the definition. Let <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> be two points in <span class="math inline">\(C(S)\)</span>, so that let <span class="math inline">\(p_{j}=\sum_{i=1}^{N} \lambda^{(j)}_{i}q_{i}\)</span> where <span class="math inline">\(\sum_{i=1}^{N}\lambda^{(j)}_{i} = 1\)</span> for <span class="math inline">\(j=1,2\)</span>. Then a little algebra shows that <span class="math display">\[
(1-s)p_1+sp_{2} = \sum_{i=1}^{N} (s\lambda^{(1)}_{i}+(1-s)\lambda^{(2)}_{i})q_{i}
\]</span> and <span class="math inline">\(\sum_{i=1}^{N} (s\lambda^{(1)}_{i}+(1-s)\lambda^{(2)}_{i}) = 1\)</span>. Therefore all of the points <span class="math inline">\((1-s)p_{1}+sp_{2}\)</span> belong to <span class="math inline">\(C(S)\)</span>, and therefore <span class="math inline">\(C(S)\)</span> is convex.</p>
<p>For the second part, we proceed by induction. Let <span class="math inline">\(U\)</span> be a convex set containing <span class="math inline">\(S\)</span>. Then by the definition of convexity, <span class="math inline">\(U\)</span> contains all sums <span class="math inline">\(\lambda_{i}q_{i}+\lambda_{j}q_{j}\)</span> where <span class="math inline">\(\lambda_i+\lambda_j=1\)</span>. Now suppose that <span class="math inline">\(U\)</span> contains all the sums <span class="math inline">\(\sum_{i=1}^{N} \lambda_{i}q_{i}\)</span> where exactly <span class="math inline">\(m-1\)</span> of the <span class="math inline">\(\lambda_{i}\)</span> are non-zero for some <span class="math inline">\(m&lt;N\)</span>.<br>
Consider a sum <span class="math display">\[
q = \sum_{i=1}^{N}\lambda_{i}q_{i}
\]</span> with exactly <span class="math inline">\(m\)</span> of the <span class="math inline">\(\lambda_{i}\not=0\)</span>. For simplicity let’s assume that <span class="math inline">\(\lambda_{i}\not=0\)</span> for <span class="math inline">\(i=1,\ldots, m\)</span>. Now let <span class="math inline">\(T=\sum_{i=1}^{m-1}\lambda_{i}\)</span> and set <span class="math display">\[
q' = \sum_{i=1}^{m-1}\frac{\lambda_{i}}{T}q_{i}.
\]</span> This point <span class="math inline">\(q'\)</span> belongs to <span class="math inline">\(U\)</span> by the inductive hypothesis. Also, <span class="math inline">\((1-T)=\lambda_{m}\)</span>. Therefore by convexity of <span class="math inline">\(U\)</span>, <span class="math display">\[
q = (1-T)q_{m}+Tq'
\]</span> also belongs to <span class="math inline">\(U\)</span>. It follows that all of <span class="math inline">\(C(S)\)</span> belongs to <span class="math inline">\(U\)</span>.</p>
<p>In <a href="#fig-convexhull">Figure&nbsp;<span>6.7</span></a> we show our penguin data together with the convex hull of points corresponding to the two types of penguins. Notice that the boundary of each convex hull is a finite collection of line segments that join the “outermost” points in the point set.</p>
<div id="fig-convexhull" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/penguinswithhulls.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.7: The Convex Hull</figcaption><p></p>
</figure>
</div>
<p>One very simple example of a convex set is a half-plane. More specifically, if <span class="math inline">\(f(x)=w\cdot x+b=0\)</span> is a hyperplane, then the two “sides” of the hyperplane, meaning the subsets <span class="math inline">\(\{x: f(x)\ge 0\}\)</span> and <span class="math inline">\(\{x: f(x)\le 0\}\)</span>, are both convex. (This is exercise 1 in <a href="#sec-exercises"><span>Section&nbsp;6.7</span></a> ).</p>
<p>As a result of this observation, and the Lemma above, we can conclude that if <span class="math inline">\(f(x)=w\cdot x+b=0\)</span> is a supporting hyperplane for the set <span class="math inline">\(S\)</span> – meaning that either <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(x\in S\)</span>, or <span class="math inline">\(f(x)\le 0\)</span> for all <span class="math inline">\(x\in S\)</span>, with at least one point <span class="math inline">\(x\in S\)</span> such that <span class="math inline">\(f(x)=0\)</span> – then <span class="math inline">\(f(x)=0\)</span> is a supporting hyperplane for the entire convex hull. After all, if <span class="math inline">\(f(x)\ge 0\)</span> for all points <span class="math inline">\(x\in S\)</span>, then <span class="math inline">\(S\)</span> is contained in the convex set of points where <span class="math inline">\(f(x)\ge 0\)</span>, and therefore <span class="math inline">\(C(S)\)</span> is contained in that set as well.</p>
<p>Interestingly, however, the converse is true as well – the supporting hyperplanes of <span class="math inline">\(C(S)\)</span> are exactly the same as those for <span class="math inline">\(S\)</span>.</p>
<p><strong>Lemma:</strong> Let <span class="math inline">\(S\)</span> be a finite set of points in <span class="math inline">\(\mathbf{R}^{k}\)</span> and let <span class="math inline">\(f(x)=w\cdot x +b=0\)</span> be a supporting hyperplane for <span class="math inline">\(C(S)\)</span>. Then <span class="math inline">\(f(x)\)</span> is a supporting hyperplane for <span class="math inline">\(S\)</span>.</p>
<p><strong>Proof:</strong> Suppose <span class="math inline">\(f(x)=0\)</span> is a supporting hyperplane for <span class="math inline">\(C(S)\)</span>. Let’s assume that <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(x\in C(S)\)</span> and <span class="math inline">\(f(x^{*})=0\)</span> for a point <span class="math inline">\(x^{*}\in C(S)\)</span>, since the case where <span class="math inline">\(f(x)\le 0\)</span> is identical. Since <span class="math inline">\(S\subset C(S)\)</span>, we have <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(x\in S\)</span>. To show that <span class="math inline">\(f(x)=0\)</span> is a supporting hyperplane, we need to know that <span class="math inline">\(f(x)=0\)</span> for at least one point <span class="math inline">\(x\in S\)</span>.<br>
Let <span class="math inline">\(x'\)</span> be the point in <span class="math inline">\(S\)</span> where <span class="math inline">\(f(x')\)</span> is minimal among all <span class="math inline">\(x\in S\)</span>. Note that <span class="math inline">\(f(x')\ge 0\)</span>. Then the hyperplane <span class="math inline">\(g(x) = f(x)-f(x')\)</span> has the property that <span class="math inline">\(g(x)\ge 0\)</span> on all of <span class="math inline">\(S\)</span>, and <span class="math inline">\(g(x')=0\)</span>. Since the halfplane <span class="math inline">\(g(x)\ge 0\)</span> is convex and contains all of <span class="math inline">\(S\)</span>, we have <span class="math inline">\(C(S)\)</span> contained in that halfplane. So, on the one hand we have <span class="math inline">\(g(x^{*})=f(x^{*})-f(x')\ge 0\)</span>. On the other hand <span class="math inline">\(f(x^{*})=0\)</span>, so <span class="math inline">\(-f(x')\ge 0\)</span>, so <span class="math inline">\(f(x')\le 0\)</span>. Since <span class="math inline">\(f(x')\)</span> is also greater or equal to zero, we have <span class="math inline">\(f(x')=0\)</span>, and so we have found a point of <span class="math inline">\(S\)</span> on the hyperplane <span class="math inline">\(f(x)=0\)</span>. Therefore <span class="math inline">\(f(x)=0\)</span> is also a supporting hyperplane for <span class="math inline">\(S\)</span>.</p>
<p>This argument can be used to give an alternative description of <span class="math inline">\(C(S)\)</span> as the intersection of all halfplanes containing <span class="math inline">\(S\)</span> arising from supporting hyperplanes for <span class="math inline">\(S\)</span>. This is exercise 2 in <a href="#sec-exercises"><span>Section&nbsp;6.7</span></a>. It also has as a corollary that <span class="math inline">\(C(S)\)</span> is a closed set.</p>
<p><strong>Lemma:</strong> <span class="math inline">\(C(S)\)</span> is compact.</p>
<p><strong>Proof:</strong> Exercise 2 in <a href="#sec-exercises"><span>Section&nbsp;6.7</span></a> shows that it is the intersection of closed sets in <span class="math inline">\(\mathbf{R}^{k}\)</span>, so it is closed. Exercise 3 shows that <span class="math inline">\(C(S)\)</span> is bounded. Thus it is compact.</p>
<p>Now let’s go back to our optimal margin problem, so that we have linearly separable sets of points <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>. Recall that we showed that the optimal margin was at most the minimal distance between points in <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>, but that there could be a gap between the minimal distance and the optimal margin – see <a href="#fig-nonstrict">Figure&nbsp;<span>6.5</span></a> for a reminder.</p>
<p>It turns out that by considering the minimal distance between <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span>, we can “close this gap.” The following proposition shows that we can change the problem of finding the optimal margin into the problem of finding the closest distance between the convex hulls of <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span>. The following proposition generalizes the Proposition at the end of <a href="#sec-linearseparable"><span>Section&nbsp;6.3.2</span></a>.</p>
<p><strong>Proposition:</strong> Let <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> be linearly separable sets in <span class="math inline">\(\mathbf{R}^{k}\)</span>. Let <span class="math inline">\(p\in C(A^{+})\)</span> and <span class="math inline">\(q\in C(A^{-})\)</span> be any two points. Then <span class="math display">\[
\|p-q\|\ge \tau(A^{+},A^{-}).
\]</span></p>
<p><strong>Proof:</strong> As in the earlier proof, choose supporting hyperplanes <span class="math inline">\(f^{+}(x)=w\cdot x-B^{+}=0\)</span> and <span class="math inline">\(f^{-}(x)=w\cdot x-B^{-}\)</span> for <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>. By our discussion above, these are also supporting hyperplanes for <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span>. Therefore if <span class="math inline">\(p\in C(A^{+})\)</span> and <span class="math inline">\(q\in C(A^{-})\)</span>, we have <span class="math inline">\(w\cdot p-B^{+}\ge 0\)</span> and <span class="math inline">\(w\cdot q-B^{-}\le 0\)</span>. As before <span class="math display">\[
w\cdot(p-q)\ge B^{+}-B^{-}&gt;0
\]</span> and so <span class="math display">\[
\|p-q\|\ge\frac{B^{+}-B^{-}}{\|w\|}=\tau_{w}(A^{+},A^{-})
\]</span> Since this holds for any <span class="math inline">\(w\)</span>, we have the result for <span class="math inline">\(\tau(A^{+},A^{-})\)</span>.</p>
<p>The reason this result is useful is that, as we’ve seen, if we restrict <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> to <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>, then there can be a gap between the minimal distance and the optimal margin. If we allow <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> to range over the convex hulls of these sets, then that gap disappears.</p>
<p>One other consequence of this is that if <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> are linearly separable then their convex hulls are disjoint.</p>
<p><strong>Corollary:</strong> If <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> are linearly separable then <span class="math inline">\(\|p-q\|&gt;0\)</span> for all <span class="math inline">\(p\in C(A^{+})\)</span> and <span class="math inline">\(q\in C(A^{-})\)</span></p>
<p><strong>Proof:</strong> The sets are linearly separable precisely when <span class="math inline">\(\tau&gt;0\)</span>.</p>
<p>Our strategy now is to show that if <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are points in <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span> respectively that are at minimal distance <span class="math inline">\(D\)</span>, and if we set <span class="math inline">\(w=p-q\)</span>, then we obtain supporting hyperplanes with margin equal to <span class="math inline">\(\|p-q\|\)</span>. Since this margin is the <em>largest possible margin</em>, this <span class="math inline">\(w\)</span> must be the optimal <span class="math inline">\(w\)</span>. This transforms the problem of finding the optimal margin into the problem of finding the closest points in the convex hulls.</p>
<p><strong>Lemma:</strong> Let <span class="math display">\[
D=\min_{p\in C(A^{+}),q\in C(A^{-})} \|p-q\|.
\]</span> Then there are points <span class="math inline">\(p^*\in C(A^{+})\)</span> and <span class="math inline">\(q^{*}\in C(A^{-})\)</span> with <span class="math inline">\(\|p^{*}-q^{*}\|=D\)</span>. If <span class="math inline">\(p_1^{*},q_1^{*}\)</span> and <span class="math inline">\(p_2^{*},q_2^{*}\)</span> are two pairs of points satisfying this condition, then <span class="math inline">\(p_1^{*}-q_1^{*}=p_2^{*}-q_{2}^{*}\)</span>.</p>
<p><strong>Proof:</strong> Consider the set of differences <span class="math display">\[
V = \{p-q: p\in C(A^{+}),q\in C(A^{-})\}.
\]</span></p>
<ul>
<li><p><span class="math inline">\(V\)</span> is compact. This is because it is the image of the compact set <span class="math inline">\(C(A^{+})\times C(A^{-})\)</span> in <span class="math inline">\(\mathbf{R}^{k}\times\mathbf{R}^{k}\)</span> under the continuous map <span class="math inline">\(h(x,y)=x-y\)</span>.</p></li>
<li><p>the function <span class="math inline">\(d(v)=\|v\|\)</span> is continuous and satisfies <span class="math inline">\(d(v)\ge D&gt;0\)</span> for all <span class="math inline">\(v\in V\)</span>.</p></li>
</ul>
<p>Since <span class="math inline">\(d\)</span> is a continuous function on a compact set, it attains its minimum <span class="math inline">\(D\)</span> and so there is a <span class="math inline">\(v=p^{*}-q^{*}\)</span> with <span class="math inline">\(d(v)=D\)</span>.</p>
<p>Now suppose that there are two distinct points <span class="math inline">\(v_1=p_1^*-q_1^*\)</span> and <span class="math inline">\(v_2=p_2^*-q_2^*\)</span> with <span class="math inline">\(d(v_1)=d(v_2)=D\)</span>. Consider the line segment <span class="math display">\[
t(s) = (1-s)v_1+sv_2\hbox{ where }0\le s\le 1
\]</span> joining <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>.<br>
Now <span class="math display">\[
t(s) = ((1-s)p_1^*+sp_2^*)-((1-s)q_1^*+sq_2^*).
\]</span> Both terms in this difference belong to <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span> respectively, regardless of <span class="math inline">\(s\)</span>, by convexity, and therefore <span class="math inline">\(t(s)\)</span> belongs to <span class="math inline">\(V\)</span> for all <span class="math inline">\(0\le s\le 1\)</span>.</p>
<p>This little argument shows that <span class="math inline">\(V\)</span> is convex. In geometric terms, <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are two points in the set <span class="math inline">\(V\)</span> equidistant from the origin and the segment joining them is a chord of a circle; as <a href="#fig-chord">Figure&nbsp;<span>6.8</span></a> shows, in that situation there must be a point on the line segment joining them that’s closer to the origin than they are. Since all the points on that segment are in <span class="math inline">\(V\)</span> by convexity, this would contradict the assumption that <span class="math inline">\(v_1\)</span> is the closet point in <span class="math inline">\(V\)</span> to the origin.</p>
<div id="fig-chord" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/chord2.png" class="img-fluid figure-img" style="width:3in"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.8: Chord of a circle</figcaption><p></p>
</figure>
</div>
<p>In algebraic terms, since <span class="math inline">\(D\)</span> is the minimal value of <span class="math inline">\(\|v\|\)</span> for all <span class="math inline">\(v\in V\)</span>, we must have <span class="math inline">\(t(s)\ge D\)</span>.<br>
On the other hand <span class="math display">\[
\frac{d}{ds}\|t(s)\|^2 = \frac{d}{ds}(t(s)\cdot t(s)) =t(s)\cdot \frac{dt(s)}{ds} = t(s)\cdot(v_2-v_1).
\]</span> Therefore <span class="math display">\[
\frac{d}{ds}\|t(s)\|^2|_{s=0} = v_{1}\cdot(v_{2}-v_{1})=v_{1}\cdot v_{2}-\|v_{1}\|^2\le 0
\]</span> since <span class="math inline">\(v_{1}\cdot v_{2}\le D^{2}\)</span> and <span class="math inline">\(\|v_{1}\|^2=D^2\)</span>. If <span class="math inline">\(v_{1}\cdot v_{2}&lt;D^{2}\)</span>, then this derivative would be negative, which would mean that there is a value of <span class="math inline">\(s\)</span> where <span class="math inline">\(t(s)\)</span> would be less than <span class="math inline">\(D\)</span>. Since that can’t happen, we conclude that <span class="math inline">\(v_{1}\cdot v_{2}=D^{2}\)</span> which means that <span class="math inline">\(v_{1}=v_{2}\)</span> – the vectors have the same magnitude <span class="math inline">\(D\)</span> and are parallel. This establishes uniqueness.</p>
<p><strong>Note:</strong> The essential ideas of this argument show that a compact convex set in <span class="math inline">\(\mathbf{R}^{k}\)</span> has a unique point closest to the origin. The convex set in this instance, <span class="math display">\[
V=\{p-q:p\in C(A^{+}),q\in C(A^{-})\},
\]</span> is called the difference <span class="math inline">\(C(A^{+})-C(A^{-})\)</span>, and it is generally true that the difference of convex sets is convex.</p>
<p>Now we can conclude this line of argument.</p>
<p><strong>Theorem:</strong> Let <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> be points in <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span> respectively are such that <span class="math inline">\(\|p-q\|\)</span> is minimal among all such pairs. Let <span class="math inline">\(w=p-q\)</span> and set <span class="math inline">\(B^{+}=w\cdot p\)</span> and <span class="math inline">\(B^{-}=w\cdot q\)</span>. Then <span class="math inline">\(f^{+}(x)=w\cdot x-B^{+}=0\)</span> and <span class="math inline">\(f^{-}(x)=w\cdot x-B^{-}\)</span> are supporting hyperplanes for <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span> respectively and the associated margin <span class="math display">\[
\tau_{w}(A^{+},A^{-})=\frac{B^{+}-B^{-}}{\|w\|} = \|p-q\|
\]</span> is optimal.</p>
<p><strong>Proof:</strong> First we show that <span class="math inline">\(f^{+}(x)=0\)</span> is a supporting hyperplane for <span class="math inline">\(C(A^{+})\)</span>. Suppose not. Then there is a point <span class="math inline">\(p'\in C(A^{+})\)</span> such that <span class="math inline">\(f^{+}(x)&lt;0\)</span>. Consider the line segment <span class="math inline">\(t(s) = (1-s)p+sp'\)</span> running from <span class="math inline">\(p\)</span> to <span class="math inline">\(p'\)</span>. By convexity it is entirely contained in <span class="math inline">\(C(A^{+})\)</span>. Now look at the distance from points on this segment to <span class="math inline">\(q\)</span>: <span class="math display">\[
D(s)=\|t(s)-q\|^2.
\]</span> We have <span class="math display">\[
\frac{dD(s)}{ds}|_{s=0} = 2(p-q)\cdot (p'-p) = 2w\cdot (p'-p) = 2\left[(f^{+}(p')+B^{+})-(f^{+}(p)+B^{+})\right]
\]</span> so <span class="math display">\[
\frac{dD(s)}{ds}|_{s=0} = 2(f^{+}(p')-f^{+}(p))&lt;0
\]</span> since <span class="math inline">\(f(p)=0\)</span>. This means that <span class="math inline">\(D(s)\)</span> is decreasing along <span class="math inline">\(t(s)\)</span> and so there is a point <span class="math inline">\(s'\)</span> along <span class="math inline">\(t(s)\)</span> where <span class="math inline">\(\|t(s')-q\|&lt;D\)</span>. This contradicts the fact that <span class="math inline">\(D\)</span> is the minimal distance. The same argument shows that <span class="math inline">\(f^{-}(x)=0\)</span> is also a supporting hyperplane.</p>
<p>Now the margin for this <span class="math inline">\(w\)</span> is <span class="math display">\[
\tau_{w}(A^{+},A^{-}) = \frac{w\cdot (p-q)}{\|w\|} = \|p-q\|=D
\]</span> and as <span class="math inline">\(w\)</span> varies we know this is the largest possible <span class="math inline">\(\tau\)</span> that can occur. Thus this is the maximal margin.</p>
<p>*<a href="#fig-strict">Figure&nbsp;<span>6.9</span></a> shows how considering the closest point in the convex hulls “fixes” the problem that we saw in <a href="#fig-nonstrict">Figure&nbsp;<span>6.5</span></a>. The closest point occurs at a point on the boundary of the convex hull that is not one of the points in <span class="math inline">\(A^{+}\)</span> or <span class="math inline">\(A^{-}\)</span>.</p>
<div id="fig-strict" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ConvexHullWithMargin.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.9: Closest distance between convex hulls gives optimal margin</figcaption><p></p>
</figure>
</div>
</section>
<section id="finding-the-optimal-margin-classifier" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="finding-the-optimal-margin-classifier"><span class="header-section-number">6.5</span> Finding the Optimal Margin Classifier</h2>
<p>Now that we have translated our problem into geometry, we can attempt to develop an algorithm for solving it. To recap, we have two sets of points <span class="math display">\[
A^{+}=\{x^+_1,\ldots, x^+_{n_{+}}\}
\]</span> and <span class="math display">\[
A^{-}=\{x^-_1,\ldots, x^-_{n_{-}}\}
\]</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span> that are linearly separable.</p>
<p>We wish to find points <span class="math inline">\(p\in C(A^{+})\)</span> and <span class="math inline">\(q\in C(A^{-})\)</span> such that <span class="math display">\[
\|p-q\|=\min_{p'\in C(A^{+}),q'\in C(A^{-})} \|p'-q'\|.
\]</span></p>
<p>Using the definition of the convex hull we can express this more concretely. Since <span class="math inline">\(p\in C(A^{+})\)</span>, there are coefficients <span class="math inline">\(\lambda^{+}_{i}\ge 0\)</span> for <span class="math inline">\(i=1,\ldots,n_{+}\)</span> and <span class="math inline">\(\lambda^{-}_{i}\ge 0\)</span> for <span class="math inline">\(i=1,\ldots, n_{-}\)</span> so that <span class="math display">\[
\begin{aligned}
p&amp;=&amp;\sum_{i=1}^{n_{+}}\lambda^{+}_{i} x^{+}_{i} \\
q&amp;=&amp;\sum_{i=1}^{n_{-}}\lambda^{-}_{i} x^{-}_{i} \\
\end{aligned}
\]</span> where <span class="math inline">\(\sum_{i=1}^{n_{\pm}} \lambda_{i}^{\pm}=1\)</span>.</p>
<p>We can summarize this as follows:</p>
<p><strong>Optimization Problem 1:</strong> Write <span class="math inline">\(\lambda^{\pm}=(\lambda^{\pm}_{1},\ldots, \lambda^{\pm}_{n_{\pm}})\)</span> Define <span class="math display">\[
w(\lambda^+,\lambda^-) = \sum_{i=1}^{n_{+}}\lambda^{+}_{i}x^{+}_{i} - \sum_{i=1}^{n_{-}}\lambda^{-}x^{-}_{i}
\]</span> To find the supporting hyperplanes that define the optimal margin between <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>, find <span class="math inline">\(\lambda^{+}\)</span> and <span class="math inline">\(\lambda^{-}\)</span> such that <span class="math inline">\(\|w(\lambda^{+},\lambda^{-})\|^2\)</span> is minimal among all such <span class="math inline">\(w\)</span> where all <span class="math inline">\(\lambda^{\pm}_{i}\ge 0\)</span> and <span class="math inline">\(\sum_{i=1}^{n_{\pm}} \lambda^{\pm}_{i}=1\)</span>.</p>
<p>This is an example of a <em>constrained optimization problem.</em> It’s worth observing that the <em>objective function</em> <span class="math inline">\(\|w(\lambda^{+},\lambda^{-})\|^2\)</span> is just a quadratic function in the <span class="math inline">\(\lambda^{\pm}.\)</span> Indeed we can expand <span class="math display">\[
\|w(\lambda^{+},\lambda^{-})\|^2 = (\sum_{i=1}^{n_{+}}\lambda^{+}_{i}x_{i}- \sum_{i=1}^{n_{-}}\lambda^{-}x^{-}_{i})\cdot(\sum_{i=1}^{n_{+}}\lambda^{+}_{i}x_{i}- \sum_{i=1}^{n_{-}}\lambda^{-}x^{-}_{i})
\]</span> to obtain <span class="math display">\[
\|w(\lambda^{+},\lambda^{-})\|^2 = R -2S +T
\]</span> where <span id="eq-kernel"><span class="math display">\[
\begin{aligned}
R &amp;=&amp; \sum_{i=1}^{n_{+}}\sum_{j=1}^{n_{+}}\lambda^{+}_{i}\lambda^{+}_{j}(x^{+}_{i}\cdot x^{+}_{j}) \\
S &amp;=&amp; \sum_{i=1}^{n_{+}}\sum_{j=1}^{n_{-}}\lambda^{+}_{i}\lambda^{-}_{j}(x^{+}_{i}\cdot x^{-}_{j}) \\
T &amp;=&amp; \sum_{i=1}^{n_{-}}\sum_{j=1}^{n_{-}}\lambda^{-}_{i}\lambda^{-}_{j}(x^{-}_{i}\cdot x^{-}_{j}) \\
\end{aligned}
\tag{6.2}\]</span></span> Thus the function we are trying to minimize is relatively simple.</p>
<p>On the other hand, unlike optimization problems we have seen earlier in these lectures, in which we can apply Lagrange multipliers, in this case some of the constraints are inequalities – namely the requirement that all of the <span class="math inline">\(\lambda^{\pm}\ge 0\)</span> – rather than equalities. There is an extensive theory of such problems that derives from the idea of Lagrange multipliers. However, in these notes, we will not dive into that theory but will instead construct an algorithm for solving the problem directly.</p>
<section id="relaxing-the-constraints" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="relaxing-the-constraints"><span class="header-section-number">6.5.1</span> Relaxing the constraints</h3>
<p>Our first step in attacking this problem is to adjust our constraints and our objective function slightly so that the problem becomes easier to attack.</p>
<p><strong>Optimization Problem 2:</strong> This is a slight revision of problem 1 above. We minimize: <span class="math display">\[
Q(\lambda^{+},\lambda^{-}) = \|w(\lambda^{+},\lambda^{-})\|^2-\sum_{i=1}^{n_{+}}\lambda^{+}_{i}-\sum_{i=1}^{n_{-}}\lambda^{-}_{i}
\]</span> subject to the constraints that all <span class="math inline">\(\lambda^{\pm}_{i}\ge 0\)</span> and <span class="math display">\[
\alpha = \sum_{i=1}^{n_{+}}\lambda^+_{i} = \sum_{i=1}^{n_{-}}\lambda^{-}_{i}.
\]</span></p>
<p>Problem 2 is like problem 1, except we don’t require the sums of the <span class="math inline">\(\lambda^{\pm}_{i}\)</span> to be one, but only that they be equal to each other; and we modify the objective function slightly. It turns out that the solution to this optimization problem easily yields the solution to our original one.</p>
<p><strong>Lemma:</strong> Suppose <span class="math inline">\(\lambda^{+}\)</span> and <span class="math inline">\(\lambda^{-}\)</span> satisfy the constraints of problem 2 and yield the minimal value for the objective function <span class="math inline">\(Q(\lambda^{+},\lambda^{-})\)</span>. Then <span class="math inline">\(\alpha\not=0\)</span>. Rescale the <span class="math inline">\(\lambda^{\pm}\)</span> to have sum equal to one by dividing by <span class="math inline">\(\alpha\)</span>, yielding <span class="math inline">\(\tau^{\pm}=(1/\alpha)\lambda^{\pm}\)</span>. Then <span class="math inline">\(w(\tau^{+},\tau^{-})\)</span> is a solution to optimization problem 1.</p>
<p><strong>Proof:</strong> To show that <span class="math inline">\(\alpha\not=0\)</span>, suppose that <span class="math inline">\(\lambda^{\pm}_{i}=0\)</span> for all <span class="math inline">\(i\not=1\)</span> and <span class="math inline">\(\lambda=\lambda^{+}_{1}=\lambda^{-}_{1}\)</span>. The one-variable quadratic function <span class="math inline">\(Q(\lambda)\)</span> takes its minimum value at <span class="math inline">\(\lambda=1/\|x_{1}^{+}-x_{1}^{-}\|^2\)</span> and its value at that point is negative. Therefore the minimum value of <span class="math inline">\(Q\)</span> is negative, which means <span class="math inline">\(\alpha\not=0\)</span> at that minimum point.</p>
<p>For the equivalence, notice that <span class="math inline">\(\tau^{\pm}\)</span> still satisfy the constraints of problem 2. Therefore <span class="math display">\[
Q(\lambda^{+},\lambda^{-}) = \|w(\lambda^{+},\lambda^{-})\|^2-2\alpha\le \|w(\tau^{+},\tau^{-})\|^2-2.
\]</span> On the other hand, suppose that <span class="math inline">\(\sigma^{\pm}\)</span> are a solution to problem 1. Then <span class="math display">\[
\|w(\sigma^{+},\sigma^{-})\|^2\le \|w(\tau^{+},\tau^{-})\|^2.
\]</span> Therefore <span class="math display">\[
\alpha^2 \|w(\sigma^{+},\sigma^{-})\|^2 = \|w(\alpha\sigma^{+},\alpha\sigma^{-})\|^2\le \|w(\lambda^{+},\lambda^{-})\|^2
\]</span> and finally <span class="math display">\[
\|w(\alpha\sigma^{+},\alpha\sigma^{-})\|^2-2\alpha\le Q(\lambda^{+},\lambda^{-})=\|w(\alpha\tau^{+},\alpha\tau^{-})\|^2-2\alpha.
\]</span> Since <span class="math inline">\(Q\)</span> is the minimal value, we have <span class="math display">\[
\alpha^{2}\|w(\sigma^{+},\sigma^{-})\|^2 = \alpha^{2}\|w(\tau^{+},\tau^{-})\|^2
\]</span> so that indeed <span class="math inline">\(w(\tau^{+},\tau^{-})\)</span> gives a solution to Problem 1.</p>
</section>
<section id="sequential-minimal-optimization" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="sequential-minimal-optimization"><span class="header-section-number">6.5.2</span> Sequential Minimal Optimization</h3>
<p>Now we outline an algorithm for solving Problem 2 that is called Sequential Minimal Optimization that was introduced by John Platt in 1998 (See <span class="citation" data-cites="plattSMO">[<a href="20-references.html#ref-plattSMO" role="doc-biblioref">5</a>]</span> and Chapter 12 of <span class="citation" data-cites="KernelMethodAdvances">[<a href="20-references.html#ref-KernelMethodAdvances" role="doc-biblioref">6</a>]</span>). The algorithm is based on the principle of “gradient ascent”, where we exploit the fact that the negative gradient of a function points in the direction of its most rapid decrease and we take small steps in the direction of the negative gradient until we reach the minimum.</p>
<p>However, in this case simplify this idea a little. Recall that the objective function <span class="math inline">\(Q(\lambda^{+},\lambda^{-})\)</span> is a quadratic function in the <span class="math inline">\(\lambda\)</span>’s and that we need to preserve the condition that <span class="math inline">\(\sum \lambda^{+}_{i}=\sum\lambda^{-}_{i}\)</span>. So our approach is going to be to take, one at a time, a pair <span class="math inline">\(\lambda^{+}_{i}\)</span> and <span class="math inline">\(\lambda^{-}_{j}\)</span> and change them <em>together</em> so that the equality of the sums is preserved and the change reduces the value of the objective function. Iterating this will take us to a minimum.</p>
<p>So, for example, let’s look at <span class="math inline">\(\lambda^{+}_i\)</span> and <span class="math inline">\(\lambda^{-}_{j}\)</span> and, for the moment, think of all of the other <span class="math inline">\(\lambda\)</span>’s as constants. Then our objective function reduces to a quadratic function of these two variables that looks something like: <span class="math display">\[
Q(\lambda_{i}^{+},\lambda_{j}^{-}) = a(\lambda^{+}_i)^2+b\lambda^{+}_i\lambda^{-}_j+c(\lambda^{-}_{i})^2+d\lambda^{+}_i+e\lambda^{-}_{j}+f.
\]</span> The constraints that remain are <span class="math inline">\(\lambda^{\pm}\ge 0\)</span>, and we are going to try to minimize <span class="math inline">\(Q\)</span> by changing <span class="math inline">\(\lambda_{i}^{+}\)</span> and <span class="math inline">\(\lambda_{j}^{-}\)</span> <em>by the same amount</em> <span class="math inline">\(\delta\)</span>. Furthermore, since we still must have <span class="math inline">\(\lambda_{i}^{+}+\delta\ge 0\)</span> and <span class="math inline">\(\lambda_{j}^{-}+\delta\ge 0\)</span>, we have</p>
<p><span id="eq-delta"><span class="math display">\[
\delta\ge M=\max\{-\lambda_{i}^{+},-\lambda_{j}^{-}\}
\tag{6.3}\]</span></span></p>
<p>In terms of this single variable <span class="math inline">\(\delta\)</span>, our optimization problem becomes the job of finding the minimum of a quadratic polynomial in one variable subject to the constraint in <a href="#eq-delta">Equation&nbsp;<span>6.3</span></a>. This is easy! There are two cases: the critical point of the quadratic is to the left of <span class="math inline">\(M\)</span>, in which case the minimum value occurs at <span class="math inline">\(M\)</span>; or the critical point of the quadratic is to the right of <span class="math inline">\(M\)</span>, in which case the critical point occurs there. This is illustrated in <a href="#fig-quadratics">Figure&nbsp;<span>6.10</span></a>.</p>
<div id="fig-quadratics" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/quadratic.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.10: Minimizing the 1-variable quadratic objective function</figcaption><p></p>
</figure>
</div>
<p>Computationally, let’s write <span class="math display">\[
w_{\delta,i,j}(\lambda^{+},\lambda^{-}) = w(\lambda^{+},\lambda^{-})+\delta(x^{+}_{i}-x^{-}_{j}).
\]</span> Then <span class="math display">\[
\frac{d}{d\delta}(\|w_{\delta,i,j}(\lambda^{+},\lambda^{-})\|^2-2\alpha)  = 2w_{\delta,i,j}(\lambda^{+},\lambda^{-})\cdot(x^{+}_{i}-x^{-}_{j})-2
\]</span> and using the definition of <span class="math inline">\(w_{\delta,i,j}\)</span> we obtain the following formula for the critical value of <span class="math inline">\(\delta\)</span> by setting this derivative to zero: <span class="math display">\[
\delta_{i,j} = \frac{(1-w(\lambda^{+},\lambda^{-})\cdot(x_{i}^{+}-x_{j}^{-})}{\|x^+_{i}-x^{-}_{j}\|^2}
\]</span></p>
<p>Using this information we can describe the SMO algorithm.</p>
<p><strong>Algorithm (SMO, see <span class="citation" data-cites="plattSMO">[<a href="20-references.html#ref-plattSMO" role="doc-biblioref">5</a>]</span>):</strong></p>
<p><strong>Given:</strong> Two linearly separable sets of points <span class="math inline">\(A^{+}=\{x_{1}^{+},\ldots,x_{n_{+}}^{+}\}\)</span> and <span class="math inline">\(A^{-}=\{x_{1}^{-},\ldots, x_{n_{-}}^{-}\}\)</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span>.</p>
<p><strong>Find:</strong> Points <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> belonging to <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span> respectively such that <span class="math display">\[
\|p-q\|^2=\min_{p'\in C(A^{+}),q'\in C(A^{-})} \|p'-q'\|^2
\]</span></p>
<p><strong>Initialization:</strong> Set <span class="math inline">\(\lambda_{i}^{+}=\frac{1}{n_{+}}\)</span> for <span class="math inline">\(i=1,\ldots, n_{+}\)</span> and <span class="math inline">\(\lambda_{i}^{-}=\frac{1}{n_{-}}\)</span> for <span class="math inline">\(i=1,\ldots, n_{-}\)</span>. Set <span class="math display">\[
p(\lambda^{+})=\sum_{i=1}^{n_{+}}\lambda^{+}_{i}x^{+}_{i}
\]</span> and <span class="math display">\[
q(\lambda^{-})=\sum_{i=1}^{n_{-}}\lambda^{-}_{i}x^{-}_{i}
\]</span> Notice that <span class="math inline">\(w(\lambda^{+},\lambda^{-})=p(\lambda^{+})-q(\lambda^{-})\)</span>. Let <span class="math inline">\(\alpha=\sum_{i=1}^{n_{+}}\lambda^{+}=\sum_{i=1}^{n_{-}}\lambda^{-}\)</span>. These sums will remain equal to each other throughout the operation of the algorithm.</p>
<p>Repeat the following steps until maximum value of <span class="math inline">\(\delta^{*}\)</span> computed in each iteration is smaller than some tolerance (so that the change in all of the <span class="math inline">\(\lambda\)</span>’s is very small):</p>
<ul>
<li>For each pair <span class="math inline">\(i,j\)</span> with <span class="math inline">\(1\le i\le n_{+}\)</span> and <span class="math inline">\(1\le j\le n_{-}\)</span>, compute <span class="math display">\[
M_{i,j} = \max\{-\lambda_{i}^{+},-\lambda_{j}^{-}\}
\]</span> and <span class="math display">\[
\delta_{i,j} = \frac{1-(p(\lambda^{+})-q(\lambda^{-}))\cdot(x_{i}^{+}-x_{j}^{-})}{\|x^+_{i}-x^{-}_{j}\|^2}.
\]</span> If <span class="math inline">\(\delta_{i,j}\ge M\)</span> then set <span class="math inline">\(\delta^{*}=\delta_{i,j}\)</span>; otherwise set <span class="math inline">\(\delta^{*}=M\)</span>. Then update the <span class="math inline">\(\lambda^{\pm}\)</span> by the equations: <span class="math display">\[
\begin{aligned}
\lambda^{+}_{i}&amp;=&amp;\lambda^{+}_{i}+\delta_{i,j}^{*} \\
\lambda^{+}_{j}&amp;=&amp;\lambda^{-}_{j}+\delta_{i,j}^{*} \\
\end{aligned}
\]</span></li>
</ul>
<p>When this algorithm finishes, <span class="math inline">\(p\approx p(\lambda^{+})\)</span> and <span class="math inline">\(q\approx q(\lambda^{-})\)</span> will be very good approximations to the desired closest points.</p>
<p>Recall that if we set <span class="math inline">\(w=p-q\)</span>, then the optimal margin classifier is</p>
<p><span class="math display">\[
f(x)=w\cdot x - \frac{B^{+}+B^{-}}{2}=0
\]</span></p>
<p>where <span class="math inline">\(B^{+}=w\cdot p\)</span> and <span class="math inline">\(B^{-}=w\cdot q\)</span>. Since <span class="math inline">\(w=p-q\)</span> we can simplify this to obtain</p>
<p><span class="math display">\[
f(x)=(p-q)\cdot x -\frac{\|p\|^2-\|q\|^2}{2}=0.
\]</span></p>
<p>In <a href="#fig-penguinsolution">Figure&nbsp;<span>6.11</span></a>, we show the result of applying this algorithm to the penguin data and illustrate the closest points as found by an implementation of the SMO algorithm, together with the optimal classifying line.</p>
<p>Bearing in mind that the y-axis is scaled by a factor of 200, we obtain the following rule for distinguishing between Adelie and Gentoo penguins – if the culmen depth and body mass put you above the red line, you are a Gentoo penguin, otherwise you are an Adelie.</p>
<div id="fig-penguinsolution" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/solution.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.11: Closest points in convex hulls of penguin data</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="inseparable-sets" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="inseparable-sets"><span class="header-section-number">6.6</span> Inseparable Sets</h2>
<p>Not surprisingly, real life is often more complicated than the penguin example we’ve discussed at length in these notes. In particular, sometimes we have to work with sets that are not linearly separable. Instead, we might have two point clouds, the bulk of which are separable, but because of some outliers there is no hyperplane we can draw that separates the two sets into two halfplanes.</p>
<p>Fortunately, all is not lost. There are two common ways to address this problem, and while we won’t take the time to develop the theory behind them, we can at least outline how they work.</p>
<section id="best-separating-hyperplanes" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="best-separating-hyperplanes"><span class="header-section-number">6.6.1</span> Best Separating Hyperplanes</h3>
<p>If our sets are not linearly separable, then their convex hulls overlap and so our technique for finding the closest points of the convex hulls won’t work. In this case, we can “shrink” the convex hull by considering combinations of points <span class="math inline">\(\sum_{i}\lambda_{i}x_{i}\)</span> where <span class="math inline">\(\sum\lambda_{i}=1\)</span> and <span class="math inline">\(C\ge\lambda_{i}\ge 0\)</span> for some <span class="math inline">\(C\le 1\)</span>. For <span class="math inline">\(C\)</span> small enough, reduced convex hulls will be linearly separable – although some outlier points from each class will lie outside of them – and we can find hyperplane that separates the reduced hulls.<br>
In practice, this means we allow a few points to lie on the “wrong side” of the hyperplane. Our tolerance for these mistakes depends on <span class="math inline">\(C\)</span>, but we can include <span class="math inline">\(C\)</span> in the optimization problem to try to find the smallest <span class="math inline">\(C\)</span> that “works”.</p>
</section>
<section id="nonlinear-kernels" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="nonlinear-kernels"><span class="header-section-number">6.6.2</span> Nonlinear kernels</h3>
<p>The second option is to look not for separating hyperplanes but instead for separating curves – perhaps polynomials or even more exotic curves. This can be achieved by taking advantage of the form of <a href="#eq-kernel">Equation&nbsp;<span>6.2</span></a>. As you see there, the only way the points <span class="math inline">\(x_{i}^{\pm}\)</span> enter in to the function being minimized is through the inner products <span class="math inline">\(x_{i}^{\pm}\cdot x_{j}^{\pm}\)</span>. We can adopt a different inner product than the usual Euclidean one, and reconsider the problem using this different inner product. This amounts to embedding our points in a higher dimensional space where they are more likely to be linearly separable. Again, we will not pursue the mathematics of this further in these notes.</p>
</section>
</section>
<section id="sec-exercises" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="sec-exercises"><span class="header-section-number">6.7</span> Exercises</h2>
<ol type="1">
<li><p>Prove that, if <span class="math inline">\(f(x)=w\cdot x+b=0\)</span> is a hyperplane in <span class="math inline">\(\mathbf{R}^{k}\)</span>, then the two “sides” of this hyperplane, consisting of the points where <span class="math inline">\(f(x)\ge 0\)</span> and <span class="math inline">\(f(x)\le 0\)</span>, are both convex sets.</p></li>
<li><p>Prove that <span class="math inline">\(C(S)\)</span> is the intersection of all the halfplanes <span class="math inline">\(f(x)\ge 0\)</span> as <span class="math inline">\(f(x)=w\cdot x+b\)</span> runs through all supporting hyperplanes for <span class="math inline">\(S\)</span> where <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(p\in S\)</span>.</p></li>
<li><p>Prove that <span class="math inline">\(C(S)\)</span> is bounded. Hint: show that <span class="math inline">\(S\)</span> is contained in a sphere of sufficiently large radius centered at zero, and then that <span class="math inline">\(C(S)\)</span> is contained in that sphere as well.</p></li>
<li><p>Confirm the final formula for the optimal margin classifier at the end of the lecture.</p></li>
</ol>


<div id="refs" class="references csl-bib-body" role="doc-bibliography" style="display: none">
<div id="ref-vapnik92" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline"><span class="smallcaps">Boser</span>, B., <span class="smallcaps">Guyon</span>, I. and <span class="smallcaps">Vapnik</span>, V. <a href="https://dl.acm.org/doi/pdf/10.1145/130385.130401"><span class="nocase">A training algorithm for optimal margin classifiers</span></a>. In <em>Colt ’92: Proceedings of the fifth annual workshop on computational learning theory</em> (D. Haussler, ed) pp 144–52. ACM.</div>
</div>
<div id="ref-bennettDuality" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline"><span class="smallcaps">Bennett</span>, K. P. and <span class="smallcaps">Bredensteiner</span>, E. J. (2000). Duality and geometry in SVM classifiers. In <em>Proceedings of the seventeenth international conference on machine learning</em> (P. Langley, ed). Morgan Kaufmann Publishers.</div>
</div>
<div id="ref-penguins" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline"><span class="smallcaps">Gorman</span>, K. B., <span class="smallcaps">Williams</span>, T. D. and <span class="smallcaps">Fraser</span>, W. R. (2014). <a href="https://doi.org/10.1371/journal.pone.0090081">Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis)</a>. <em>PLoS ONE</em> <strong>9(3)</strong> –13.</div>
</div>
<div id="ref-penguindata" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline"><span class="smallcaps">Horst</span>, A. Palmer penguins.Available at <a href="https://github.com/allisonhorst/palmerpenguins">https://github.com/allisonhorst/palmerpenguins</a>.</div>
</div>
<div id="ref-plattSMO" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline"><span class="smallcaps">Platt</span>, J. C. (1998). <em><a href="https://www-ai.cs.tu-dortmund.de/LEHRE/SEMINARE/SS09/AKTARBEITENDESDM/LITERATUR/PlattSMO.pdf">Sequential minimal optimization: A fast algorithm for training support vector machines</a></em>. Microsoft Research.</div>
</div>
<div id="ref-KernelMethodAdvances" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline"><span class="smallcaps">Schölkopf</span>, B., <span class="smallcaps">Burges</span>, C. and <span class="smallcaps">Smola</span>, A. (1998). <em>Advances in kernel methods: Support vector learning</em>. MIT Press.</div>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/05-logistic-regression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/20-references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">© 2021. This work is licensed by Jeremy Teitelbaum under the <a href="http://creativecommons.org/licenses/by-sa/4.0">Creative Commons Attribution-ShareAlike License</a>.</div>
  </div>
</footer>



</body></html>