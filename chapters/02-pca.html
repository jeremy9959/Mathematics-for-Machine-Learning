<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-99.9.9">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Lectures on Machine Learning - 2&nbsp; Principal Component Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/03-probability.html" rel="next">
<link href="../chapters/01-linear-regression.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Principal Component Analysis</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Lectures on Machine Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-linear-regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-pca.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Principal Component Analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Probability and Bayes Theorem</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-naive-bayes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Naive Bayes classification method</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-logistic-regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Logistic Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-svm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">2.1</span>  Introduction</a></li>
  <li><a href="#variance-and-covariance" id="toc-variance-and-covariance" class="nav-link" data-scroll-target="#variance-and-covariance"><span class="toc-section-number">2.2</span>  Variance and Covariance</a>
  <ul class="collapse">
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance"><span class="toc-section-number">2.2.1</span>  Variance</a></li>
  <li><a href="#covariance" id="toc-covariance" class="nav-link" data-scroll-target="#covariance"><span class="toc-section-number">2.2.2</span>  Covariance</a></li>
  <li><a href="#correlation" id="toc-correlation" class="nav-link" data-scroll-target="#correlation"><span class="toc-section-number">2.2.3</span>  Correlation</a></li>
  <li><a href="#sec-covarmat" id="toc-sec-covarmat" class="nav-link" data-scroll-target="#sec-covarmat"><span class="toc-section-number">2.2.4</span>  The covariance matrix</a></li>
  <li><a href="#sec-visualizecovar" id="toc-sec-visualizecovar" class="nav-link" data-scroll-target="#sec-visualizecovar"><span class="toc-section-number">2.2.5</span>  Visualizing the covariance matrix</a></li>
  <li><a href="#linear-combinations-of-features-scores" id="toc-linear-combinations-of-features-scores" class="nav-link" data-scroll-target="#linear-combinations-of-features-scores"><span class="toc-section-number">2.2.6</span>  Linear Combinations of Features (Scores)</a></li>
  <li><a href="#mean-and-variance-of-scores" id="toc-mean-and-variance-of-scores" class="nav-link" data-scroll-target="#mean-and-variance-of-scores"><span class="toc-section-number">2.2.7</span>  Mean and variance of scores</a></li>
  <li><a href="#geometry-of-scores" id="toc-geometry-of-scores" class="nav-link" data-scroll-target="#geometry-of-scores"><span class="toc-section-number">2.2.8</span>  Geometry of Scores</a></li>
  </ul></li>
  <li><a href="#principal-components" id="toc-principal-components" class="nav-link" data-scroll-target="#principal-components"><span class="toc-section-number">2.3</span>  Principal Components</a>
  <ul class="collapse">
  <li><a href="#change-of-variance-with-direction" id="toc-change-of-variance-with-direction" class="nav-link" data-scroll-target="#change-of-variance-with-direction"><span class="toc-section-number">2.3.1</span>  Change of variance with direction</a></li>
  <li><a href="#sec-extremalvariance" id="toc-sec-extremalvariance" class="nav-link" data-scroll-target="#sec-extremalvariance"><span class="toc-section-number">2.3.2</span>  Directions of extremal variance</a></li>
  <li><a href="#sec-critvals" id="toc-sec-critvals" class="nav-link" data-scroll-target="#sec-critvals"><span class="toc-section-number">2.3.3</span>  Critical values of the variance</a></li>
  <li><a href="#sec-subspaces" id="toc-sec-subspaces" class="nav-link" data-scroll-target="#sec-subspaces"><span class="toc-section-number">2.3.4</span>  Subspaces of extremal variance</a></li>
  <li><a href="#definition-of-principal-components" id="toc-definition-of-principal-components" class="nav-link" data-scroll-target="#definition-of-principal-components"><span class="toc-section-number">2.3.5</span>  Definition of Principal Components</a></li>
  </ul></li>
  <li><a href="#dimensionality-reduction-via-principal-components" id="toc-dimensionality-reduction-via-principal-components" class="nav-link" data-scroll-target="#dimensionality-reduction-via-principal-components"><span class="toc-section-number">2.4</span>  Dimensionality Reduction via Principal Components</a>
  <ul class="collapse">
  <li><a href="#loadings" id="toc-loadings" class="nav-link" data-scroll-target="#loadings"><span class="toc-section-number">2.4.1</span>  Loadings</a></li>
  <li><a href="#sec-svd" id="toc-sec-svd" class="nav-link" data-scroll-target="#sec-svd"><span class="toc-section-number">2.4.2</span>  The singular value decomposition</a></li>
  </ul></li>
  <li><a href="#sec-spectraltheorem" id="toc-sec-spectraltheorem" class="nav-link" data-scroll-target="#sec-spectraltheorem"><span class="toc-section-number">2.5</span>  Eigenvalues and Eigenvectors of Real Symmetric Matrices (The Spectral Theorem)</a>
  <ul class="collapse">
  <li><a href="#sec-gsprocess" id="toc-sec-gsprocess" class="nav-link" data-scroll-target="#sec-gsprocess"><span class="toc-section-number">2.5.1</span>  Gram-Schmidt</a></li>
  <li><a href="#the-spectral-theorem" id="toc-the-spectral-theorem" class="nav-link" data-scroll-target="#the-spectral-theorem"><span class="toc-section-number">2.5.2</span>  The spectral theorem</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
::: {.hidden} 
$$
    \newcommand{\df}[1]{\frac{\partial}{\partial #1}}
$$
:::

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Principal Component Analysis</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>Suppose that, as usual, we begin with a collection of measurements of different features for a group of samples. Some of these measurements will tell us quite a bit about the difference among our samples, while others may contain relatively little information. For example, if we are analyzing the effect of a certain weight loss regimen on a group of people, the age and weight of the subjects may have a great deal of influence on how successful the regimen is, while their blood pressure might not. One way to help identify which features are more significant is to ask whether or not the feature varies a lot among the different samples. If nearly all the measurements of a feature are the same, it can’t have much power in distinguishing the samples, while if the measurements vary a great deal then that feature has a chance to contain useful information.</p>
<p>In this section we will discuss a way to measure the variability of measurements and then introduce principal component analysis (PCA). PCA is a method for finding which linear combinations of measurements have the greatest variability and therefore might contain the most information. It also allows us to identify combinations of measurements that don’t vary much at all. Combining this information, we can sometimes replace our original system of features with a smaller set that still captures most of the interesting information in our data, and thereby find hidden characteristics of the data and simplify our analysis a great deal.</p>
</section>
<section id="variance-and-covariance" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="variance-and-covariance"><span class="header-section-number">2.2</span> Variance and Covariance</h2>
<section id="variance" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="variance"><span class="header-section-number">2.2.1</span> Variance</h3>
<p>Suppose that we have a collection of measurements <span class="math inline">\((x_1,\ldots, x_N)\)</span> of a particular feature <span class="math inline">\(X\)</span>. For example, <span class="math inline">\(x_i\)</span> might be the initial weight of the <span class="math inline">\(ith\)</span> participant in our weight loss study. The mean of the values <span class="math inline">\((x_1,\ldots, x_N)\)</span> is</p>
<p><span class="math display">\[
\mu_{X} = \frac{1}{N}\sum_{i=1}^{N} x_{i}.
\]</span></p>
<p>The simplest measure of the variability of the data is called its <em>variance.</em></p>
<p><strong>Definition:</strong> The (sample) variance of the data <span class="math inline">\(x_1,\ldots, x_N\)</span> is</p>
<p><span id="eq-variance"><span class="math display">\[
\sigma_{X}^2 = \frac{1}{N}\sum_{i=1}^{N} \left(x_{i}-\mu_{X}\right)^2 = \frac{1}{N}\left(\sum_{i=1}^{N} x_{i}^2\right)- \mu_{X}^2
\tag{2.1}\]</span></span></p>
<p>The square root of the variance is called the <em>standard deviation.</em></p>
<p>As we see from the formula, the variance is a measure of how ‘spread out’ the data is from the mean.</p>
<p>Recall that in our discussion of linear regression we thought of our set of measurements <span class="math inline">\(x_1,\ldots, x_N\)</span> as a vector – it’s one of the columns of our data matrix. From that point of view, the variance has a geometric interpretation – it is <span class="math inline">\(\frac{1}{N}\)</span> times the square of the distance from the point <span class="math inline">\(X=(x_1,\ldots, x_N)\)</span> to the point <span class="math inline">\(\mu_{X}(1,1,\ldots,1)=\mu_{X}E\)</span>:</p>
<p><span id="eq-variancedot"><span class="math display">\[
\sigma_{X}^2 = \frac{1}{N}(X-\mu_{X}E)\cdot(X-\mu_{X}E)  = \frac{1}{N}\|X-\mu_{X}E\|^2.
\tag{2.2}\]</span></span></p>
</section>
<section id="covariance" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="covariance"><span class="header-section-number">2.2.2</span> Covariance</h3>
<p>The variance measures the dispersion of measures of a single feature. Often, we have measurements of multiple features and we might want to know something about how two features are related. The <em>covariance</em> is a measure of whether two features tend to be related, in the sense that when one increases, the other one increases; or when one increases, the other one decreases.</p>
<p><strong>Definition:</strong> Given measurements <span class="math inline">\((x_1,\ldots, x_N)\)</span> and <span class="math inline">\((y_1,\ldots, y_N)\)</span> of two features <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is</p>
<p><span id="eq-covariancedot"><span class="math display">\[
\sigma_{XY} = \frac{1}{N}\sum_{i=1}^{N} (x_i-\mu_{X})(y_i-\mu_{Y})
\tag{2.3}\]</span></span></p>
<p>There is a nice geometric interpretation of this, as well, in terms of the dot product. If <span class="math inline">\(X=(x_1,\ldots, x_N)\)</span> and <span class="math inline">\(Y=(y_1\ldots,y_N)\)</span> then</p>
<p><span class="math display">\[
\sigma_{XY} = \frac{1}{N} ((X-\mu_{X}E)\cdot (Y-\mu_{Y}E)).
\]</span></p>
<p>From this point of view, we can see that <span class="math inline">\(\sigma_{XY}\)</span> is positive if the <span class="math inline">\(X-\mu_{X}E\)</span> and <span class="math inline">\(Y-\mu_{Y}E\)</span> vectors “point roughly in the same direction” and its negative if they “point roughly in the opposite direction.”</p>
</section>
<section id="correlation" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="correlation"><span class="header-section-number">2.2.3</span> Correlation</h3>
<p>One problem with interpreting the variance and covariance is that we don’t have a scale – for example, if <span class="math inline">\(\sigma_{XY}\)</span> is large and positive, then we’d like to say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are closely related, but it could be just that the entries of <span class="math inline">\(X-\mu_{X}E\)</span> and <span class="math inline">\(Y-\mu_{Y}E\)</span> are large. Here, though, we can really take advantage of the geometric interpretation. Recall that the dot product of two vectors satisfies the formula</p>
<p><span class="math display">\[
a \cdot b = \|a\|\|b\|\cos(\theta)
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. So</p>
<p><span class="math display">\[
\cos(\theta) = \frac{a\cdot b}{\|a\|\|b\|}.
\]</span></p>
<p>Let’s apply this to the variance and covariance, by noticing that</p>
<p><span class="math display">\[
\frac{(X-\mu_{X}E)\cdot (Y-\mu_{Y}E)}{\|(X-\mu_{X}E)\|\|(Y-\mu_{Y}E)\|} = \frac{\sigma_{XY}}{\sigma_{XX}\sigma_{YY}}
\]</span></p>
<p>so the quantity</p>
<p><span id="eq-rxy"><span class="math display">\[
r_{XY} = \frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}}
\tag{2.4}\]</span></span></p>
<p>measures the cosine of the angle between the vectors <span class="math inline">\(X-\mu_{X}E\)</span> and <span class="math inline">\(Y-\mu_{Y}E\)</span>.</p>
<p><strong>Definition:</strong> The quantity <span class="math inline">\(r_{XY}\)</span> defined in <a href="#eq-rxy">Equation&nbsp;<span>2.4</span></a> is called the (sample) <em>correlation coefficient</em> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We have <span class="math inline">\(0\le |r_{XY}|\le 1\)</span> with <span class="math inline">\(r_{XY}=\pm 1\)</span> if and only if the two vectors <span class="math inline">\(X-\mu_{X}\)</span> and <span class="math inline">\(Y-\mu_{Y}\)</span> are collinear in <span class="math inline">\(\mathbf{R}^{N}\)</span>.</p>
<p>*<a href="#fig-corrfig">Figure&nbsp;<span>2.1</span></a> illustrates data with different values of the correlation coefficient.</p>
<div id="fig-corrfig" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/correlation.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.1: Correlation</figcaption><p></p>
</figure>
</div>
</section>
<section id="sec-covarmat" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="sec-covarmat"><span class="header-section-number">2.2.4</span> The covariance matrix</h3>
<p>In a typical situation we have many features for each of our (many) samples, that we organize into a data matrix <span class="math inline">\(X\)</span>. To recall, each column of <span class="math inline">\(X\)</span> corresponds to a feature that we measure, and each row corresponds to a sample. For example, each row of our matrix might correspond to a person enrolled in a study, and the columns correspond to height (cm), weight (kg), systolic blood pressure, and age (in years):</p>
<table class="table">
<caption>A sample data matrix <span class="math inline">\(X\)</span></caption>
<thead>
<tr class="header">
<th>sample</th>
<th style="text-align: right;">Ht</th>
<th style="text-align: right;">Wgt</th>
<th style="text-align: right;">Bp</th>
<th style="text-align: right;">Age</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td style="text-align: right;">180</td>
<td style="text-align: right;">75</td>
<td style="text-align: right;">110</td>
<td style="text-align: right;">35</td>
</tr>
<tr class="even">
<td>B</td>
<td style="text-align: right;">193</td>
<td style="text-align: right;">80</td>
<td style="text-align: right;">130</td>
<td style="text-align: right;">40</td>
</tr>
<tr class="odd">
<td>…</td>
<td style="text-align: right;">…</td>
<td style="text-align: right;">…</td>
<td style="text-align: right;">…</td>
<td style="text-align: right;">…</td>
</tr>
<tr class="even">
<td>U</td>
<td style="text-align: right;">150</td>
<td style="text-align: right;">92</td>
<td style="text-align: right;">105</td>
<td style="text-align: right;">55</td>
</tr>
</tbody>
</table>
<p>If we have multiple features, as in this example, we might be interested in the variance of each feature and all of their mutual covariances. This “package” of information can be obtained “all at once” by taking advantage of some matrix algebra.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X\)</span> be a <span class="math inline">\(N\times k\)</span> data matrix, where the <span class="math inline">\(k\)</span> columns of <span class="math inline">\(X\)</span> correspond to different features and the <span class="math inline">\(N\)</span> rows to different samples. Let <span class="math inline">\(X_{0}\)</span> be the centered version of this data matrix, obtained by subtracting the mean <span class="math inline">\(\mu_{i}\)</span> of column <span class="math inline">\(i\)</span> from all the entries <span class="math inline">\(x_{si}\)</span> in that column. Then the <span class="math inline">\(k\times k\)</span> symmetric matrix</p>
<p><span class="math display">\[
D_{0} = \frac{1}{N}X_{0}^{\intercal}X_{0}
\]</span></p>
<p>is called the (sample) covariance matrix for the data.</p>
<p><strong>Proposition:</strong> The diagonal entries <span class="math inline">\(d_{ii}\)</span> of <span class="math inline">\(D_{0}\)</span> are the variances of the columns of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
d_{ii} = \sigma_{i}^2 = \frac{1}{N}\sum_{s=1}^{N}(x_{si}-\mu_i)^2
\]</span></p>
<p>and the off-diagonal entries <span class="math inline">\(d_{ij} = d_{ji}\)</span> are the covariances of the <span class="math inline">\(i^{th}\)</span> and <span class="math inline">\(j^{th}\)</span> columns of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
d_{ij} = \sigma_{ij} = \frac{1}{N}\sum_{s=1}^{N}(x_{si}-\mu_{i})(x_{sj}-\mu_{j})
\]</span></p>
<p>The sum of the diagonal entries, the trace of <span class="math inline">\(D_{0}\)</span> is the <strong>total</strong> variance of the data.</p>
<p><strong>Proof:</strong> This follows from the definitions, but it’s worth checking the details, which we leave as an exercise.</p>
</section>
<section id="sec-visualizecovar" class="level3" data-number="2.2.5">
<h3 data-number="2.2.5" class="anchored" data-anchor-id="sec-visualizecovar"><span class="header-section-number">2.2.5</span> Visualizing the covariance matrix</h3>
<p>If the number of features in the data is not too large, a density matrix plot provides a tool for visualizing the covariance matrix of the data. A density matrix plot is an <span class="math inline">\(k\times k\)</span> grid of plots (where <span class="math inline">\(k\)</span> is the number of features). The entry with <span class="math inline">\((i,j)\)</span> coordinates in the grid is a scatter plot of the <span class="math inline">\(i^{th}\)</span> feature against the <span class="math inline">\(j^{th}\)</span> one if <span class="math inline">\(i\not=j\)</span>, and is a histogram of the <span class="math inline">\(i^{th}\)</span> variable if <span class="math inline">\(i=j\)</span>.</p>
<p>*<a href="#fig-density0">Figure&nbsp;<span>2.2</span></a> is an example of a density matrix plot for a dataset with <span class="math inline">\(50\)</span> samples and <span class="math inline">\(2\)</span> features. This data has been centered, so it can be represented in a <span class="math inline">\(50\times 2\)</span> data matrix <span class="math inline">\(X_{0}\)</span>. The upper left and lower right graphs are scatter plots of the two columns, while the lower left and upper right are the histograms of the columns.</p>
<div id="fig-density0" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/density2x2.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.2: Density Matrix Plot</figcaption><p></p>
</figure>
</div>
</section>
<section id="linear-combinations-of-features-scores" class="level3" data-number="2.2.6">
<h3 data-number="2.2.6" class="anchored" data-anchor-id="linear-combinations-of-features-scores"><span class="header-section-number">2.2.6</span> Linear Combinations of Features (Scores)</h3>
<p>Sometimes useful information about our data can be revealed if we combine different measurements together to obtain a “hybrid” measure that captures something interesting. For example, in the Auto MPG dataset that we studied in the section on Linear Regression, we looked at the influence of both vehicle weight <span class="math inline">\(w\)</span> and engine displacement <span class="math inline">\(e\)</span> on gas mileage; perhaps their is some value in considering a hybrid “score” defined as <span class="math display">\[
S = aw + be
\]</span> for some constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> – maybe by choosing a good combination we could find a better predictor of gas mileage than using one or the other of the features individually.</p>
<p>As another example, suppose we are interested in the impact of the nutritional content of food on weight gain in a study. We know that both calorie content and the level dietary fiber contribute to the weight gain of participants eating this particular food; maybe there is some kind of combined “calorie/fiber” score we could introduce that captures the impact of that food better.</p>
<p>Finally, when we assign grades in a course, we typically compute a weighted combination of the scores of each student on a series of assignments. Such a combination is another example of a “score” and may help explain the origin of the term.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X_{0}\)</span> be a (centered) <span class="math inline">\(N\times k\)</span> data matrix giving information about <span class="math inline">\(k\)</span> features for each of <span class="math inline">\(N\)</span> samples. A linear synthetic feature, or a linear score, is a linear combination of the <span class="math inline">\(k\)</span> features. The linear score is defined by constants <span class="math inline">\(a_{1},\ldots, a_{k}\)</span> so that If <span class="math inline">\(y_{1},\ldots, y_{k}\)</span> are the values of the features for a particular sample, then the linear score for that sample is</p>
<p><span class="math display">\[
S = a_{1}y_{1}+a_{2}y_{2}+\cdots+a_{k}y_{k}
\]</span></p>
<p><strong>Lemma:</strong> The values of the linear score for each of the <span class="math inline">\(N\)</span> samples can be calculated as</p>
<p><span id="eq-linearscore"><span class="math display">\[
\left[\begin{matrix} S_{1} \\ \vdots \\ S_{N}\\ \end{matrix}\right] =
X_{0}\left[
\begin{matrix} a_{1} \\ \vdots \\ a_{k}\end{matrix}\right].
\tag{2.5}\]</span></span></p>
<p><strong>Proof:</strong> Multiplying a matrix by a column vector computes a linear combination of the columns – that’s what this lemma says. Exercise 3 asks you to write out the indices and make sure you believe this.</p>
</section>
<section id="mean-and-variance-of-scores" class="level3" data-number="2.2.7">
<h3 data-number="2.2.7" class="anchored" data-anchor-id="mean-and-variance-of-scores"><span class="header-section-number">2.2.7</span> Mean and variance of scores</h3>
<p>When we combine features to make a hybrid score, we assume that the features were centered to begin with, so that each features has mean zero. As a result, the mean of the hybrid features is again zero.</p>
<p><strong>Lemma:</strong> A linear combination of features with mean zero again has mean zero.</p>
<p><strong>Proof:</strong> Let <span class="math inline">\(S_{i}\)</span> be the score for the <span class="math inline">\(i^{th}\)</span> sample, so <span class="math display">\[
S_{i} = \sum_{j=1}^{k} x_{ij}a_{j}.
\]</span> where <span class="math inline">\(X_{0}\)</span> has entries <span class="math inline">\(x_{ij}\)</span>. Then the mean value of the score is <span class="math display">\[
\mu_{S} = \frac{1}{k}\sum_{i=1}^{N} S_{i} = \frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{k} x_{ij}a_{j}.
\]</span> Reversing the order of the sum yields <span class="math display">\[
\mu_{S} = \frac{1}{N}\sum_{j=1}^{k}\sum_{i=1}^{N} x_{ij}a_{j} = \sum_{j=1}^{k} a_{j}\frac{1}{N}(\sum_{i=1}^{N} x_{ij})=
\sum_{j=1}^{k}a_{j}\mu_{j}=0
\]</span> where <span class="math inline">\(\mu_{j}=0\)</span> is the mean of the <span class="math inline">\(j^{th}\)</span> feature (column) of <span class="math inline">\(X_{0}\)</span>.</p>
<p>The variance is more interesting, and gives us an opportunity to put the covariance matrix to work. Remember from <a href="#eq-variancedot">Equation&nbsp;<span>2.2</span></a> that, since a score <span class="math inline">\(S\)</span> has mean zero, it’s variance is <span class="math inline">\(\sigma_{S}^2=\frac{1}{N}S\cdot S\)</span> – where here the score <span class="math inline">\(S\)</span> is represented by the column vector with entries <span class="math inline">\(S_{1},\ldots S_{k}\)</span> as in <a href="#eq-linearscore">Equation&nbsp;<span>2.5</span></a>.</p>
<p><strong>Lemma:</strong> The variance of the score <span class="math inline">\(S\)</span> with weights <span class="math inline">\(a_1,\ldots a_k\)</span> is <span id="eq-ada"><span class="math display">\[
\sigma_{S}^2 = a^{\intercal}D_{0}a = \left[\begin{matrix}a_{1} &amp; \cdots &amp; a_{k}\end{matrix}\right]D_{0}
\left[\begin{matrix} a_{1} \\ \vdots \\ a_{k}\end{matrix}\right]
\tag{2.6}\]</span></span> More generally, if <span class="math inline">\(S_{1}\)</span> and <span class="math inline">\(S_{2}\)</span> are scores with weights <span class="math inline">\(a_1,\ldots, a_k\)</span> and <span class="math inline">\(b_1,\ldots, b_k\)</span> respectively, then the covariance <span class="math inline">\(\sigma_{S_{1}S_{2}}\)</span> is <span class="math display">\[
\sigma_{S_{1}S_{2}} = a^{\intercal}D_{0}b.
\]</span></p>
<p><strong>Proof:</strong> From <a href="#eq-variancedot">Equation&nbsp;<span>2.2</span></a> and <a href="#eq-linearscore">Equation&nbsp;<span>2.5</span></a> we know that <span class="math display">\[
\sigma_{S}^2 = \frac{1}{N}S\cdot S
\]</span> and <span class="math display">\[
S = X_{0}a.
\]</span> Since <span class="math inline">\(\frac{1}{N}S\cdot S = \frac{1}{N}S^{\intercal}S\)</span>, this gives us <span class="math display">\[
\frac{1}{N}\sigma_{S}^2 = \frac{1}{N}(X_{0}a)^{\intercal}(X_{0}a) = \frac{1}{N}a^{\intercal}X_{0}^{\intercal}X_{0}a = a^{\intercal}D_{0}a
\]</span> as claimed.</p>
<p>For the covariance, use a similar argument with <a href="#eq-covariancedot">Equation&nbsp;<span>2.3</span></a> and <a href="#eq-linearscore">Equation&nbsp;<span>2.5</span></a>. writing <span class="math inline">\(\sigma_{S_{1}S_{2}}=\frac{1}{N}S_{1}\cdot S_{2}\)</span> and the fact that <span class="math inline">\(S_{1}\)</span> and <span class="math inline">\(S_{2}\)</span> can be written as <span class="math inline">\(X_{0}a\)</span> and <span class="math inline">\(X_{0}b\)</span>.</p>
<p>The point of this lemma is that the covariance matrix contains not just the variances and covariances of the original features, but also enough information to construct the variances and covariances for <em>any linear combination of features.</em></p>
<p>In the next section we will see how to exploit this idea to reveal hidden structure in our data.</p>
</section>
<section id="geometry-of-scores" class="level3" data-number="2.2.8">
<h3 data-number="2.2.8" class="anchored" data-anchor-id="geometry-of-scores"><span class="header-section-number">2.2.8</span> Geometry of Scores</h3>
<p>Let’s return to the dataset that we looked at in <a href="#sec-visualizecovar"><span>Section&nbsp;2.2.5</span></a>. We simplify the density matrix plot in <a href="#fig-pcasimfig">Figure&nbsp;<span>2.3</span></a>, which shows one of the scatter plots and the two histograms.</p>
<p>The scatter plot shows that the data points are arranged in a more or less elliptical cloud oriented at an angle to the <span class="math inline">\(xy\)</span>-axes which represent the two given features. The two individual histograms show the distribution of the two features – each has mean zero, with the <span class="math inline">\(x\)</span>-features distributed between <span class="math inline">\(-2\)</span> and <span class="math inline">\(2\)</span> and the <span class="math inline">\(y\)</span> feature between <span class="math inline">\(-4\)</span> and <span class="math inline">\(4\)</span>. Looking just at the two features individually, meaning only at the two histograms, we can’t see the overall elliptical structure.</p>
<div id="fig-pcasimfig" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/PCAsimulated-1.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.3: Simulated Data with Two Features</figcaption><p></p>
</figure>
</div>
<p>How can we get a better grip on our data in this situation? We can try to find a “direction” in our data that better illuminates the variation of the data. For example, suppose that we pick a unit vector at the origin pointing in a particular direction in our data. See <a href="#fig-pcasimfig-1">Figure&nbsp;<span>2.4</span></a>.</p>
<div id="fig-pcasimfig-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/PCAsimulated-2.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.4: A direction in the data</figcaption><p></p>
</figure>
</div>
<p>Now we can orthogonally project the datapoints onto the line defined by this vector, as shown in <a href="#fig-pcasimfig-2">Figure&nbsp;<span>2.5</span></a>.</p>
<div id="fig-pcasimfig-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/PCAsimulated-3.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.5: Projecting the datapoints</figcaption><p></p>
</figure>
</div>
<p>Recall that if the unit vector is defined by coordinates <span class="math inline">\(u=[u_0,u_1]\)</span>, then the orthogonal projection of the point <span class="math inline">\(x\)</span> with coordinates <span class="math inline">\((x_0,x_1)\)</span> is <span class="math inline">\((x\cdot u)u\)</span>. Now <span class="math display">\[
x\cdot u = u_0 x_0 + u_1 x_1
\]</span> so the coordinates of the points along the line defined by <span class="math inline">\(u\)</span> are the values of the score <span class="math inline">\(Z\)</span> defined by <span class="math inline">\(u=[u_0,u_1]\)</span>. Using our work in the previous section, we see that we can find all of these coordinates by matrix multiplication: <span class="math display">\[
Z = X_0 u
\]</span> where <span class="math inline">\(X_0\)</span> is our data matrix. Now let’s add a histogram of the values of <span class="math inline">\(Z\)</span> to our picture:</p>
<div id="fig-pcasimfig-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/PCAsimulated-4.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.6: Distribution of Z</figcaption><p></p>
</figure>
</div>
<p>This histogram shows the distribution of the values of <span class="math inline">\(Z\)</span> along the tilted line defined by the unit vector <span class="math inline">\(u\)</span>.</p>
<p>Finally, using our work on the covariance matrix, we see that the variance of <span class="math inline">\(Z\)</span> is given by <span class="math display">\[
\sigma_{Z}^2 = \frac{1}{50}u^{\intercal}X_{0}^{\intercal}X_{0}u = u^{\intercal}D_{0}u
\]</span> where <span class="math inline">\(D_{0}\)</span> is the covariance matrix of the data <span class="math inline">\(X_{0}\)</span>.</p>
<p><strong>Lemma:</strong> Let <span class="math inline">\(X_{0}\)</span> be a <span class="math inline">\(N\times k\)</span> centered data matrix, and let <span class="math inline">\(D_{0}=\frac{1}{N}X_{0}^{\intercal}X_{0}\)</span> be the associated covariance matrix. Let <span class="math inline">\(u\)</span> be a unit vector in “feature space” <span class="math inline">\(\mathbf{R}^{k}\)</span>. Then the score <span class="math inline">\(S=X_{0}u\)</span> can be interpreted as the coordinates of the points of <span class="math inline">\(X_{0}\)</span> projected onto the line generated by <span class="math inline">\(u\)</span>. The variance of this score is <span class="math display">\[
\sigma^{2}_{S} = u^{\intercal}D_{0}u = \sum_{i=1}^{N} s_{i}^2
\]</span> where <span class="math inline">\(s_{i} = X_{0}[i,:]u\)</span> is the dot product of the <span class="math inline">\(i^{th}\)</span> row <span class="math inline">\(X_{0}[i,:]\)</span> with <span class="math inline">\(u\)</span>. It measures the variability in the data “in the direction of the unit vector <span class="math inline">\(u\)</span>”.</p>
</section>
</section>
<section id="principal-components" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="principal-components"><span class="header-section-number">2.3</span> Principal Components</h2>
<section id="change-of-variance-with-direction" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="change-of-variance-with-direction"><span class="header-section-number">2.3.1</span> Change of variance with direction</h3>
<p>As we’ve seen in the previous section, if we choose a unit vector <span class="math inline">\(u\)</span> in the feature space and find the projection <span class="math inline">\(X_{0}u\)</span> of our data onto the line through <span class="math inline">\(u\)</span>, we get a “score” that we can use to measure the variance of the data in the direction of <span class="math inline">\(u\)</span>. What happens as we vary <span class="math inline">\(u\)</span>?</p>
<p>To study this question, let’s continue with our simulated data from the previous section, and introduce a unit vector <span class="math display">\[
u(\theta) = \left[\begin{matrix} \cos(\theta) &amp; \sin(\theta)\end{matrix}\right].
\]</span> This is in fact a unit vector, since <span class="math inline">\(\sin^2(\theta)+\cos^2(\theta)=1\)</span>, and it is oriented at an angle <span class="math inline">\(\theta\)</span> from the <span class="math inline">\(x\)</span>-axis.</p>
<p>The variance of the data in the direction of <span class="math inline">\(u(\theta)\)</span> is given by <span class="math display">\[
\sigma_{\theta}^2 = u(\theta)^{\intercal}D_{0}u(\theta).
\]</span></p>
<p>A plot of this function for the data we have been considering is in <a href="#fig-pcatheta">Figure&nbsp;<span>2.7</span></a>. As you can see, the variance goes through two full periods with the angle, and it reaches a maximum and minimum value at intervals of <span class="math inline">\(\pi/2\)</span> – so the two angles where the variance are maximum and minimum are orthogonal to one another.</p>
<div id="fig-pcatheta" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/PCAtheta.png" class="img-fluid figure-img" style="width:25.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.7: Change of variance with angle theta</figcaption><p></p>
</figure>
</div>
<p>The two directions where the variance is maximum and minimum are drawn on the original data scatter plot in <a href="#fig-pcaprincipal">Figure&nbsp;<span>2.8</span></a> .</p>
<div id="fig-pcaprincipal" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/PCAprincipal.png" class="img-fluid figure-img" style="width:25.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.8: Data with principal directions</figcaption><p></p>
</figure>
</div>
<p>Let’s try to understand why this is happening.</p>
</section>
<section id="sec-extremalvariance" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="sec-extremalvariance"><span class="header-section-number">2.3.2</span> Directions of extremal variance</h3>
<p>Given our centered, <span class="math inline">\(N\times i\)</span> data matrix <span class="math inline">\(X_{0}\)</span>, with its associated covariance matrix <span class="math inline">\(D_{0}=\frac{1}{N}X_{0}^{\intercal}X_{0}\)</span>, we would like to find unit vectors <span class="math inline">\(u\)</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span> so that <span class="math display">\[
\sigma_{u}^{2} = u^{\intercal}D_{0}u
\]</span> reaches its maximum and its minimum. Here <span class="math inline">\(\sigma_{u}^2\)</span> is the variance of the “linear score” <span class="math inline">\(X_{0}u\)</span> and it represents how dispersed the data is in the “u direction” in <span class="math inline">\(\mathbf{R}^{k}\)</span>.</p>
<p>In this problem, remember that the coordinates of <span class="math inline">\(u=(u_1,\ldots, u_{k})\)</span> are the variables and the symmetric matrix <span class="math inline">\(D_{0}\)</span> is given. As usual, we to find the maximum and minimum values of <span class="math inline">\(\sigma_{u}^{2}\)</span>, we should look at the partial derivatives of <span class="math inline">\(\sigma_{u}^{2}\)</span> with respect to the variables <span class="math inline">\(u_{i}\)</span> and set them to zero. Here, however, there is a catch – we want to restrict <span class="math inline">\(u\)</span> to being a unit vector, with <span class="math inline">\(u\cdot u =\sum u_{i}^2=1\)</span>.</p>
<p>So this is a <em>constrained optimization problem</em>:</p>
<ul>
<li>Find extreme values of the function <span class="math display">\[
\sigma_{u}^{2} = u^{\intercal}D_{0}u
\]</span></li>
<li>Subject to the constraint <span class="math inline">\(\|u\|^2 = u\cdot u=1\)</span> (or <span class="math inline">\(u\cdot u-1=0\)</span>)</li>
</ul>
<p>We will use the technique of <em>Lagrange Multipliers</em> to solve such a problem.</p>
<p>To apply this method, we introduce the function</p>
<p><span id="eq-lagrange"><span class="math display">\[
S(u, \lambda) = u^{\intercal}D_{0}u - \lambda(u\cdot u -1)
\tag{2.7}\]</span></span></p>
<p>Then we compute the gradient</p>
<p><span id="eq-lagrangegradient"><span class="math display">\[
\nabla S = \left[\begin{matrix} \frac{\partial S}{\partial u_{1}} \\ \vdots \\ \frac{\partial S}{\partial u_{k}} \\ \frac{\partial S}{\partial \lambda}\end{matrix}\right]
\tag{2.8}\]</span></span></p>
<p>and solve the system of equations <span class="math inline">\(\nabla S=0\)</span>. Here we have written the gradient as a column vector for reasons that will become clearer shortly.</p>
<p>Computing all of these partial derivatives looks messy, but actually if we take advantage of matrix algebra it’s not too bad. The following two lemmas explain how to do this.</p>
<p><strong>Lemma</strong>: Let <span class="math inline">\(M\)</span> be a <span class="math inline">\(N\times k\)</span> matrix with constant coefficients and let <span class="math inline">\(u\)</span> be a <span class="math inline">\(k\times 1\)</span> column vector whose entries are <span class="math inline">\(u_1,\ldots u_{k}\)</span>. The function <span class="math inline">\(F(u) = Mu\)</span> is a linear map from <span class="math inline">\(\mathbf{R}^{k}\to\mathbf{R}^{N}\)</span>. Its (total) derivative is a linear map between the same vector spaces, and satisfies <span class="math display">\[
D(F)(v) = Mv
\]</span> for any <span class="math inline">\(k\times 1\)</span> vector <span class="math inline">\(v\)</span>. If <span class="math inline">\(u\)</span> is a <span class="math inline">\(1\times N\)</span> matrix, and <span class="math inline">\(G(u) = uM\)</span>, then <span class="math display">\[
D(G)(v) = vM
\]</span></p>
<p>for any <span class="math inline">\(1\times N\)</span> vector <span class="math inline">\(v\)</span>. (This is the matrix version of the derivative rule that <span class="math inline">\(\frac{d}{dx}(ax)=a\)</span> for a constant <span class="math inline">\(a\)</span>.)</p>
<p><strong>Proof:</strong> Since <span class="math inline">\(F:\mathbf{R}^{k}\to\mathbf{R}^{N}\)</span>, we can write out <span class="math inline">\(F\)</span> in more traditional function notation as <span class="math display">\[
F(u) = (F_{1}(u_1,\ldots, u_k), \ldots, F_{N}(u_1,\ldots, u_{k})
\]</span> where <span class="math display">\[
F_{i}(u_1,\ldots u_k) = \sum_{j=1}^{k} m_{ij}u_{j}.
\]</span> Thus <span class="math inline">\(\frac{\partial F_{i}}{\partial u_{j}} = m_{ij}\)</span>. The total derivative <span class="math inline">\(D(F)\)</span> is the linear map with matrix <span class="math display">\[
D(F)_{ij} = \frac{\partial F_{i}}{\partial u_{j}} = m_{ij}
\]</span> and so <span class="math inline">\(D(F)=M\)</span>.</p>
<p>The other result is proved the same way.</p>
<p><strong>Lemma</strong>: Let <span class="math inline">\(D\)</span> be a symmetric <span class="math inline">\(k\times k\)</span> matrix with constant entries and let <span class="math inline">\(u\)</span> be an <span class="math inline">\(k\times 1\)</span> column vector of variables <span class="math inline">\(u_{1},\ldots, u_{k}\)</span>. Let <span class="math inline">\(F:\mathbf{R}^{k}\to R\)</span> be the function <span class="math inline">\(F(u) = u^{\intercal}Du\)</span>. Then the gradient <span class="math inline">\(\nabla_{u} F\)</span> is a vector field – that is, a vector-valued function of <span class="math inline">\(u\)</span>, and is given by the formula <span class="math display">\[
\nabla_{u} F = 2Du
\]</span></p>
<p><strong>Proof:</strong> Let <span class="math inline">\(d_{ij}\)</span> be the <span class="math inline">\(i,j\)</span> entry of <span class="math inline">\(D\)</span>. We can write out the function <span class="math inline">\(F\)</span> to obtain <span class="math display">\[
F(u_1,\ldots, u_{k}) = \sum_{i=1}^{k} \sum_{j=1}^{k} u_i d_{ij} u_j.
\]</span> Now <span class="math inline">\(\frac{\partial F}{\partial u_{i}}\)</span> is going to pick out only terms where <span class="math inline">\(u_{i}\)</span> appears, yielding: <span class="math display">\[
\frac{\partial F}{\partial u_{i}} = \sum_{j=1}^{k} d_{ij}u_{j} + \sum_{j=1}^{k} u_{j}d_{ji}
\]</span> Here the first sum catches all of the terms where the first “u” is <span class="math inline">\(u_{i}\)</span>; and the second sum catches all the terms where the second “u” is <span class="math inline">\(u_{i}\)</span>. The diagonal terms <span class="math inline">\(u_{i}^2d_{ii}\)</span> contribute once to each sum, which is consistent with the rule that the derivative of <span class="math inline">\(u_{i}^2d_{ii} = 2u_{i}d_{ii}\)</span>. To finish the proof, notice that <span class="math display">\[
\sum_{j=1}^{k} u_{j}d_{ji} = \sum_{j=1}^{k} d_{ij}u_{j}
\]</span> since <span class="math inline">\(D\)</span> is symmetric, so in fact the two terms are the same Thus <span class="math display">\[
\df{u_{i}}F = 2\sum_{j=1}^{k} d_{ij}u_{j}
\]</span> But the right hand side of this equation is twice the <span class="math inline">\(i^{th}\)</span> entry of <span class="math inline">\(Du\)</span>, so putting the results together we get <span class="math display">\[
\nabla_{u}F = \left[\begin{matrix} \frac{\partial F}{\partial u_{1}} \\ \vdots \\ \frac{\partial F}{\partial u_{k}}\end{matrix}\right] = 2Du.
\]</span></p>
<p>The following theorem puts all of this work together to reduce our questions about how variance changes with direction.</p>
</section>
<section id="sec-critvals" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="sec-critvals"><span class="header-section-number">2.3.3</span> Critical values of the variance</h3>
<p><strong>Theorem:</strong> The critical values of the variance <span class="math inline">\(\sigma_{u}^2\)</span>, as <span class="math inline">\(u\)</span> varies over unit vectors in <span class="math inline">\(\mathbf{R}^{N}\)</span>, are the eigenvalues <span class="math inline">\(\lambda_{1},\ldots,\lambda_{k}\)</span> of the covariance matrix <span class="math inline">\(D\)</span>, and if <span class="math inline">\(e_{i}\)</span> is a unit eigenvector corresponding to <span class="math inline">\(\lambda_{i}\)</span>, then <span class="math inline">\(\sigma_{e_{i}}^2 = \lambda_{i}\)</span>.</p>
<p><strong>Proof:</strong> Recall that we introduced the Lagrange function <span class="math inline">\(S(u,\lambda)\)</span>, whose critical points give us the solutions to our constrained optimization problem. As we said in <a href="#eq-lagrange">Equation&nbsp;<span>2.7</span></a>: <span class="math display">\[
S(u,\lambda) = u^{\intercal}D_{0}u - \lambda(u\cdot u - 1) = u^{\intercal}D_{0}u -\lambda(u\cdot u) + \lambda
\]</span> Now apply our Matrix calculus lemmas. First, let’s treat <span class="math inline">\(\lambda\)</span> as a constant and focus on the <span class="math inline">\(u\)</span> variables. We can write <span class="math inline">\(u\cdot u = u^{\intercal} I_{N} u\)</span> where <span class="math inline">\(I_{N}\)</span> is the identity matrix to compute: <span class="math display">\[
\nabla_{u} S = 2D_{0}u -2\lambda u
\]</span> For <span class="math inline">\(\lambda\)</span> we have <span class="math display">\[
\df{\lambda}S = -u\cdot u +1.
\]</span> The critical points occur when <span class="math display">\[
\nabla_{u} S = 2(D_{0}-\lambda)u = 0
\]</span> and <span class="math display">\[
\df{\lambda}S = 1-u\cdot u = 0
\]</span> The first equation says that <span class="math inline">\(\lambda\)</span> must be an eigenvalue, and <span class="math inline">\(u\)</span> an eigenvector: <span class="math display">\[
D_{0}u = \lambda u
\]</span> while the second says <span class="math inline">\(u\)</span> must be a unit vector <span class="math inline">\(u\cdot u=\|u\|^2=1\)</span>. The second part of the result follows from the fact that if <span class="math inline">\(e_{i}\)</span> is a unit eigenvector with eigenvalue <span class="math inline">\(\lambda_{i}\)</span> then <span class="math display">\[
\sigma_{e_{i}}^2 = e_{i}^{\intercal}D_{0}e_{i} = \lambda_{i}\|e_{i}\|^2=\lambda_{i}.
\]</span></p>
<p>To really make this result pay off, we need to recall some key facts about the eigenvalues and eigenvectors of symmetric matrices. Because these facts are so central to this result, and to other applications throughout machine learning and mathematics generally, we provide proofs in <a href="#sec-spectraltheorem"><span>Section&nbsp;2.5</span></a>.</p>
<div id="tbl-symmmat" class="anchored">
<table class="table">
<caption>Table&nbsp;2.1: Properties of Eigenvalues of Real Symmetric Matrices</caption>
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Summary</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1. All of the eigenvalues <span class="math inline">\(\lambda_{1},\ldots, \lambda_{l}\)</span> of <span class="math inline">\(D\)</span> are real. If <span class="math inline">\(u^{\intercal}Du\ge 0\)</span> for all <span class="math inline">\(u\in\mathbf{R}^{k}\)</span>, then all eigenvalues <span class="math inline">\(\lambda_{i}\)</span> are non-negative. In the latter case we say that <span class="math inline">\(D\)</span> is <em>positive semi-definite.</em></td>
</tr>
<tr class="even">
<td style="text-align: left;">2. If <span class="math inline">\(v\)</span> is an eigenvector for <span class="math inline">\(D\)</span> with eigenvalue <span class="math inline">\(\lambda\)</span>, and <span class="math inline">\(w\)</span> is an eigenvector with a different eigenvalue <span class="math inline">\(\lambda'\)</span>, then <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> are orthogonal: <span class="math inline">\(v\cdot w = 0\)</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3. There is an orthonormal basis <span class="math inline">\(u_{1},\ldots, u_{k}\)</span> of <span class="math inline">\(\mathbf{R}^{k}\)</span> made up of eigenvectors of <span class="math inline">\(D\)</span> corresponding to the eigenvalues <span class="math inline">\(\lambda_{i}\)</span>.</td>
</tr>
<tr class="even">
<td style="text-align: left;">4. Let <span class="math inline">\(\Lambda\)</span> be the diagonal matrix with entries <span class="math inline">\(\lambda_{1},\ldots, \lambda_{N}\)</span> and let <span class="math inline">\(P\)</span> be the matrix whose columns are made up of the vectors <span class="math inline">\(u_{i}\)</span>. Then <span class="math inline">\(D = P\Lambda P^{\intercal}.\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>If we combine our theorem on the critical values with the spectral theorem we get a complete picture. Let <span class="math inline">\(D_{0}\)</span> be the covariance matrix of our data. Since <span class="math display">\[
\sigma_{u}^2 = u^{\intercal}D_{0}u\ge 0 \hbox{(it's a sum of squares)}
\]</span> we know that the eigenvalues <span class="math inline">\(\lambda_{1}\ge\lambda_{2}\ge \cdots \ge \lambda_{k}\ge 0\)</span> are all nonnegative. Choose a corresponding sequence <span class="math inline">\(u_{1},\ldots u_{k}\)</span> of orthogonal eigenvectors where all <span class="math inline">\(\|u_{i}\|^2=1\)</span>. Since the <span class="math inline">\(u_{i}\)</span> form a basis of <span class="math inline">\(\mathbf{R}^{N}\)</span>, any score is a linear combination of the <span class="math inline">\(u_{i}\)</span>: <span class="math display">\[
S = \sum_{i=1}^{k} a_{i}u_{i}.
\]</span> Since <span class="math inline">\(u_{i}^{\intercal}D_{0}u_{j} = \lambda_{j}u_{i}^{\intercal}u_{j} = 0\)</span> unless <span class="math inline">\(i=j\)</span>, in which case it is <span class="math inline">\(\lambda_{i}\)</span>, we can compute <span class="math display">\[
\sigma_{S}^2 = \sum_{i=1}^{k} \lambda_{i}a_{i}^2,
\]</span> and <span class="math inline">\(\|S\|^2=\sum_{i=1}^{k} a_{i}^2\)</span> since the <span class="math inline">\(u_{i}\)</span> are an orthonormal set. So in these coordinates, our optimization problem is:</p>
<ul>
<li>maximize <span class="math inline">\(\sum \lambda_{i}a_{i}^2\)</span></li>
<li>subject to the constraint <span class="math inline">\(\sum a_{i}^2 = 1\)</span>.</li>
</ul>
<p>We don’t need any fancy math to see that the maximum happens when <span class="math inline">\(a_{1}=1\)</span> and the other <span class="math inline">\(a_{j}=0\)</span>, and in that case, the maximum is <span class="math inline">\(\lambda_{1}\)</span>. (If <span class="math inline">\(\lambda_{1}\)</span> occurs more than once, there may be a whole subspace of directions where the variance is maximal). Similarly, the minimum value is <span class="math inline">\(\lambda_{k}\)</span> and occurs when <span class="math inline">\(a_{k}=1\)</span> and the others are zero.</p>
</section>
<section id="sec-subspaces" class="level3" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="sec-subspaces"><span class="header-section-number">2.3.4</span> Subspaces of extremal variance</h3>
<p>We can generalize the idea of the variance of our data in a particular direction to a higher dimensional version of <em>total variance</em> in a subspace. Suppose that <span class="math inline">\(E\)</span> is a subspace of <span class="math inline">\(\mathbf{R}^{k}\)</span> and <span class="math inline">\(U\)</span> is a matrix whose columns span <span class="math inline">\(E\)</span> – the columns of <span class="math inline">\(U\)</span> are the weights of a family of scores that span <span class="math inline">\(E\)</span>. The values of these scores are <span class="math inline">\(XU\)</span> and the covariance matrix of this projected data is <span class="math display">\[\frac{1}{N}U^{\intercal}X^{\intercal}XU=U^{\intercal}D_{0}U.\]</span>.</p>
<p>Finally, the <em>total variance</em> <span class="math inline">\(\sigma_{E}^2\)</span> of the data projected into <span class="math inline">\(E\)</span> is the sum of the diagonal entries of the matrix</p>
<p><span class="math display">\[
\sigma^2_{E} = \mathop{trace}(U^{\intercal}D_{0}U)
\]</span></p>
<p>Just as the variance in a given direction <span class="math inline">\(u\)</span> depends on the scaling of <span class="math inline">\(u\)</span>, the variance in a subspace depends on the scaling of the columns of <span class="math inline">\(U\)</span>. To normalize this scaling, we assume that the columns of <span class="math inline">\(U\)</span> are an orthonormal basis of the subspace <span class="math inline">\(E\)</span>.</p>
<p>Now we can generalize the question asked in <a href="#sec-extremalvariance"><span>Section&nbsp;2.3.2</span></a> by seeking, not just a vector <span class="math inline">\(u\)</span> pointing in the direction of the extremal variance, but instead the <em>subspace</em> <span class="math inline">\(U_{s}\)</span> of dimension <span class="math inline">\(s\)</span> with the property that the total variance of the projection of the data into <span class="math inline">\(U_{s}\)</span> is maximal compared to its projection into other subspaces of that dimension. This is called a <em>subspace of extremal variance.</em></p>
<p>To make this concrete, suppose we consider a subspace <span class="math inline">\(E\)</span> of <span class="math inline">\(\mathbf{R}^{k}\)</span> of dimension <span class="math inline">\(t\)</span> with basis <span class="math inline">\(w_{1},\ldots, w_{t}\)</span>. Complete this to a basis <span class="math inline">\(w_{1},\ldots, w_{t},w_{t+1},\ldots, w_{k}\)</span> of <span class="math inline">\(\mathbf{R}^{k}\)</span> and then apply the Gram Schmidt Process (see <a href="#sec-gsprocess"><span>Section&nbsp;2.5.1</span></a>) to find an orthonormal basis <span class="math inline">\(w'_{1},\ldots,w'_{s},w'_{s+1},\ldots, w'_{k}\)</span> where the <span class="math inline">\(w'_{1},\ldots, w'_{t}\)</span> are an orthonormal basis for <span class="math inline">\(E\)</span>. Let <span class="math inline">\(W\)</span> be the <span class="math inline">\(k\times t\)</span> matrix whose columns are the <span class="math inline">\(w'_{i}\)</span> for <span class="math inline">\(i=1,\ldots,t\)</span>. The rows of the matrix <span class="math inline">\(X_{0}W\)</span> given the coordinates of the projection of each sample into the subspace <span class="math inline">\(E\)</span> expressed in terms of the scores corresponding to these vectors <span class="math inline">\(w'_{i}\)</span>. The total variance of these projections is</p>
<p><span class="math display">\[
\sigma_{E}^2 = \sum_{i=1}^{t} \|X_{0}w'_{i}\|^2 = \sum_{i=1}^{t} (w'_{i})^{\intercal}X_{0}^{\intercal}X_{0}w'_{i}  = \sum_{i=1}^{t} (w'_{i})^{\intercal}D_{0}w'_{i}
\]</span></p>
<p>If we want to maximize this, we have the constrained optimization problem of finding <span class="math inline">\(w'_{1},\ldots, w'_{t}\)</span> so that</p>
<ul>
<li><span class="math inline">\(\sum_{i=1}^{t} (w'_{i})^{\intercal}D_{0}w'_{i}\)</span> is maximal</li>
<li>subject to the constraint that each <span class="math inline">\(w_{i}\)</span> has <span class="math inline">\(\|w'_{i}\|^2=1\)</span>,</li>
<li>and that the <span class="math inline">\(w'_{i}\)</span> are orthogonal, meaning <span class="math inline">\(w'_{i}\cdot w'_{j}=0\)</span> for <span class="math inline">\(i\not=j\)</span>,</li>
<li>and that the <span class="math inline">\(w'_{i}\)</span> are linearly independent.</li>
</ul>
<p>Then the span <span class="math inline">\(E\)</span> of these <span class="math inline">\(w'_{i}\)</span> is subspace of extremal variance.</p>
<p><strong>Theorem:</strong> A <span class="math inline">\(t\)</span>-dimensional subspace <span class="math inline">\(E\)</span> is a subspace of extremal variance if and only if it is spanned by <span class="math inline">\(t\)</span> orthonormal eigenvectors of the matrix <span class="math inline">\(D_{0}\)</span> corresponding to the <span class="math inline">\(t\)</span> largest eigenvalues for <span class="math inline">\(D_{0}\)</span>.</p>
<p><strong>Proof:</strong> We can approach this problem using Lagrange multipliers and matrix calculus if we are careful. Our unknown is <span class="math inline">\(k\times t\)</span> matrix <span class="math inline">\(W\)</span> whose columns are the <span class="math inline">\(t\)</span> (unknown) vectors <span class="math inline">\(w'_{i}\)</span>. The objective function that we are seeking to maximize is <span class="math display">\[
F = \mathop{trace}(W^{\intercal}D_{0}W) = \sum_{i=1}^{t} (w'_{i})^{\intercal}D_{0}w_{i}.
\]</span> The constraints are the requirements that <span class="math inline">\(\|w'_{i}\|^2=1\)</span> and <span class="math inline">\(w'_{i}\cdot w'_{j}=0\)</span> if <span class="math inline">\(i\not=j\)</span>. If we introduction a matrix of lagrange multipliers <span class="math inline">\(\Lambda=(\lambda_{ij})\)</span>, where <span class="math inline">\(\lambda_{ij}\)</span> is the multiplier that goes with the the first of these constraints when <span class="math inline">\(i=j\)</span>, and the second when <span class="math inline">\(i\not=j\)</span>, we can express our Lagrange function as: <span class="math display">\[
S(W,\Lambda) = \mathop{trace}(W^{\intercal}D_{0}W) - (W^{\intercal}W-I)\Lambda
\]</span> where <span class="math inline">\(I\)</span> is the <span class="math inline">\(t\times t\)</span> identity matrix.</p>
<p>Taking the derivatives with respect to the entries of <span class="math inline">\(W\)</span> and of <span class="math inline">\(\Lambda\)</span> yields the following two equations: <span class="math display">\[\begin{align*}
D_{0}W &amp;= W\Lambda \\
W^{\intercal}W &amp;= I \\
\end{align*}\]</span></p>
<p>The first of these equations says that the space <span class="math inline">\(E\)</span> spanned by the columns of <span class="math inline">\(W\)</span> is <em>invariant</em> under <span class="math inline">\(D_{0}\)</span>, while the second says that the columns of <span class="math inline">\(W\)</span> form an orthonormal basis.</p>
<p>Let’s assume for the moment that we have a matrix <span class="math inline">\(W\)</span> that satisfies these conditions.<br>
Then it must be the case that <span class="math inline">\(\Lambda\)</span> is a symmetric, real valued <span class="math inline">\(t\times t\)</span> matrix, since <span class="math display">\[
W^{\intercal}D_{0}W = W^{\intercal}W\Lambda = \Lambda.
\]</span> and the matrix on the left is symmetric.</p>
<p>By the properties of real symmetric matrices (the spectral theorem), there are orthonormal vectors <span class="math inline">\(q_{1},\ldots q_{t}\)</span> that are eigenvectors of <span class="math inline">\(\Lambda\)</span> with corresponding eigenvalues <span class="math inline">\(\tau_{i}\)</span>. If we let <span class="math inline">\(Q\)</span> be the matrix whose columns are the vectors <span class="math inline">\(q_{i}\)</span> and let <span class="math inline">\(T\)</span> be the diagonal <span class="math inline">\(t\times t\)</span> matrix whose entries are the <span class="math inline">\(\tau_{i}\)</span>, we have <span class="math display">\[
\Lambda Q = QT.
\]</span></p>
<p>If we go back to our original equations, we see that if <span class="math inline">\(W\)</span> exists such that <span class="math inline">\(DW=W\Lambda\)</span>, then there is a matrix <span class="math inline">\(Q\)</span> with orthonormal columns and a diagonal matrix <span class="math inline">\(T\)</span> such that <span class="math display">\[
D_{0}WQ = W\Lambda Q = W Q T.
\]</span> In other words, <span class="math inline">\(WQ\)</span> is a matrix whose columns are eigenvectors of <span class="math inline">\(D_{0}\)</span> with eigenvalues <span class="math inline">\(\tau_{i}\)</span> for <span class="math inline">\(i=1,\ldots, t\)</span>.</p>
<p>Thus we see how to construct an invariant subspace <span class="math inline">\(E\)</span> and a solution matrix <span class="math inline">\(W\)</span>. Such an <span class="math inline">\(E\)</span> is spanned by <span class="math inline">\(t\)</span> orthonormal eigenvectors <span class="math inline">\(q_{i}\)</span> with eigenvalues <span class="math inline">\(\tau_{i}\)</span> of <span class="math inline">\(D_{0}\)</span>; and <span class="math inline">\(W\)</span> is is the matrix whose columns are the <span class="math inline">\(q_{i}\)</span>. Further, in that case, the total variance associated to <span class="math inline">\(E\)</span> is the sum of the eigenvalues <span class="math inline">\(\tau_{i}\)</span>; to make this as large as possible, we should choose our eigenvectors to correspond to <span class="math inline">\(t\)</span> of the largest eigenvalues of <span class="math inline">\(D_{0}\)</span>. This concludes the proof.</p>
</section>
<section id="definition-of-principal-components" class="level3" data-number="2.3.5">
<h3 data-number="2.3.5" class="anchored" data-anchor-id="definition-of-principal-components"><span class="header-section-number">2.3.5</span> Definition of Principal Components</h3>
<p><strong>Definition:</strong> The orthonormal unit eigenvectors <span class="math inline">\(u_{i}\)</span> for <span class="math inline">\(D_{0}\)</span> are the <em>principal directions</em> or <em>principal components</em> for the data <span class="math inline">\(X_{0}\)</span>.</p>
<p><strong>Theorem:</strong> The maximum variance occurs in the principal direction(s) associated to the largest eigenvalue, and the minimum variance in the principal direction(s) associated with the smallest one. The covariance between scores in principal directions associatedwith different eigenvalues is zero.</p>
<p>At this point, the picture in <a href="#fig-pcaprincipal">Figure&nbsp;<span>2.8</span></a> makes sense – the red and green dashed lines are the principal directions, they are orthogonal to one another, and the point in the directions where the data is most (and least) “spread out.”</p>
<p><strong>Proof:</strong> The statement about the largest and smallest eigenvalues is proved at the very end of the last section. The covariance of two scores corresponding to different eigenvectors <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(u_{j}\)</span> is <span class="math display">\[u_{i}^{\intercal}D_{0}u_{j} = \lambda_{j}(u_{i}\cdot u_{j}) = 0\]</span> since the <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(u_{j}\)</span> are orthogonal.</p>
<p>Sometimes the results above are presented in a slightly different form, and may be referred to, in part, as Rayleigh’s theorem.</p>
<p><strong>Corollary:</strong> (Rayleigh’s Theorem) Let <span class="math inline">\(D\)</span> be a real symmetric matrix and let <span class="math display">\[
H(v) = \max_{v\not = 0}\frac{v^{\intercal}Dv}{v^{\intercal}v}.
\]</span> Then <span class="math inline">\(H(v)\)</span> is the largest eigenvalue of <span class="math inline">\(D\)</span>. (Similarly, if we replace <span class="math inline">\(\max\)</span> by <span class="math inline">\(\min\)</span>, then the minimum is the least eigenvalue).</p>
<p><strong>Proof:</strong> The maximum of the function <span class="math inline">\(H(v)\)</span> is the solution to the same optimization problem that we considered above.</p>
<p><strong>Exercises.</strong></p>
<ol type="1">
<li><p>Prove that the two expressions for <span class="math inline">\(\sigma_{X}^2\)</span> given in <a href="#eq-variance">Equation&nbsp;<span>2.1</span></a> are the same.</p></li>
<li><p>Prove that the covariance matrix is as described in the proposition in <a href="#sec-covarmat"><span>Section&nbsp;2.2.4</span></a>.</p></li>
<li><p>Let <span class="math inline">\(X_{0}\)</span> be a <span class="math inline">\(k\times N\)</span> matrix with entries <span class="math inline">\(x_{ij}\)</span> for <span class="math inline">\(1\le i\le k\)</span> and <span class="math inline">\(1\le j\le N\)</span>. If a linear score is defined by the constants <span class="math inline">\(a_{1},\ldots a_{N}\)</span>, check that equation <a href="#eq-linearscore">Equation&nbsp;<span>2.5</span></a> holds as claimed.</p></li>
<li><p>Why is it important to use a unit vector when computing the variance of <span class="math inline">\(X_{0}\)</span> in the direction of <span class="math inline">\(u\)</span>? Suppose <span class="math inline">\(v=\lambda u\)</span> where <span class="math inline">\(u\)</span> is a unit vector and <span class="math inline">\(\lambda&gt;0\)</span> is a constant. Let <span class="math inline">\(S'\)</span> be the score <span class="math inline">\(X_{0}v\)</span>. How is the variance of <span class="math inline">\(S'\)</span> related to that of <span class="math inline">\(S=X_{0}u\)</span>?</p></li>
</ol>
</section>
</section>
<section id="dimensionality-reduction-via-principal-components" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="dimensionality-reduction-via-principal-components"><span class="header-section-number">2.4</span> Dimensionality Reduction via Principal Components</h2>
<p>The principal components associated with a dataset separate out directions in the feature space in which the data is most (or least) variable. One of the main applications of this information is to enable us to take data with a great many features – a set of points in a high dimensional space – and, by focusing our attention on the scores corresponding to the principal directions, capture most of the information in the data in a much lower dimensional setting.</p>
<p>To illustrate how this is done, let <span class="math inline">\(X\)</span> be a <span class="math inline">\(N\times k\)</span> data matrix, let <span class="math inline">\(X_{0}\)</span> be its centered version, and let <span class="math inline">\(D_{0} = \frac{1}{N}X_{0}^{\intercal}X\)</span> be the associated covariance matrix.</p>
<p>Apply the spectral theorem (proved in <a href="#sec-spectraltheorem"><span>Section&nbsp;2.5</span></a>) to the covariance matrix to obtain eigenvalues <span class="math inline">\(\lambda_{1}\ge \lambda_{2}\ge\cdots \lambda_{k}\ge 0\)</span> and associated eigenvectors <span class="math inline">\(u_{1},\ldots, u_{k}\)</span>. The scores <span class="math inline">\(S_{i}=X_{0}u_{i}\)</span> give the values of the data in the principal directions. The variance of <span class="math inline">\(S_{i}\)</span> is <span class="math inline">\(\lambda_{i}\)</span>.</p>
<p>Now choose a number <span class="math inline">\(t&lt;k\)</span> and consider the vectors <span class="math inline">\(S_{1},\ldots, S_{t}\)</span>. The <span class="math inline">\(j^{th}\)</span> entry in <span class="math inline">\(S_{i}\)</span> is the value of the score <span class="math inline">\(S_{i}\)</span> for the <span class="math inline">\(j^{th}\)</span> data point. Because <span class="math inline">\(S_{1},\ldots, S_{t}\)</span> capture the most significant variability in the original data, we can learn a lot about our data by considering just these <span class="math inline">\(t\)</span> features of the data, instead of needing all <span class="math inline">\(N\)</span>.</p>
<p>To illustrate, let’s look at an example. We begin with a synthetic dataset <span class="math inline">\(X_{0}\)</span> which has <span class="math inline">\(200\)</span> samples and <span class="math inline">\(15\)</span> features. The data (some of it) for some of the samples is shown in <a href="#tbl-rawdata">Table&nbsp;<span>2.2</span></a>.</p>
<div id="tbl-rawdata" class="anchored">
<table class="table">
<caption>Table&nbsp;2.2: Simulated Data for PCA Analysis</caption>
<thead>
<tr class="header">
<th></th>
<th style="text-align: left;">f-0</th>
<th style="text-align: left;">f-1</th>
<th style="text-align: left;">f-2</th>
<th style="text-align: left;">f-3</th>
<th style="text-align: left;">f-4</th>
<th>...</th>
<th style="text-align: left;">f-10</th>
<th style="text-align: left;">f-11</th>
<th style="text-align: left;">f-12</th>
<th style="text-align: left;">f-13</th>
<th style="text-align: left;">f-14</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>s-0</td>
<td style="text-align: left;">1.18</td>
<td style="text-align: left;">-0.41</td>
<td style="text-align: left;">2.02</td>
<td style="text-align: left;">0.44</td>
<td style="text-align: left;">2.24</td>
<td>...</td>
<td style="text-align: left;">0.32</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">0.88</td>
<td style="text-align: left;">1.10</td>
<td style="text-align: left;">0.89</td>
</tr>
<tr class="even">
<td>s-1</td>
<td style="text-align: left;">0.74</td>
<td style="text-align: left;">0.58</td>
<td style="text-align: left;">1.54</td>
<td style="text-align: left;">0.23</td>
<td style="text-align: left;">2.05</td>
<td>...</td>
<td style="text-align: left;">0.99</td>
<td style="text-align: left;">1.14</td>
<td style="text-align: left;">1.56</td>
<td style="text-align: left;">0.99</td>
<td style="text-align: left;">0.59</td>
</tr>
<tr class="odd">
<td>...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td>...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
</tr>
<tr class="even">
<td>s-198</td>
<td style="text-align: left;">1.04</td>
<td style="text-align: left;">2.02</td>
<td style="text-align: left;">1.44</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">1.33</td>
<td>...</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">0.54</td>
<td style="text-align: left;">1.96</td>
<td style="text-align: left;">0.04</td>
</tr>
<tr class="odd">
<td>s-199</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">2.09</td>
<td style="text-align: left;">1.58</td>
<td style="text-align: left;">1.19</td>
<td style="text-align: left;">1.17</td>
<td>...</td>
<td style="text-align: left;">0.42</td>
<td style="text-align: left;">0.85</td>
<td style="text-align: left;">0.83</td>
<td style="text-align: left;">2.22</td>
<td style="text-align: left;">0.90</td>
</tr>
</tbody>
</table>
</div>
<p>The full dataset is a <span class="math inline">\(200\times 15\)</span> matrix; it has <span class="math inline">\(3000\)</span> numbers in it and we’re not really equipped to make sense of it. We could try some graphing – for example, <a href="#fig-features">Figure&nbsp;<span>2.9</span></a> shows a scatter plot of two of the features plotted against each other.</p>
<div id="fig-features" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/features.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.9: Scatter Plot of Two Features</figcaption><p></p>
</figure>
</div>
<p>Unfortunately there’s not much to see in <a href="#fig-features">Figure&nbsp;<span>2.9</span></a> – just a blob – because the individual features of the data don’t tell us much in isolation, whatever structure there is in this data arises out of the relationship between different features.</p>
<p>In <a href="#fig-densitygrid">Figure&nbsp;<span>2.10</span></a> we show a “density grid” plot of the data. The graph in position <span class="math inline">\(i,j\)</span> shows a scatter plot of the <span class="math inline">\(i^{th}\)</span> and <span class="math inline">\(j^{th}\)</span> columns of the data, except in the diagonal positions, where in position <span class="math inline">\(i,i\)</span> we plot a histogram of column <span class="math inline">\(i\)</span>. There’s not much structure visible; it is a lot of blobs.</p>
<div id="fig-densitygrid" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/density.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.10: Density Grid Plot of All Features</figcaption><p></p>
</figure>
</div>
<p>So let’s apply the theory of principal components. We use a software package to compute the eigenvalues and eigenvectors of the matrix <span class="math inline">\(D_{0}\)</span>. The <span class="math inline">\(15\)</span> eigenvalues <span class="math inline">\(\lambda_{1}\ge \cdots \ge \lambda_{15}\)</span> are plotted, in descending order, in <a href="#fig-eigenvalues">Figure&nbsp;<span>2.11</span></a> .</p>
<div id="fig-eigenvalues" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/eigenvalues.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.11: Eigenvalues of the Covariance Matrix</figcaption><p></p>
</figure>
</div>
<p>This plot shows that the first <span class="math inline">\(4\)</span> eigenvalues are relatively large, while the remaining <span class="math inline">\(11\)</span> are smaller and not much different from each other. We interpret this as saying that <em>most of the variation in the data is accounted for by the first four principal components.</em> We can even make this quantitative. The <em>total variance</em> of the data is the sum of the eigenvalues of the covariance matrix – the trace of <span class="math inline">\(D_{0}\)</span> – and in this example that sum is around <span class="math inline">\(5\)</span>. The sum of the first <span class="math inline">\(4\)</span> eigenvalues is about <span class="math inline">\(4\)</span>, so the first four eignvalues account for about <span class="math inline">\(4/5\)</span> of the total variance, or about <span class="math inline">\(80\%\)</span> of the variation of the data.</p>
<p>Now let’s focus in on the two largest eigenvalues <span class="math inline">\(\lambda_{1}\)</span> and <span class="math inline">\(\lambda_{2}\)</span> and their corresponding eigenvectors <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span>. The <span class="math inline">\(200\times 1\)</span> column vectors <span class="math inline">\(S_{1}=X_{0}u_{1}\)</span> and <span class="math inline">\(S_{2}=X_{0}u_{2}\)</span> are the values of the scores associated with these two eigenvectors. So for each data point (each row of <span class="math inline">\(X_{0}\)</span>) we have two values (the corresponding entries of <span class="math inline">\(S_{1}\)</span> and <span class="math inline">\(S_{2}\)</span>.) In <a href="#fig-principalvalues">Figure&nbsp;<span>2.12</span></a> we show a scatter plot of these scores.</p>
<div id="fig-principalvalues" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/pcadimred.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.12: Scatter Plot of Scores in the First Two Principal Directions</figcaption><p></p>
</figure>
</div>
<p>Notice that suddenly some structure emerges in our data! We can see that the 200 points are separated into five clusters, distinguished by the values of their scores! This ability to find hidden structure in complicated data, is one of the most important applications of principal components.</p>
<p>If we were dealing with real data, we would now want to investigate the different groups of points to see if we can understand what characteristics the principal components have identified.</p>
<section id="loadings" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="loadings"><span class="header-section-number">2.4.1</span> Loadings</h3>
<p>There’s one last piece of the PCA puzzle that we are going to investigate. In <a href="#fig-principalvalues">Figure&nbsp;<span>2.12</span></a>, we plotted our data points in the coordinates given by the first two principal components. In geometric terms, we took the cloud of <span class="math inline">\(200\)</span> points in <span class="math inline">\(\mathbf{R}^{15}\)</span> given by the rows of <span class="math inline">\(X_{0}\)</span> and projected those points into the two dimensional plane spanned by the eigenvectors <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span>, and then plotted the distribution of the points in that plane.</p>
<p>More generally, suppose we take our dataset <span class="math inline">\(X_{0}\)</span> and consider the first <span class="math inline">\(t\)</span> principal components corresponding to the eigenvectors <span class="math inline">\(u_{1},\ldots, u_{t}\)</span>. The projection of the data into the space spanned by these eigenvectors is the represented by the <span class="math inline">\(S = k\times t\)</span> matrix <span class="math inline">\(X_{0}U\)</span> where <span class="math inline">\(U\)</span> is the <span class="math inline">\(k\times t\)</span> matrix whose columns are the eigenvectors <span class="math inline">\(u_{i}\)</span>. Each row of <span class="math inline">\(S\)</span> gives the values of the score arising from <span class="math inline">\(u_{i}\)</span> in the <span class="math inline">\(i^{th}\)</span> column for <span class="math inline">\(i=1,\ldots, t\)</span>.</p>
<p>The remaining question that we wish to consider is: how can we see some evidence of the original features in subspace? We can answer this by imagining that we had an artificial sample <span class="math inline">\(x\)</span> that has a measurement of <span class="math inline">\(1\)</span> for the <span class="math inline">\(i^{th}\)</span> feature and a measurement of zero for all the other features. The corresponding point is represented by a <span class="math inline">\(1\times k\)</span> row vector with a <span class="math inline">\(1\)</span> in position <span class="math inline">\(i\)</span>. The projection of this synthetic sample into the span of the first <span class="math inline">\(t\)</span> principal components is the <span class="math inline">\(1\times t\)</span> vector <span class="math inline">\(xU\)</span>. Notice, however, that <span class="math inline">\(xU\)</span> is just the <span class="math inline">\(i^{th}\)</span> row of the matrix <span class="math inline">\(U\)</span>. This vector in the space spanned by the <span class="math inline">\(u_{i}\)</span> is called the “loading” of the <span class="math inline">\(i^{th}\)</span> feature in the principal components.</p>
<p>This is illustrated in <a href="#fig-loadings">Figure&nbsp;<span>2.13</span></a>, which shows a line along the direction of the loading corresponding to the each feature added to the scatter plot of the data in the plane spanned by the first two principal components. One observation one can make is that some of the features are more “left to right”, like features <span class="math inline">\(7\)</span> and <span class="math inline">\(8\)</span>, while others are more “top to bottom”, like <span class="math inline">\(6\)</span>. So points that lie on the left side of the plot have smaller values of features <span class="math inline">\(7\)</span> and <span class="math inline">\(8\)</span>, while those at the top of the plot have larger values of feature <span class="math inline">\(6\)</span>.</p>
<div id="fig-loadings" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/loading.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.13: Loadings in the Principal Component Plane</figcaption><p></p>
</figure>
</div>
</section>
<section id="sec-svd" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="sec-svd"><span class="header-section-number">2.4.2</span> The singular value decomposition</h3>
<p>The singular value decomposition is a slightly more detailed way of looking at principal components. Let <span class="math inline">\(\Lambda\)</span> be the diagonal matrix of eigenvalues of <span class="math inline">\(D_{0}\)</span> and let <span class="math inline">\(P\)</span> be the <span class="math inline">\(k\times k\)</span> orthogonal matrix whose columns are the principal components. Then we have <span class="math display">\[
D_{0} =\frac{1}{N}X_{0}^{\intercal}X_{0}= P\Lambda P^{\intercal}.
\]</span> Consider the <span class="math inline">\(N\times k\)</span> matrix <span class="math display">\[
X_{0}P = A.
\]</span></p>
<p>As we saw in the previous section, the columns of <span class="math inline">\(A\)</span> give the projection of the data into the <span class="math inline">\(k\)</span> principal directions. Then</p>
<p><span class="math display">\[A^{\intercal}A=P^{\intercal}X_{0}^{\intercal}X_{0}P=N\Lambda.\]</span></p>
<p>In other words, the columns of <span class="math inline">\(A\)</span> are orthogonal and the diagonal entries of <span class="math inline">\(A^{\intercal}A\)</span> are <span class="math inline">\(N\)</span> times the variance of the data in the various principal directions.</p>
<p>Now we are going to tinker with the matrix <span class="math inline">\(A\)</span> in order to make an <span class="math inline">\(N\times N\)</span> orthogonal matrix. The first modification we make is to normalize the columns of <span class="math inline">\(A\)</span> so that they have length <span class="math inline">\(1\)</span>. We do this by setting <span class="math display">\[
A_{1} = A(N\Lambda)^{-1/2}.
\]</span> Then <span class="math inline">\(A_{1}^{\intercal}A_{1}\)</span> is the identity, so the columns of <span class="math inline">\(A_{1}\)</span> are orthonormal. Here we are assuming that the eigenvalues of <span class="math inline">\(D_{0}\)</span> are nonzero – this isn’t strictly necessary, and we could work around this, but for simplicity we will assume it’s true. It amounts to the assumption that the independent variables are not linearly related, as we’ve seen before.</p>
<p>The second modification is to extend <span class="math inline">\(A_{1}\)</span> to an <span class="math inline">\(N\times N\)</span> matrix. The <span class="math inline">\(k\)</span> columns of <span class="math inline">\(A_1\)</span> span only a <span class="math inline">\(k\)</span>-dimensional subspace of the <span class="math inline">\(N\)</span>-dimensional space where the feature vectors lie.<br>
Complete the subspace by finding an orthogonal complement to it – that is, find <span class="math inline">\(N-k\)</span> mutually orthogonal unit vectors all orthogonal to the column space of <span class="math inline">\(A_1\)</span>. By adding these vectors to <span class="math inline">\(A\)</span> as columns, create an extended <span class="math inline">\(N\times N\)</span> matrix <span class="math inline">\(\tilde{A}_1\)</span> which is orthogonal.</p>
<p>Notice that <span class="math inline">\(\tilde{A}_{1}^{\intercal}A\)</span> is an <span class="math inline">\(N\times k\)</span> matrix whose upper <span class="math inline">\(k\times k\)</span> block is <span class="math inline">\((N\Lambda)^{1/2}\)</span> and whose final <span class="math inline">\(N-k\)</span> rows are all zero. We call this matrix <span class="math inline">\(\tilde{\Lambda}\)</span>.</p>
<p>To maintain consistency with the traditional formulation, we let <span class="math inline">\(U=\tilde{A}_{1}^{\intercal}\)</span> and then we have the following proposition.</p>
<p><strong>Proposition:</strong> We have a factorization <span id="eq-svd"><span class="math display">\[
X_{0} = U\tilde{\Lambda}P^{\intercal}
\tag{2.9}\]</span></span> where <span class="math inline">\(U\)</span> and <span class="math inline">\(P\)</span> are orthogonal matrices of size <span class="math inline">\(N\times N\)</span> and <span class="math inline">\(k\times k\)</span> respectively, and <span class="math inline">\(\tilde{\Lambda}\)</span> is an <span class="math inline">\(N\times k\)</span> diagonal matrix. This is called the “singular value decomposition” of <span class="math inline">\(X_{0}\)</span>, and the entries of <span class="math inline">\(\tilde{\Lambda}\)</span> are called the singular values. If we let <span class="math inline">\(u_1,\ldots, u_k\)</span> be the first <span class="math inline">\(k\)</span> rows of <span class="math inline">\(U\)</span>, then the <span class="math inline">\(k\)</span> column vectors <span class="math inline">\(u_{i}^{\intercal}\)</span> are an orthonormal basis for the feature space spanned by the columns of <span class="math inline">\(X_{0}\)</span>, and they point in the “principal directions” for the data matrix <span class="math inline">\(X_{0}\)</span>.</p>
<p>In this section we take a slight detour and apply what we’ve learned about the covariance matrix, principal components, and the singular value decomposition to the original problem of linear regression that we studied in Chapter 1.</p>
<p>In this setting, in addition to our centered data matrix <span class="math inline">\(X_{0}\)</span>, we have a vector <span class="math inline">\(Y\)</span> of target values and we find the “best” approximation <span class="math display">\[
\hat{Y} = X_{0}M
\]</span> using the least squares method. As we showed in Chapter 1, the optimum <span class="math inline">\(M\)</span> is found as <span class="math display">\[
M = (X_{0}^{\intercal}X_{0})^{-1}X^{\intercal}Y = ND_{0}^{-1}X_0^{\intercal}Y
\]</span> and the predicted values <span class="math inline">\(\hat{Y}\)</span> are <span class="math display">\[
\hat{Y} = NX_{0}D_{0}^{-1}X_0^{\intercal}Y.
\]</span></p>
<p>Geometrically, we understood this process as defining <span class="math inline">\(\hat{Y}\)</span> to be the orthogonal projection of <span class="math inline">\(Y\)</span> into the subspace spanned by the columns of <span class="math inline">\(X_{0}\)</span>.</p>
<p>Let’s use the decomposition (see <a href="#eq-svd">Equation&nbsp;<span>2.9</span></a> ) <span class="math inline">\(X_{0}=U\tilde{\Lambda}P^{\intercal}\)</span> in this formula. First, notice that <span class="math display">\[
X_{0}^{\intercal}X_{0}= P\tilde{\Lambda}^{\intercal}U^{\intercal}U\tilde{\Lambda}P^{\intercal} = P\tilde{\Lambda}^{\intercal}\tilde{\Lambda}P^{\intercal}.
\]</span></p>
<p>The middle term <span class="math inline">\(\tilde{\Lambda}^{\intercal}\tilde{\Lambda}\)</span> is the <span class="math inline">\(k\times k\)</span> matrix <span class="math inline">\(\Lambda\)</span> whose diagonal entries are <span class="math inline">\(N\lambda_{i}\)</span> where <span class="math inline">\(\lambda_{i}\)</span> are the eigenvalues of the covariance matrix <span class="math inline">\(D_{0}\)</span>. Assuming these are all nonzero (which is tantamount to the assumption that the covariance matrix is invertible), we obtain <span class="math display">\[
\hat{Y} = NU\tilde{\Lambda}P^{\intercal}P\Lambda^{-1}P^{\intercal}P\tilde{\Lambda}^{\intercal}U^{\intercal}Y.
\]</span> There is a lot of cancellation here, and in the end what’s left is <span class="math display">\[
\hat{Y}=UEU^{\intercal}Y
\]</span> where <span class="math inline">\(E\)</span> is and <span class="math inline">\(N\times N\)</span> matrix whose upper <span class="math inline">\(k\times k\)</span> block is the identity and whose remaining entries are zero. Rearranging a bit more we have <span class="math display">\[
U^{\intercal}\hat{Y} = EU^{\intercal}Y.
\]</span></p>
<p>To unpack this equation, let <span class="math inline">\(u_{1},\ldots, u_{N}\)</span> be the rows of the matrix <span class="math inline">\(U\)</span>. Since <span class="math inline">\(U\)</span> is an orthogonal matrix, the column vectors <span class="math inline">\(u_{i}^{\intercal}\)</span> are an orthonormal basis for the <span class="math inline">\(N\)</span> dimensional space where the columns of <span class="math inline">\(X_{0}\)</span> lie. We can write the target vector <span class="math inline">\(Y\)</span> <span class="math display">\[
Y = \sum_{j=1}^{N} (u_{j}\cdot Y)u_{j}^{\intercal}.
\]</span></p>
<p>Then the projection <span class="math inline">\(\hat{Y}\)</span> of <span class="math inline">\(Y\)</span> into the subspace spanned by the data is obtained by dropping the last <span class="math inline">\(N-k\)</span> terms in the sum:</p>
<p><span class="math display">\[
\hat{Y}=\sum_{j=1}^{k} (u_{j}\cdot Y)u_{j}^{\intercal}
\]</span></p>
</section>
</section>
<section id="sec-spectraltheorem" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-spectraltheorem"><span class="header-section-number">2.5</span> Eigenvalues and Eigenvectors of Real Symmetric Matrices (The Spectral Theorem)</h2>
<p>Now that we’ve shown how to apply the theory of eigenvalues and eigenvectors of symmetric matrices to extract principal directions from data, and to use those principal directions to find structure, we will give a proof of the properties that we summarized in <a href="#tbl-symmmat">Table&nbsp;<span>2.1</span></a>.</p>
<p>A key tool in the proof is the Gram-Schmidt orthogonalization process.</p>
<section id="sec-gsprocess" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="sec-gsprocess"><span class="header-section-number">2.5.1</span> Gram-Schmidt</h3>
<p><strong>Proposition (Gram-Schmidt Process):</strong> Let <span class="math inline">\(w_{1},\ldots, w_{k}\)</span> be a collection of linearly independent vectors in <span class="math inline">\(\mathbf{R}^{N}\)</span> and let <span class="math inline">\(W\)</span> be the span of the <span class="math inline">\(w_{i}\)</span>. Let <span class="math inline">\(u_{1} = w_{1}\)</span> and let <span class="math display">\[
u_{i} = w_{i} - \sum_{j=1}^{i-1} \frac{w_{i}\cdot u_{j}}{u_{j}\cdot u_{j}}u_{j}
\]</span> for <span class="math inline">\(i=2,\ldots, k\)</span>. Then</p>
<ul>
<li>The vectors <span class="math inline">\(u_{i}\)</span> are orthogonal: <span class="math inline">\(u_{i}\cdot u_{j}=0\)</span> unless <span class="math inline">\(i=j\)</span>.</li>
<li>The vectors <span class="math inline">\(u_{i}\)</span> span <span class="math inline">\(W\)</span>.</li>
<li>Each <span class="math inline">\(u_{i}\)</span> is orthogonal to the all of <span class="math inline">\(w_{1},\ldots, w_{i-1}\)</span>.</li>
<li>The vectors <span class="math inline">\(u'_{i} = u_{i}/\|u_{i}\|\)</span> are orthonormal.</li>
</ul>
<p><strong>Proof:</strong> This is an inductive exercise, and we leave it to you to work out the details.</p>
</section>
<section id="the-spectral-theorem" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="the-spectral-theorem"><span class="header-section-number">2.5.2</span> The spectral theorem</h3>
<p><strong>Theorem:</strong> Let <span class="math inline">\(D\)</span> be a real symmetric <span class="math inline">\(N\times N\)</span> matrix. Then:</p>
<ol type="1">
<li>All of the <span class="math inline">\(N\)</span> eigenvalues <span class="math inline">\(\lambda_1\ge \lambda_2\ge \cdots \ge \lambda_{N}\)</span> are real. If <span class="math inline">\(u^{\intercal}Du\ge 0\)</span> for all <span class="math inline">\(u\in\mathbf{R}^{N}\)</span>, then all eigenvalues <span class="math inline">\(\lambda_{i}\ge 0\)</span>.</li>
<li>The matrix <span class="math inline">\(D\)</span> is diagonalizable – that is, it has <span class="math inline">\(N\)</span> linearly independent eigenvectors.</li>
<li>If <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> are eigenvectors corresponding to eigenvalues <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\lambda'\)</span>, with <span class="math inline">\(\lambda\not=\lambda'\)</span>, then <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> are orthogonal: <span class="math inline">\(v\cdot w=0\)</span>.</li>
<li>There is an orthonormal basis <span class="math inline">\(u_{1},\ldots, u_{N}\)</span> of <span class="math inline">\(\mathbf{R}^{N}\)</span> made up of eigenvectors for the eigenvalues <span class="math inline">\(\lambda_{i}\)</span>.</li>
<li>Let <span class="math inline">\(\Lambda\)</span> be the diagonal matrix with entries <span class="math inline">\(\lambda_{1},\ldots, \lambda_{N}\)</span> and let <span class="math inline">\(P\)</span> be the matrix whose columns are made up of the eigenvectors <span class="math inline">\(u_{i}\)</span>. Then <span class="math inline">\(D=P\Lambda P^{\intercal}\)</span>.</li>
</ol>
<p><strong>Proof:</strong> First of all, we use the fact that any matrix has at least one eigenvector with associated eigenvalue. This is a theorem from linear algebra that relies on the fundamental theorem of algebra. With this result available, we start by proving part 1. Suppose that <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(D\)</span>. Let <span class="math inline">\(u\)</span> be a corresponding nonzero eigenvector. Then <span class="math inline">\(Du=\lambda u\)</span> and <span class="math inline">\(D\overline{u}=\overline{\lambda}\overline{u}\)</span>, where <span class="math inline">\(\overline{u}\)</span> is the vector whose entries are the conjugates of the entries of <span class="math inline">\(u\)</span> (and <span class="math inline">\(\overline{D}=D\)</span> since <span class="math inline">\(D\)</span> is real). Now we have <span class="math display">\[
\overline{u}^{\intercal}Du = \lambda \overline{u}\cdot u = \lambda\|u\|^2
\]</span> and <span class="math display">\[
u^{\intercal}D\overline{u} = \overline{\lambda}u\cdot \overline{u} = \overline{\lambda}\|u\|^2.
\]</span> But the left hand side of both of these equations are the same (take the transpose and use the symmetry of <span class="math inline">\(D\)</span>) so we must have <span class="math inline">\(\lambda\|u\|^2 = \overline{\lambda}\|u\|^2\)</span> so <span class="math inline">\(\lambda=\overline{\lambda}\)</span>, meaning <span class="math inline">\(\lambda\)</span> is real.</p>
<p>If we have the additional property that <span class="math inline">\(u^{\intercal}Du\ge 0\)</span> for all <span class="math inline">\(u\)</span>, then in particular <span class="math inline">\(u_{i}^{\intercal}Du_{i} = \lambda\|u\|^2\ge 0\)</span>, and since <span class="math inline">\(\|u\|^2&gt; 0\)</span> we must have <span class="math inline">\(\lambda\ge 0\)</span>.</p>
<p>Property <span class="math inline">\(2\)</span> is in some ways the most critical fact. We know from the general theory of the characteristic polynomial, and the fundamental theorem of algebra, that <span class="math inline">\(D\)</span> has <span class="math inline">\(N\)</span> complex eigenvalues, although some may be repeated. However, it may not be the case that <span class="math inline">\(D\)</span> has <span class="math inline">\(N\)</span> linearly independent eigenvectors – it may not be <em>diagonalizable</em>. So we will establish that any <em>symmetric</em> matrix over the real numbers is diagonalizable.</p>
<p>A one-by-one matrix is automatically symmetric and diagonalizable. In the <span class="math inline">\(N\)</span>-dimensional case, we know, at least, that <span class="math inline">\(D\)</span> has at least one eigenvector, and real one at that by part <span class="math inline">\(1\)</span>, and this gives us a place to begin an inductive argument.</p>
<p>Let <span class="math inline">\(v_{N}\not=0\)</span> be an eigenvector with eigenvalue <span class="math inline">\(\lambda\)</span> and normalized so that <span class="math inline">\(\|v_{N}\|^2=1\)</span>,<br>
and extend this to a basis <span class="math inline">\(v_{1},\ldots v_{N}\)</span> of <span class="math inline">\(\mathbf{R}^{N}\)</span>. Apply the Gram-Schmidt process to construct an orthonormal basis of <span class="math inline">\(\mathbf{R}^{N}\)</span> <span class="math inline">\(u_{1},\ldots, u_{N}\)</span> so that <span class="math inline">\(u_{N}=v_{N}\)</span>.</p>
<p>Any vector <span class="math inline">\(v\in\mathbf{R}^{N}\)</span> is a linear combination <span class="math display">\[
v = \sum_{i=1}^{N} a_{i}u_{i}
\]</span> and, since the <span class="math inline">\(u_{i}\)</span> are orthonormal, the coefficients can be calculated as <span class="math inline">\(a_{i}=(u_{i}\cdot v)\)</span>.</p>
<p>Using this, we can find the matrix <span class="math inline">\(D'\)</span> of the linear map defined by our original matrix <span class="math inline">\(D\)</span> in this new basis. By definition, if <span class="math inline">\(d'_{ij}\)</span> are the entries of <span class="math inline">\(D'\)</span>, then</p>
<p><span class="math display">\[
Du_{i} = \sum_{j=1}^{N} d'_{ij} u_{j}
\]</span></p>
<p>and so</p>
<p><span class="math display">\[
d'_{ij} = u_{j}\cdot Du_{i} = u_{j}^{\intercal}Du_{i}.
\]</span></p>
<p>Since <span class="math inline">\(D\)</span> is symmetric, <span class="math inline">\(u_{j}^{\intercal}Du_{i} =u_{i}^{\intercal}Du_{j}\)</span> and so <span class="math inline">\(d'_{ij}=d'_{ji}\)</span>. In other words, the matrix <span class="math inline">\(D'\)</span> is still symmetric. Furthermore,</p>
<p><span class="math display">\[
d'_{Ni} = u_{i}\cdot Du_{N} = u_{i}\cdot \lambda u_{N} = \lambda (u_{i}\cdot u_{N})
\]</span></p>
<p>since <span class="math inline">\(u_{N}=v_{N}\)</span>. Since the <span class="math inline">\(u_{i}\)</span> are an orthonormal basis, we see that <span class="math inline">\(d'_{iN}=0\)</span> unless <span class="math inline">\(i=N\)</span>, and <span class="math inline">\(d'_{NN}=\lambda\)</span>.</p>
<p>In other words, the matrix <span class="math inline">\(D'\)</span> has a block form: <span class="math display">\[
D' = \left(\begin{matrix} *&amp;* &amp; \cdots &amp;*  &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots   &amp; \vdots &amp; \vdots \\
* &amp; *&amp; \cdots &amp;*  &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp;0 &amp;\lambda \end{matrix}\right)
\]</span> and the block denoted by <span class="math inline">\(*\)</span>’s is symmetric. If we call that block <span class="math inline">\(D_{*}\)</span>, the inductive hypothesis tells us that the symmetric matrix <span class="math inline">\(D_{*}\)</span> is diagonalizable, so it has a basis of eigenvectors <span class="math inline">\(u'_{1},\ldots, u'_{N-1}\)</span> with eigenvalues <span class="math inline">\(\lambda_{1},\ldots, \lambda_{N-1}\)</span>; this gives us a basis for the subspace of <span class="math inline">\(\mathbf{R}^{N}\)</span> spanned by <span class="math inline">\(u_{1},\ldots, u_{N-1}\)</span> which, together with <span class="math inline">\(u_{N}\)</span> gives us a basis of <span class="math inline">\(\mathbf{R}^{N}\)</span> consisting of eigenvectors of <span class="math inline">\(D\)</span>.</p>
<p>This finishes the proof of Property <span class="math inline">\(2\)</span>.</p>
<p>For property <span class="math inline">\(3\)</span>, compute <span class="math display">\[
v^{\intercal}Dw = \lambda'(v\cdot w)=w^{\intercal}Dv = \lambda (w\cdot v).
\]</span> Since <span class="math inline">\(\lambda\not=\lambda'\)</span>, we must have <span class="math inline">\(v\cdot w=0\)</span>.</p>
<p>For property <span class="math inline">\(4\)</span>, if the eigenvalues are all distinct, this is a consequence of property <span class="math inline">\(2\)</span> – you have <span class="math inline">\(N\)</span> eigenvectors, scaled to length <span class="math inline">\(1\)</span>, for different eigenvalues, and by <span class="math inline">\(2\)</span> they are orthogonal. So the only complication is the case where some eigenvalues are repeated. If <span class="math inline">\(\lambda\)</span> occurs <span class="math inline">\(r\)</span> times, then you have <span class="math inline">\(r\)</span> linearly independent vectors <span class="math inline">\(u_{1},\ldots, u_{r}\)</span> that span the <span class="math inline">\(\lambda\)</span> eigenspace. The Gram-Schmidt process allows you to construct an orthonormal set that spans this eigenspace, and while this orthonormal set isn’t unique, any one of them will do.</p>
<p>For property <span class="math inline">\(5\)</span>, let <span class="math inline">\(e_{i}\)</span> be the column vector that is zero except for a <span class="math inline">\(1\)</span> in position <span class="math inline">\(i\)</span>. The product <span class="math inline">\(e_{j}^{\intercal}De_{i}=d_{ij}\)</span>. Let’s write <span class="math inline">\(e_{i}\)</span> and <span class="math inline">\(e_{j}\)</span> in terms of the orthonormal basis <span class="math inline">\(u_{1},\ldots u_{N}\)</span>: <span class="math display">\[
e_{i} = \sum_{k=1}^{N} (e_{i}\cdot u_{k})u_k \hbox{ and } e_{j} = \sum_{k=1}^{N}(e_{j}\cdot u_{k})u_{k}.
\]</span> Using this expansion, we compute <span class="math inline">\(e_{j}^{\intercal}De_{i}\)</span> in a more complicated way: <span class="math display">\[
e_{j}^{\intercal}De_{i} = \sum_{r=1}^{N}\sum_{s=1}^{N} (e_{j}\cdot u_{r})(e_{i}\cdot u_{s})(u_{r}^{\intercal}Du_{s}).
\]</span> But <span class="math inline">\(u_{r}^{\intercal}Du_{s}=\lambda_{s}(u_{r}\cdot u_{s})=0\)</span> unless <span class="math inline">\(r=s\)</span>, in which case it equals <span class="math inline">\(\lambda_{r}\)</span>, so <span class="math display">\[
e_{j}^{\intercal}De_{i} = \sum_{r=1}^{N} \lambda_{r}(e_{j}\cdot u_{r})(e_{i}\cdot u_{r}).
\]</span> On the other hand, <span class="math display">\[
P^{\intercal}e_{i} = \left[\begin{matrix} (e_{i}\cdot u_{1})\\ (e_{i}\cdot u_{2})\\ \vdots \\(e_{i}\cdot u_{N})\end{matrix}\right]
\]</span> and <span class="math display">\[
\Lambda P^{\intercal}e_{i} = \left[\begin{matrix} \lambda_{1}(e_{i}\cdot u_{i})\\ \lambda_{2}(e_{i}\cdot u_{2})\\ \vdots \\ \lambda_{N}(e_{i}\cdot u_{N})\end{matrix}\right]
\]</span> Therefore the <span class="math inline">\(i,j\)</span> entry of <span class="math inline">\(P\Lambda P^{\intercal}\)</span> is <span class="math display">\[
(e_{j}^{\intercal}P)\Lambda (P^{\intercal}e_{j}) = \sum_{r=1}^{N} \lambda_{r}(e_{i}\cdot u_{r})(e_{j}\cdot u_{r}) = d_{ij}
\]</span> so the two matrices <span class="math inline">\(D\)</span> and <span class="math inline">\(P\Lambda P^{\intercal}\)</span> are in fact equal.</p>
<p><strong>Exercises:</strong></p>
<ol type="1">
<li><p>Prove the rest of the first lemma in <a href="#sec-svd"><span>Section&nbsp;2.4.2</span></a>.</p></li>
<li><p>Prove the Gram-Schmidt Process has the claimed properties in <a href="#sec-gsprocess"><span>Section&nbsp;2.5.1</span></a>.</p></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/01-linear-regression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/03-probability.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Probability and Bayes Theorem</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">© 2021. This work is licensed by Jeremy Teitelbaum under the <a href="http://creativecommons.org/licenses/by-sa/4.0">Creative Commons Attribution-ShareAlike License</a>.</div>
  </div>
</footer>



</body></html>