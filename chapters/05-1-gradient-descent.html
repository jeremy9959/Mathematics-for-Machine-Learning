<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-99.9.9">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Lectures on Machine Learning - 6&nbsp; Gradient Descent</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/05-logistic-regression.html" rel="next">
<link href="../chapters/04-naive-bayes.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
</head><body class="nav-sidebar floating">$$
    \newcommand{\df}[1]{\frac{\partial}{\partial #1}}
    \newcommand{\R}{\mathbf{R}}
$$



  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Gradient Descent</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Lectures on Machine Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-linear-regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-pca.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Principal Component Analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Probability and Bayes Theorem</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-naive-bayes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Naive Bayes classification method</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-1-gradient-descent.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Gradient Descent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-logistic-regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Logistic Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-svm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">6.1</span>  Introduction</a></li>
  <li><a href="#the-key-idea" id="toc-the-key-idea" class="nav-link" data-scroll-target="#the-key-idea"><span class="toc-section-number">6.2</span>  The Key Idea</a></li>
  <li><a href="#the-algorithm" id="toc-the-algorithm" class="nav-link" data-scroll-target="#the-algorithm"><span class="toc-section-number">6.3</span>  The Algorithm</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-gradient_descent" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">6.1</span> Introduction</h2>
<p>A common mathematical theme throughout machine learning is the problem of finding the minimum or maximum value of a function. For example, in linear regression, we find the “best-fitting” linear function by identifying the parameters that minimize the mean squared error. In principal component analysis, we try to identify the scores which have the greatest variation for the given set of data, and for this we needed to maximize a function using Lagrange multipliers. In later lectures, we will see many more examples where we construct the “best” function for a particular task by minimizing some kind of error between our constructed function and the true observed values.</p>
<p>In our discussion of PCA and linear regression, we were able to give analytic formulae for the solution to our problems. These solutions involved (in the case of linear regression) inverting a matrix, and in the case of PCA, finding eigenvalues and eigenvectors. These are elegant mathematical results, but at that time we begged the question of how to actually <em>compute</em> these quantities of interest in an efficient way. In this section, we will discuss the technique known as gradient descent, which is perhaps the simplest approach to minimizing a function using calculus, and which is at the foundation of many practical machine learning algorithms.</p>
</section>
<section id="the-key-idea" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="the-key-idea"><span class="header-section-number">6.2</span> The Key Idea</h2>
<p>Suppose that we have a function <span class="math inline">\(f(x_0,\ldots, x_{k-1})\)</span> and we wish to find its minimum value. In Calculus classes, we are taught to take the derivates of the function and set them equal to zero, but for anything other than the simplest functions this problem is not solvable in practice. In real life, we use iterative methods to find the minimum of the function <span class="math inline">\(f\)</span>.</p>
<p>The main tool in this approach is a fact from multivariate calculus.</p>
<p><strong>Proposition:</strong> Let <span class="math inline">\(f(x_0,\ldots, x_{k-1})\)</span> be a function and let <span class="math inline">\(\nabla f\)</span> be its gradient. Then at each point <span class="math inline">\(x\)</span> in <span class="math inline">\(\R^{k}\)</span>, the gradient <span class="math inline">\((\nabla f)(x)\)</span> is a vector that points in the direction in which <span class="math inline">\(f\)</span> is increasing most rapidly from <span class="math inline">\(x\)</span> and <span class="math inline">\((-\nabla f)(x)\)</span> points in the direction in which <span class="math inline">\(f\)</span> is decreasing most rapidly. If <span class="math inline">\(\nabla f=0\)</span> at <span class="math inline">\(x\)</span> then <span class="math inline">\(x\)</span> is a critical point of <span class="math inline">\(f\)</span>.</p>
<p>This fact arises from thinking about the <em>directional derivative</em> of a function.<br>
The directional derivative <span class="math inline">\(D_{v}f\)</span> measures the rate of change of <span class="math inline">\(f\)</span> as one moves with velocity vector <span class="math inline">\(v\)</span> from the point <span class="math inline">\(x\)</span> and it is defined as <span class="math display">\[
D_{v}f(x) = \frac{d}{dt}f(x+tv)|_{t=0}
\]</span> From the chain rule, we can compute that <span class="math display">\[
D_{v}f(x) = \sum_{i=0}^{k-1} \frac{\partial f}{\partial x_{i}}\frac{dx_{i}}{dt} = (\nabla f)\cdot v
\]</span> where <span class="math display">\[
\nabla f = \left[\frac{\partial f}{\partial x_{i}}\right]_{i=0}^{k-1}
\]</span> is the gradient of <span class="math inline">\(f\)</span>.</p>
<p>The directional derivative <span class="math inline">\(D_{v}(f)=(\nabla f)\cdot v\)</span> measures the rate of change of <span class="math inline">\(f\)</span> if we travel with velocity <span class="math inline">\(v\)</span> from a point <span class="math inline">\(x\)</span>. To remove the dependence on the magnitude of <span class="math inline">\(v\)</span> (since obviously <span class="math inline">\(f\)</span> will change more quickly if we travel more quickly in a given direction), we scale <span class="math inline">\(v\)</span> to be a unit vector. Then, since <span class="math display">\[
\nabla f\cdot v=\|\nabla f\|\|v\|\cos\theta=\|\nabla f\|\cos \theta
\]</span> where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(v\)</span> and <span class="math inline">\(\nabla f\)</span>, the dot product giving the rate is maximized when <span class="math inline">\(v\)</span> is parallel to <span class="math inline">\(\nabla f\)</span>. If <span class="math inline">\(v\)</span> is opposite to <span class="math inline">\(\nabla f\)</span>, the dot product is minimized.</p>
</section>
<section id="the-algorithm" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="the-algorithm"><span class="header-section-number">6.3</span> The Algorithm</h2>
<p>To exploit the fact that the gradient points in the direction of most rapid increase of our function <span class="math inline">\(f\)</span>, we adopt the following strategy. Starting from a point <span class="math inline">\(x\)</span>, compute the gradient <span class="math inline">\(\nabla f\)</span> of <span class="math inline">\(f\)</span>. Take a small step in the direction of the gradient – that should increase the value of <span class="math inline">\(f\)</span>. Then do it again, and again; each time, you move in the direction of increasing <span class="math inline">\(x\)</span>, but at some point the gradient becomes very small and you stop moving much. At that moment, you quit. This is called “gradient ascent.”</p>
<p>If we want to <em>minimize</em>, not maximize, our function, then we want to move <em>opposite</em> to the gradient in small steps. This is the more common formulation.</p>
<div id="alg-gradient_descent" class="theorem algorithm">
<p><span class="theorem-title"><strong>Algorithm 6.1 (Gradient Descent Algorithm) </strong></span>Given a function <span class="math inline">\(f:\mathbb{R}^{k}\to \mathbb{R}\)</span>, to find a point where it is mimized, choose:</p>
<ul>
<li>a starting point <span class="math inline">\(c^{(0)}\)</span>,</li>
<li>a small constant <span class="math inline">\(\nu\)</span> (called the <em>learning rate</em>)</li>
<li>and a small constant <span class="math inline">\(\epsilon\)</span> (the <em>tolerance</em>).</li>
</ul>
<p>Iteratively compute <span class="math display">\[
c^{(n+1)}=c^{(n)} -\nu\nabla f(c^{(n)})
\]</span> until <span class="math inline">\(|c^{(n+1)}-c^{(n)}|&lt;\epsilon\)</span>.</p>
<p>Then <span class="math inline">\(c^{(n+1)}\)</span> is an (approximate) critical point of <span class="math inline">\(f\)</span>.</p>
</div>
<div id="fig-graddescentillust" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/gradient_descent.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.1: Gradient Descent Illustrated</figcaption><p></p>
</figure>
</div>
<p>The behavior of gradient descent, at least when all goes well, is illustrated in <a href="#fig-graddescentillust">Figure&nbsp;<span>6.1</span></a> for the function <span class="math display">\[
f(x,y) = 1.3e^{-2.5((x-1.3)^2+(y-0.8)^2))}-1.2e^{-2((x-1.8)^2)+(y-1.3)^2)}.
\]</span> <a href="#fig-graddescentillust">Figure&nbsp;<span>6.1</span></a> is a contour plot, with the black lines at constant height and the colors indicating the height of the function. This function has two “pits” or “wells” indicated by the darker, “cooler” colored regions. The red line shows the path that the gradient descent algorithm takes, from a higher, “hotter” region to a lower cooler one.</p>
<p>To get a little more perspective on gradient descent, consider the one-dimensional case, with <span class="math display">\[
f(x)=3x^4+4x^3-12x^2+5.
\]</span> This is a quartic polynomial whose graph has two local minima and a local maximum, depicted in <a href="#fig-graddescentquartic">Figure&nbsp;<span>6.2</span></a>.</p>
<div id="fig-graddescentquartic" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/GradDescentQuartic.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.2: A quartic polynomial</figcaption><p></p>
</figure>
</div>
<p>In this case the gradient is just the derivative <span class="math display">\[
f'(x)=12x^3+12x^2-24x
\]</span> and the iteration is <span class="math display">\[
c^{(n+1)} = c^{(n)}-12\nu((c^{(n)})^3+(c^{(n)})^2-2c^{(n)}).
\]</span></p>
<p>From this simple example we can see the power and also the pitfalls of this method. Suppose we choose <span class="math inline">\(x_0=.5\)</span>, <span class="math inline">\(\nu=.01\)</span>, and do <span class="math inline">\(30\)</span> iterations of the main loop in our algorithm. The result is shown in <a href="#fig-grad_descent_local_minimum">Figure&nbsp;<span>6.3</span></a> .</p>
<div id="fig-grad_descent_local_minimum" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/grad_descent_local_minimum.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.3: Gradient descent to a local minimum</figcaption><p></p>
</figure>
</div>
<p>As we hope, the red dots quickly descend to the bottom of the “valley” at the point <span class="math inline">\(x=1\)</span>. However, this valley is only a <em>local minimum</em> of the function; the true minimum is at <span class="math inline">\(x=-2\)</span>. Gradient descent can’t see that far away point and so we don’t find the true minimum of the function. One way to handle this is to <em>run gradient descent multiple times with random starting coordinates</em> and then look for the minimum value it finds among all of these tries.</p>
<p>Gradient descent can fail more spectacularly if we choose an unfortunate combination of learning rate and starting point.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/04-naive-bayes.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Naive Bayes classification method</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/05-logistic-regression.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">© 2021. This work is licensed by Jeremy Teitelbaum under the <a href="http://creativecommons.org/licenses/by-sa/4.0">Creative Commons Attribution-ShareAlike License</a>.</div>
  </div>
</footer>



</body></html>