[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lectures on Machine Learning",
    "section": "",
    "text": "Preface\nThese notes are being developed for the UConn Math Department’s undergraduate course on the Mathematics of Machine Learning, Math 3180.\nThis is a (very) rough draft.\nYou can view this project on GitHub. Please let the author know if you have questions or find mistakes by opening an issue there."
  },
  {
    "objectID": "chapters/01-linear-regression.html#sec-Intro",
    "href": "chapters/01-linear-regression.html#sec-Intro",
    "title": "1  Linear Regression",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nSuppose that we are trying to study two quantities \\(x\\) and \\(y\\) that we suspect are related – at least approximately – by a linear equation \\(y=ax+b\\). Sometimes this linear relationship is predicted by theoretical considerations, and sometimes it is just an empirical hypothesis.\nFor example, if we are trying to determine the velocity of an object travelling towards us at constant speed, and we measure measure the distances \\(d_1, d_2, \\ldots, d_n\\) between us and the object at a series of times \\(t_1, t_2, \\ldots, t_n\\), then since “distance equals rate times time” we have a theoretical foundation for the assumption that \\(d=rt+b\\) for some constants \\(r\\) and \\(b\\). On the other hand, because of unavoidable experimental errors, we can’t expect that this relationship will hold exactly for the observed data; instead, we likely get a graph like that shown in Figure 1.1. We’ve drawn a line on the plot that seems to capture the true slope (and hence velocity) of the object.\n\n\n\nFigure 1.1: Physics Experiment\n\n\nOn the other hand, we might look at a graph such as Figure 1.2, which plots the gas mileage of various car models against their engine size (displacement), and observe a general trend in which bigger engines get lower mileage. In this situation we could ask for the best line of the form \\(y=mx+b\\) that captures this relationship and use that to make general conclusions without necessarily having an underlying theory.\n\n\n\nFigure 1.2: MPG vs Displacement ( [1] )"
  },
  {
    "objectID": "chapters/01-linear-regression.html#sec-Calculus",
    "href": "chapters/01-linear-regression.html#sec-Calculus",
    "title": "1  Linear Regression",
    "section": "1.2 Least Squares (via Calculus)",
    "text": "1.2 Least Squares (via Calculus)\nIn either of the two cases above, the question we face is to determine the line \\(y=mx+b\\) that “best fits” the data \\(\\{(x_i,y_i)_{i=1}^{N}\\}\\). The classic approach is to determine the equation of a line \\(y=mx+b\\) that minimizes the “mean squared error”:\n\\[ MSE(m,b) = \\frac{1}{N}\\sum_{i=1}^{N} (y_i-mx_i-b)^2 \\]\nIt’s worth emphasizing that the \\(MSE\\) is a function of two variables – the slope \\(m\\) and the intercept \\(b\\) – and that the data points \\(\\{(x_i,y_i)\\}\\) are constants for these purposes. Furthermore, it’s a quadratic function in those two variables. Since our goal is to find \\(m\\) and \\(b\\) that minimize the \\(MSE\\), we have a Calculus problem that we can solve by taking partial derivatives and setting them to zero.\nTo simplify the notation, let’s abbreviate \\(MSE\\) by \\(E\\).\n\\[\n\\begin{aligned} \\frac{\\partial E}{\\partial m} &=\n\\frac{1}{N}\\sum_{1}^{N}-2x_i(y_i-mx_i-b) \\\\ \\frac{\\partial E}{\\partial\nb} &= \\frac{1}{N}\\sum_{1}^{N}-2(y_i-mx_i-b) \\\\\n\\end{aligned}\n\\]\nWe set these two partial derivatives to zero, so we can drop the \\(-2\\) and regroup the sums to obtain two equations in two unknowns (we keep the \\(\\frac{1}{N}\\) because it is illuminating in the final result):\n\\[\n\\begin{aligned} \\frac{1}{N}(\\sum_{i=1}^{N} x_i^2)m &+&\n\\frac{1}{N}(\\sum_{i=1}^{N} x_i)b &=& \\frac{1}{N}\\sum_{i=1}^{N} x_i y_i\n\\\\ \\frac{1}{N}(\\sum_{i=1}^{N} x_i)m &+& b &=&\n\\frac{1}{N}\\sum_{i=1}^{N} y_{i} \\\\ \\end{aligned}\n\\tag{1.1}\\]\nIn these equations, notice that \\(\\frac{1}{N}\\sum_{i=1}^{N} x_i\\) is the average (or mean) value of the \\(x_i\\). Let’s call this \\(\\overline{x}\\). Similarly, \\(\\frac{1}{N}\\sum_{i=1}^{N} y_{i}\\) is the mean of the \\(y_i\\), and we’ll call it \\(\\overline{y}\\). If we further simplify the notation and write \\(S_{xx}\\) for \\(\\frac{1}{N}\\sum_{i=1}^{N} x_i^2\\) and \\(S_{xy}\\) for \\(\\frac{1}{N}\\sum_{i=1}^{N}x_iy_i\\) then we can write down a solution to this system using Cramer’s rule:\n\\[ \\begin{aligned} m &=\n\\frac{S_{xy}-\\overline{x}\\overline{y}}{S_{xx}-\\overline{x}^2} \\\\ b &=\n\\frac{S_{xx}\\overline{y}-S_{xy}\\overline{x}}{S_{xx}-\\overline{x}^2} \\\\\n\\end{aligned}\n\\tag{1.2}\\]\nwhere we must have \\(S_{xx}-\\overline{x}^2\\not=0\\).\n\n1.2.1 Exercises\n\nVerify that Equation 1.2 is in fact the solution to the system in Equation 1.1 .\nSuppose that \\(S_{xx}-\\overline{x}^2=0\\). What does that mean about the \\(x_i\\)? Does it make sense that the problem of finding the “line of best fit” fails in this case?"
  },
  {
    "objectID": "chapters/01-linear-regression.html#sec-LinAlg",
    "href": "chapters/01-linear-regression.html#sec-LinAlg",
    "title": "1  Linear Regression",
    "section": "1.3 Least Squares (via Geometry)",
    "text": "1.3 Least Squares (via Geometry)\nIn our discussion above, we thought about our data as consisting of \\(N\\) pairs \\((x_i,y_i)\\) corresponding to \\(N\\) points in the \\(xy\\)-plane \\(\\mathbf{R}^2\\). Now let’s turn that picture “on its side”, and instead think of our data as consisting of two points in \\(\\mathbf{R}^{N}\\):\n\\[ X=\\left[\\begin{matrix} x_1\\cr x_2\\cr \\vdots\\cr\nx_n\\end{matrix}\\right] \\mathrm{\\ and\\ } Y = \\left[\\begin{matrix}\ny_1\\cr y_2\\cr \\vdots\\cr y_n\\end{matrix}\\right]\n\\]\nLet’s also introduce one other vector\n\\[ E = \\left[\\begin{matrix} 1 \\cr 1 \\cr \\vdots \\cr\n1\\end{matrix}\\right].  \n\\]\nFirst, let’s assume that \\(E\\) and \\(X\\) are linearly independent. If not, then \\(X\\) is a constant vector (why?) which we already know is a problem from Section 1.2, Exercise 2. Therefore \\(E\\) and \\(X\\) span a plane in \\(\\mathbf{R}^{N}\\).\n\n\n\nFigure 1.3: Distance to A Plane\n\n\nNow if our data points \\((x_i,y_i)\\) all did lie on a line \\(y=mx+b\\), then the three vectors \\(X\\), \\(Y\\), and \\(E\\) would be linearly dependent:\n\\[ Y = mX + bE.  \\]\nSince our data is only approximately linear, that’s not the case. So instead we look for an approximate solution. One way to phrase that is to ask:\nWhat is the point \\(\\hat{Y}\\) in the plane \\(H\\) spanned by \\(X\\) and \\(E\\) in \\(\\mathbf{R}^{N}\\) which is closest to \\(Y\\)?\nIf we knew this point \\(\\hat{Y}\\), then since it lies in \\(H\\) we would have \\(\\hat{Y}=mX+bE\\) and the coefficients \\(m\\) and \\(b\\) would be a candidate for defining a line of best fit \\(y=mx+b\\). Finding the point in a plane closest to another point in \\(\\mathbf{R}^{N}\\) is a geometry problem that we can solve.\nProposition: The point \\(\\hat{Y}\\) in the plane spanned by \\(X\\) and \\(E\\) is the point such that the vector \\(Y-\\hat{Y}\\) is perpendicular to \\(H\\).\nProof: See Figure 1.3 for an illustration – perhaps you are already convinced by this, but let’s be careful. \\(\\hat{Y}=mX+bE\\) such that \\[ D = \\|Y-\\hat{Y}\\|^2 = \\|Y-mX-bE\\|^2 \\] is minimal. Using some vector calculus, we have \\[ \\frac{\\partial D}{\\partial m} =\n\\frac{\\partial}{\\partial m} (Y-mX-bE)\\cdot (Y-mX-bE) =\n-2(Y-mX-bE)\\cdot X \\] and \\[ \\frac{\\partial D}{\\partial b} =\n\\frac{\\partial}{\\partial b} (Y-mX-bE)\\cdot (Y-mX-bE) =\n-2(Y-mX-bE)\\cdot E.  \\]\nSo both derivatives are zero exactly when \\(\\hat{Y}=(Y-mX-bE)\\) is orthogonal to both \\(X\\) and \\(E\\), and therefore every vector in \\(H\\).\nWe also obtain equations for \\(m\\) and \\(b\\) just as in our first look at this problem.\n\\[ \\begin{aligned} m(X\\cdot E) &+ b(E\\cdot E) &= (Y\\cdot E) \\cr\nm(X\\cdot X) &+ b(E\\cdot X) &= (Y\\cdot X) \\cr \\end{aligned}\n\\tag{1.3}\\]\nWe leave it is an exercise below to check that these are the same equations that we obtained in Equation 1.2.\n\n1.3.1 Exercises\n\nVerify that Equation 1.2 and Equation 1.3 are equivalent."
  },
  {
    "objectID": "chapters/01-linear-regression.html#sec-Multivariate-calculus",
    "href": "chapters/01-linear-regression.html#sec-Multivariate-calculus",
    "title": "1  Linear Regression",
    "section": "1.4 The Multivariate Case (Calculus)",
    "text": "1.4 The Multivariate Case (Calculus)\nHaving worked through the problem of finding a “line of best fit” from two points of view, let’s look at a more general problem. We looked above at a scatterplot showing the relationship between gas mileage and engine size (displacement). There are other factors that might contribute to gas mileage that we want to consider as well – for example:\n\na car that is heavy compared to its engine size may get worse mileage\na sports car with a drive train that gives fast acceleration as compared to a car with a transmission designed for long trips may have different mileage for the same engine size.\n\nSuppose we wish to use engine displacement, vehicle weight, and acceleration all together to predict mileage. Instead of looking points \\((x_i,y_i)\\) where \\(x_i\\) is the displacement of the \\(i^{th}\\) car model and we try to predict a value \\(y\\) from a corresponding \\(x\\) as \\(y=mx+b\\) – let’s look at a situation in which our measured value \\(y\\) depends on multiple variables – say displacement \\(d\\), weight \\(w\\), and acceleration \\(a\\) with \\(k=3\\) – and we are trying to find the best linear equation\n\\[\ny=m_1 d + m_2 w + m_3 a +b\n\\tag{1.4}\\]\nBut to handle this situation more generally we need to adopt a convention that will allow us to use indexed variables instead of \\(d\\), \\(w\\), and \\(a\\). We will use the tidy data convention.\nTidy Data: A dataset is tidy if it consists of values \\(x_{ij}\\) for \\(i=1,\\ldots,N\\) and \\(j=1,\\ldots, k\\) so that:\n\nthe row index corresponds to a sample – a set of measurements from a single event or item;\nthe column index corresponds to a feature – a particular property measured for all of the events or items.\n\nIn our case,\n\nthe samples are the different types of car models,\nthe features are the properties of those car models.\n\nFor us, \\(N\\) is the number of different types of cars, and \\(k\\) is the number of properties we are considering. Since we are looking at displacement, weight, and acceleration, we have \\(k=3\\).\nSo the “independent variables” for a set of data that consists of \\(N\\) samples, and \\(k\\) measurements for each sample, can be represented by a \\(N\\times k\\) matrix\n\\[ X = \\left(\\begin{matrix} x_{11} & x_{12} & \\cdots & x_{1k} \\\\\nx_{21} & x_{22} & \\cdots & x_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\n\\\\ x_{N1} & x_{k2} & \\cdots & x_{Nk} \\\\ \\end{matrix}\\right)\n\\]\nand the measured dependent variables \\(Y\\) are a column vector \\[ Y =\n\\left[\\begin{matrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\\end{matrix}\\right].\n\\]\nIf \\(m_1,\\ldots, m_k\\) are “slopes” associated with these properties in Equation 1.4, and \\(b\\) is the “intercept”, then the predicted value \\(\\hat{Y}\\) is given by a matrix equation\n\\[\n\\hat{Y} = X\\left[\\begin{matrix} m_1 \\\\ m_2 \\\\ \\cdots \\\\\nm_k\\end{matrix}\\right]+\\left[\\begin{matrix} 1 \\\\ 1 \\\\ \\cdots \\\\\n1\\end{matrix}\\right]b\n\\]\nand our goal is to choose these parameters \\(m_i\\) and \\(b\\) to make the mean squared error:\n\\[ MSE(m_1,\\ldots, m_k,b) = \\|Y-\\hat{Y}\\|^2 = \\sum_{i=1}^{N} (y_i -\n\\sum_{j=1}^{k} x_{ij}m_j -b )^2.\n\\]\nHere we are summing over the \\(N\\) different car models, and for each model taking the squared difference between the true mileage \\(y_i\\) and the “predicted” mileage \\(\\sum_{j=1}^{k} x_{ij}m_j +b\\). We wish to minimize this MSE.\nLet’s make one more simplification. The intercept variable \\(b\\) is annoying because it requires separate treatment from the \\(m_i\\). But we can use a trick to eliminate the need for special treatment. Let’s add a new feature to our data matrix (a new column) that has the constant value \\(1\\).\n\\[ X = \\left(\\begin{matrix} x_{11} & x_{12} & \\cdots & x_{1k} & 1\\\\\nx_{21} & x_{22} & \\cdots & x_{2k} & 1\\\\ \\vdots & \\vdots & \\ddots &\n\\vdots & 1\\\\ x_{N1} & x_{k2} & \\cdots & x_{Nk} & 1\\\\\n\\end{matrix}\\right)\n\\]\nNow our data matrix \\(X\\) is \\(N\\times(k+1)\\) and we can put our “intercept” \\(b=m_{k+1}\\) into our vector of “slopes” \\(m_1, \\ldots, m_k,m_{k+1}\\):\n\\[ \\hat{Y} = X\\left[\\begin{matrix} m_1 \\\\ m_2 \\\\ \\cdots \\\\ m_k \\\\\nm_{k+1}\\end{matrix}\\right]\n\\]\nand our MSE becomes\n\\[\nMSE(M) = \\|Y - XM\\|^2\n\\]\nwhere\n\\[ M=\\left[\\begin{matrix} m_1 \\\\ m_2 \\\\ \\cdots \\\\ m_k \\\\\nm_{k+1}\\end{matrix}\\right].\n\\]\nRemark: Later on (see {Section 1.6}) we will see that if we “center” our features about their mean, by subtracting the average value of each column of \\(X\\) from that column; and we also subtract the average value of \\(Y\\) from the entries of \\(Y\\), then the \\(b\\) that emerges from the least squares fit is zero. As a result, instead of adding a column of \\(1\\)’s, you can change coordinates to center each feature about its mean, and keep your \\(X\\) matrix \\(N\\times k\\).\nThe Calculus approach to minimizing the \\(MSE\\) is to take its partial derivatives with respect to the \\(m_{i}\\) and set them to zero. Let’s first work out the derivatives in a nice form for later.\nProposition: The gradient of \\(MSE(M)=E\\) is given by\n\\[\n\\nabla E = \\left[\\begin{matrix} \\df{M_1}E \\\\ \\df{M_2}E \\\\ \\vdots \\\\\n\\df{m_{M+1}}E\\end{matrix}\\right] = -2 X^{\\intercal}Y + 2\nX^{\\intercal}XM\n\\tag{1.5}\\]\nwhere \\(X^{\\intercal}\\) is the transpose of \\(X\\).\nProof: First, remember that the \\(ij\\) entry of \\(X^{\\intercal}\\) is the \\(ji\\) entry of \\(X\\). Also, we will use the notation \\(X[j,:]\\) to mean the \\(j^{th}\\) row of \\(X\\) and \\(X[:,i]\\) to mean the \\(i^{th}\\) column of \\(X\\). (This is copied from the Python programming language; the ‘:’ means that index runs over all possibilities).\nSince \\[ E = \\sum_{j=1}^{N} (Y_j-\\sum_{s=1}^{k+1} X_{js}M_{s})^2 \\] we compute: \\[\\begin{aligned} \\df{M_t}E &= -2\\sum_{j=1}^{N}\nX_{jt}(Y_{j}-\\sum_{s=1}^{k+1} X_{js}M_{s}) \\\\ &= -2(\\sum_{j=1}^{N}\nY_{j}X_{jt} - \\sum_{j=1}^{N}\\sum_{s=1}^{k+1} X_{jt}X_{js}M_{s}) \\\\ &=\n-2(\\sum_{j=1}^{N} X^{\\intercal}_{tj}Y_{j}\n-\\sum_{j=1}^{N}\\sum_{s=1}^{k+1} X^{\\intercal}_{tj}X_{js}M_{s}) \\\\ &=\n-2(X^{\\intercal}[t,:]Y - \\sum_{s=1}^{k+1}\\sum_{j=1}^{N}\nX^{\\intercal}_{tj}X_{js}M_{s}) \\\\ &= -2(X^{\\intercal}[t,:]Y -\n\\sum_{s=1}^{k+1} (X^{\\intercal}X)_{ts}M_{s}) \\\\ &=\n-2[X^{\\intercal}[t,:]Y - (X^{\\intercal}X](t,:)M)\\\\\n\\end{aligned} \\tag{1.6}\\]\nStacking up the different rows to make \\(E\\) yields the desired formula.\nProposition: Assume that \\(D=X^{\\intercal}X\\) is invertible (notice that it is a \\((k+1)\\times(k+1)\\) square matrix so this makes sense). The solution \\(M\\) to the multivariate least squares problem is \\[ M =\nD^{-1}X^{\\intercal}Y  \\tag{1.7}\\] and the “predicted value” \\(\\hat{Y}\\) for \\(Y\\) is \\[\n\\hat{Y} = XD^{-1}X^{\\intercal}Y.\n\\tag{1.8}\\]"
  },
  {
    "objectID": "chapters/01-linear-regression.html#the-multivariate-case-geometry",
    "href": "chapters/01-linear-regression.html#the-multivariate-case-geometry",
    "title": "1  Linear Regression",
    "section": "1.5 The Multivariate Case (Geometry)",
    "text": "1.5 The Multivariate Case (Geometry)\nLet’s look more closely at the equation obtained by setting the gradient of the error, Equation 1.5, to zero. Remember that \\(M\\) is the unknown vector in this equation, everything else is known:\n\\[ X^{\\intercal}Y = X^{\\intercal}XM \\]\nHere is how to think about this:\n\nAs \\(M\\) varies, the \\(N\\times 1\\) matrix \\(XM\\) varies over the space spanned by the columns of the matrix \\(X\\). So as \\(M\\) varies \\(XM\\) is a general element of the subspace \\(H\\) of \\(R^{N}\\) spanned by the \\(k+1\\) columns of \\(X\\).\nThe product \\(X^{\\intercal}XM\\) is a \\((k+1)\\times 1\\) matrix. Each entry is the dot product of the general element of \\(H\\) with one of the \\(k+1\\) basis vectors of \\(H\\).\nThe product \\(X^{\\intercal}Y\\) is a \\((k+1)\\times 1\\) matrix whose entries are the dot product of the basis vectors of \\(H\\) with \\(Y\\).\n\nTherefore, this equation asks for us to find \\(M\\) so that the vector \\(XM\\) in \\(H\\) has the same dot products with the basis vectors of \\(H\\) as \\(Y\\) does. The condition\n\\[ X^{\\intercal}\\cdot (Y-XM)=0 \\]\nsays that \\(Y-XM\\) is orthogonal to \\(H\\). This argument establishes the following proposition.\nProposition: Just as in the simple one-dimensional case, the predicted value \\(\\hat{Y}\\) of the least squares problem is the point in \\(H\\) closest to \\(Y\\) – or in other words the point \\(\\hat{Y}\\) in \\(H\\) such that \\(Y-\\hat{Y}\\) is perpendicular to \\(H\\).\n\n1.5.1 Orthogonal Projection\nRecall that we introduced the notation \\(D=X^{\\intercal}X\\), and let’s assume, for now, that \\(D\\) is an invertible matrix. We have the formula (see Equation 1.8): \\[ \\hat{Y} = XD^{-1}X^{\\intercal}Y.  \\] Proposition: The matrix \\(P=XD^{-1}X^{\\intercal}\\) is an \\(N\\times N\\) matrix called the orthogonal projection operator onto the subspace \\(H\\) spanned by the columns of \\(X\\). It has the following properties:\n\n\\(PY\\) belongs to the subspace \\(H\\) for any \\(Y\\in\\mathbf{R}^{N}\\).\n\\((Y-PY)\\) is orthogonal to \\(H\\).\n\\(P*P = P\\).\n\nProof: First of all, \\(PY=XD^{-1}X^{\\intercal}Y\\) so \\(PY\\) is a linear combination of the columns of \\(X\\) and is therefore an element of \\(H\\). Next, we can compute the dot product of \\(PY\\) against a basis of \\(H\\) by computing\n\\[ X^{\\intercal}PY = X^{\\intercal}XD^{-1}X^{\\intercal}Y =\nX^{\\intercal}Y\n\\]\nsince \\(X^{\\intercal}X=D\\). This equation means that \\(X^{\\intercal}(Y-PY)=0\\) which tells us that \\(Y-PY\\) has dot product zero with a basis for \\(H\\). Finally,\n\\[ PP = XD^{-1}X^{\\intercal}XD^{-1}X^{\\intercal} =\nXD^{-1}X^{\\intercal}=P.  \n\\]\nIt should be clear from the above discussion that the matrix \\(D=X^{\\intercal}X\\) plays an important role in the study of this problem. In particular it must be invertible or our analysis above breaks down. In the next section we will look more closely at this matrix and what information it encodes about our data."
  },
  {
    "objectID": "chapters/01-linear-regression.html#sec-centered",
    "href": "chapters/01-linear-regression.html#sec-centered",
    "title": "1  Linear Regression",
    "section": "1.6 Centered coordinates",
    "text": "1.6 Centered coordinates\nRecall from last section that the matrix \\(D=X^{\\intercal}X\\) is of central importance to the study of the multivariate least squares problem. Let’s look at it more closely.\nLemma: The \\(i,j\\) entry of \\(D\\) is the dot product \\[\nD_{ij}=X[:,i]\\cdot X[:,j] \\] of the \\(i^{th}\\) and \\(j^{th}\\) columns of \\(X\\).\nProof: In the matrix multiplication \\(X^{\\intercal}X\\), the \\(i^{th}\\) row of \\(X^{\\intercal}\\) gets “dotted” with the \\(j^{th}\\) column of \\(X\\) to product the \\(i,j\\) entry. But the \\(i^{th}\\) row of \\(X^{\\intercal}\\) is the \\(i^{th}\\) column of \\(X\\), as asserted in the statement of the lemma.\nA crucial point in our construction above relied on the matrix \\(D\\) being invertible. The following Lemma shows that \\(D\\) fails to be invertible only when the different features (the columns of \\(X\\)) are linearly dependent.\nLemma: \\(D\\) is not invertible if and only if the columns of \\(X\\) are linearly dependent.\nProof: If the columns of \\(X\\) are linearly dependent, then there is a nonzero vector \\(m\\) so that \\(Xm=0\\). In that case clearly \\(Dm=X^{\\intercal}Xm=0\\) so \\(D\\) is not invertible. Suppose \\(D\\) is not invertible. Then there is a nonzero vector \\(m\\) with \\(Dm=X^{\\intercal}Xm=0\\). This means that the vector \\(Xm\\) is orthogonal to all of the columns of \\(X\\). Since \\(Xm\\) belongs to the span \\(H\\) of the columns of \\(X\\), if it is orthogonal to \\(H\\) it must be zero.\nIn fact, the matrix \\(D\\) captures some important statistical measures of our data, but to see this clearly we need to make a slight change of basis. First recall that \\(X[:,k+1]\\) is our column of all \\(1\\), added to handle the intercept. As a result, the dot product \\(X[:,i]\\cdot X[:,k+1]\\) is the sum of the entries in the \\(i^{th}\\) column, and so if we let \\(\\mu_{i}\\) denote the average value of the entries in column \\(i\\), we have \\[ \\mu_{i} = \\frac{1}{N}(X[:,i]\\cdot\nX[:,k+1]) \\]\nNow change the matrix \\(X\\) by elementary column operations to obtain a new data matrix \\(X_{0}\\) by setting \\[ X_{0}[:,i] =\nX[:,i]-\\frac{1}{N}(X[:,i]\\cdot X[:,k+1])X[:,k+1] =\nX[:,i]-\\mu_{i}X[:,k+1] \\] for \\(i=1,\\ldots, k\\).\nIn terms of the original data, we are changing the measurement scale of the data so that each feature has average value zero, and the subspace \\(H\\) spanned by the columns of \\(X_{0}\\) is the same as that spanned by the columns of \\(X\\). Using \\(X_{0}\\) instead of \\(X\\) for our least squares problem, we get\n\\[ \\hat{Y} = X_{0}D_{0}^{-1}X_{0}^{\\intercal}Y \\]\nand\n\\[ M_{0} = D_{0}^{-1}X_{0}^{\\intercal}Y \\]\nwhere \\(D_{0}=X_{0}^{\\intercal}X_{0}.\\)\nProposition: The matrix \\(D_{0}\\) has a block form. Its upper left block is a \\(k\\times k\\) symmetric block with entries \\[ (D_{0})_{ij} =\n(X[:,i]-\\mu_{i}X[:,k+1])\\cdot(X[:,j]-\\mu_{j}X[:,k+1]) \\] Its \\((k+1)^{st}\\) row and column are all zero, except for the \\((k+1),(k+1)\\) entry, which is \\(N\\).\nProof: This follows from the fact that the last row and column entries are (for \\(i\\not=k+1\\)): \\[ (X[:,i]-\\mu_{i}X[:,k+1])\\cdot\nX[:,k+1] = (X[:,i]\\cdot X[:,k+1])-N\\mu_{i} = 0 \\] and for \\(i=k+1\\) we have \\(X[:,k+1]\\cdot X[:,k+1]=N\\) since that column is just \\(N\\) \\(1\\)’s.\nProposition: If the \\(x\\) coordinates (the features) are centered so that they have mean zero, then the intercept \\(b\\) is \\[ \\overline{Y} =\n\\frac{1}{N}\\sum y_{i}.  \\]\nProof: By centering the coordinates, we replace the matrix \\(X\\) by \\(X_{0}\\) and \\(D\\) by \\(D_{0}\\). and we are trying to minimize \\(\\|Y-X_{0}M_{0}\\|^2\\). Use the formula from Equation 1.7 to see that \\[ M_{0} = D_{0}^{-1}X_{0}^{\\intercal}Y.  \\] The \\(b\\) value we are interested in is the last entry \\(m_{k+1}\\) in \\(M_{0}\\). From the block form of \\(D_{0}\\), we know that \\(D_{0}^{-1}\\) has bottom row and last column zero except for \\(1/N\\) in position \\((k+1)\\times(k+1)\\). Also \\(X_{0}^{\\intercal}\\) has last row consisting entirely of \\(1\\). So the bottom entry of \\(X_{0}^{\\intercal}Y\\) is \\(\\sum_{i=1}^{N} y_{i}\\), and the bottom entry \\(b\\) of \\(D_{0}^{-1}X_{0}^{\\intercal}Y\\) is \\[ \\mu_{Y} =\n\\frac{1}{N}\\sum_{i=1}^{N} y_{i}.  \\] as claimed.\nCorollary: If we make a further change of coordinates to define \\[\nY_{0} = Y - \\mu_{Y}\\left[\\begin{matrix} 1 \\\\ 1 \\\\ \\vdots \\\\\n1\\end{matrix}\\right] \\] then the associated \\(b\\) is zero. As a result we can forget about the extra column of \\(1's\\) that we added to \\(X\\) to account for it and reduce the dimension of our entire problem by \\(1\\).\nJust to recap, if we center our data so that \\(\\mu_{Y}=0\\) and \\(\\mu_{i}=0\\) for \\(i=1,\\ldots, k\\), then the least squares problem reduces to minimizing \\[ E(M) = \\|Y-XM\\|^2 \\] where \\(X\\) is the \\(N\\times k\\) matrix with \\(j^{th}\\) row \\((x_{j1},x_{j2},\\ldots, x_{jk})\\) for \\(j=1,\\ldots, N\\) and the solutions are as given in Equation 1.7 and Equation 1.8."
  },
  {
    "objectID": "chapters/01-linear-regression.html#caveats-about-linear-regression",
    "href": "chapters/01-linear-regression.html#caveats-about-linear-regression",
    "title": "1  Linear Regression",
    "section": "1.7 Caveats about Linear Regression",
    "text": "1.7 Caveats about Linear Regression\n\n1.7.1 Basic considerations\nReflecting on our long discussion up to this point, we should take note of some of the potential pitfalls that lurk in the use of linear regression.\n\nWhen we apply linear regression, we are explicitly assuming that the variable \\(Y\\) is associated to \\(X\\) via linear equations. This is a big assumption!\nWhen we use multilinear regression, we are assuming that changes in the different features have independent effects on the target variable \\(y\\). In other words, suppose that \\(y=ax_1+bx_2\\). Then an increase of \\(x_1\\) by \\(1\\) increases \\(y\\) by \\(a\\), and an increase of \\(x_2\\) by \\(1\\) increases \\(y\\) by \\(b\\). These effects are independent of one another and combine to yield an increase of \\(a+b\\).\nWe showed in our discussion above that linear regression problem has a solution when the matrix \\(D=X^{\\intercal}X\\) is invertible, and this happens when the columns of \\(D\\) are linearly independent. When working with real data, which is messy, we could have a situation in which the features we are studying are, in fact, dependent – but because of measurement error, the samples that we collected aren’t. In this case, the matrix \\(D\\) will be “close” to being non-invertible, although formally still invertible. In this case, computing \\(D^{-1}\\) leads to numerical instability and the solution we obtain is very unreliable.\n\n\n\n1.7.2 Simpson’s Effect\nSimpson’s effect is a famous phenomenon that illustrates that linear regression can be very misleading in some circumstances. It is often a product of “pooling” results from multiple experiments. Suppose, for example, that we are studying the relationship between a certain measure of blood chemistry and an individual’s weight gain or less on a particular diet. We do our experiments in three labs, the blue, green, and red labs. Each lab obtains similar results – higher levels of the blood marker correspond to greater weight gain, with a regression line of slope around 1. However, because of differences in the population that each lab is studying, some populations are more susceptible to weight gain and so the red lab sees a mean increase of almost 9 lbs while the blue lab sees a weight gain of only 3 lbs on average.\nThe three groups of scientists pool their results to get a larger sample size and do a new regression. Surprise! Now the regression line has slope \\(-1.6\\) and increasing amounts of the marker seem to lead to less weight gain!\nThis is called Simpson’s effect, or Simpson’s paradox, and it shows that unknown factors (confounding factors) may cause linear regression to yield misleading results. This is particularly true when data from experiments conducted under different conditions is combined; in this case, the differences in experimental setting, called batch effects, can throw off the analysis very dramatically. See Figure 1.4 .\n\n\n\nFigure 1.4: Simpson’s Effect\n\n\n\n\n1.7.3 Exercises\n\nWhen proving that \\(D\\) is invertible if and only if the columns of \\(X\\) are linearly independent, we argued that if \\(X^{\\intercal}Xm=0\\) for a nonzero vector \\(m\\), then \\(Xm\\) is orthogonal to the span of the columns of \\(X\\), and is also an element of that span, and is therefore zero. Provide the details: show that if \\(H\\) is a subspace of \\(\\mathbf{R}^{N}\\), and \\(x\\) is a vector in \\(H\\) such that \\(x\\cdot h=0\\) for all \\(h\\in H\\), then \\(x=0\\).\n\n\n\n\n\n[1] U.C. Irvine ML Repository. Auto MPG Dataset.Available at http://https://archive.ics.uci.edu/ml/datasets/Auto+MPG."
  },
  {
    "objectID": "chapters/02-pca.html#introduction",
    "href": "chapters/02-pca.html#introduction",
    "title": "2  Principal Component Analysis",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nSuppose that, as usual, we begin with a collection of measurements of different features for a group of samples. Some of these measurements will tell us quite a bit about the dif mergefference among our samples, while others may contain relatively little information. For example, if we are analyzing the effect of a certain weight loss regimen on a group of people, the age and weight of the subjects may have a great deal of influence on how successful the regimen is, while their blood pressure might not. One way to help identify which features are more significant is to ask whether or not the feature varies a lot among the different samples. If nearly all the measurements of a feature are the same, it can’t have much power in distinguishing the samples, while if the measurements vary a great deal then that feature has a chance to contain useful information.\nIn this section we will discuss a way to measure the variability of measurements and then introduce principal component analysis (PCA). PCA is a method for finding which linear combinations of measurements have the greatest variability and therefore might contain the most information. It also allows us to identify combinations of measurements that don’t vary much at all. Combining this information, we can sometimes replace our original system of features with a smaller set that still captures most of the interesting information in our data, and thereby find hidden characteristics of the data and simplify our analysis a great deal."
  },
  {
    "objectID": "chapters/02-pca.html#variance-and-covariance",
    "href": "chapters/02-pca.html#variance-and-covariance",
    "title": "2  Principal Component Analysis",
    "section": "2.2 Variance and Covariance",
    "text": "2.2 Variance and Covariance\n\n2.2.1 Variance\nSuppose that we have a collection of measurements \\((x_1,\\ldots, x_N)\\) of a particular feature \\(X\\). For example, \\(x_i\\) might be the initial weight of the \\(ith\\) participant in our weight loss study. The mean of the values \\((x_1,\\ldots, x_N)\\) is\n\\[\n\\mu_{X} = \\frac{1}{N}\\sum_{i=1}^{N} x_{i}.\n\\]\nThe simplest measure of the variability of the data is called its variance.\nDefinition: The (sample) variance of the data \\(x_1,\\ldots, x_N\\) is\n\\[\n\\sigma_{X}^2 = \\frac{1}{N}\\sum_{i=1}^{N} \\left(x_{i}-\\mu_{X}\\right)^2 = \\frac{1}{N}\\left(\\sum_{i=1}^{N} x_{i}^2\\right)- \\mu_{X}^2\n\\tag{2.1}\\]\nThe square root of the variance is called the standard deviation.\nAs we see from the formula, the variance is a measure of how ‘spread out’ the data is from the mean.\nRecall that in our discussion of linear regression we thought of our set of measurements \\(x_1,\\ldots, x_N\\) as a vector – it’s one of the columns of our data matrix. From that point of view, the variance has a geometric interpretation – it is \\(\\frac{1}{N}\\) times the square of the distance from the point \\(X=(x_1,\\ldots, x_N)\\) to the point \\(\\mu_{X}(1,1,\\ldots,1)=\\mu_{X}E\\):\n\\[\n\\sigma_{X}^2 = \\frac{1}{N}(X-\\mu_{X}E)\\cdot(X-\\mu_{X}E)  = \\frac{1}{N}\\|X-\\mu_{X}E\\|^2.\n\\tag{2.2}\\]\n\n\n2.2.2 Covariance\nThe variance measures the dispersion of measures of a single feature. Often, we have measurements of multiple features and we might want to know something about how two features are related. The covariance is a measure of whether two features tend to be related, in the sense that when one increases, the other one increases; or when one increases, the other one decreases.\nDefinition: Given measurements \\((x_1,\\ldots, x_N)\\) and \\((y_1,\\ldots, y_N)\\) of two features \\(X\\) and \\(Y\\), the covariance of \\(X\\) and \\(Y\\) is\n\\[\n\\sigma_{XY} = \\frac{1}{N}\\sum_{i=1}^{N} (x_i-\\mu_{X})(y_i-\\mu_{Y})\n\\tag{2.3}\\]\nThere is a nice geometric interpretation of this, as well, in terms of the dot product. If \\(X=(x_1,\\ldots, x_N)\\) and \\(Y=(y_1\\ldots,y_N)\\) then\n\\[\n\\sigma_{XY} = \\frac{1}{N} ((X-\\mu_{X}E)\\cdot (Y-\\mu_{Y}E)).\n\\]\nFrom this point of view, we can see that \\(\\sigma_{XY}\\) is positive if the \\(X-\\mu_{X}E\\) and \\(Y-\\mu_{Y}E\\) vectors “point roughly in the same direction” and its negative if they “point roughly in the opposite direction.”\n\n\n2.2.3 Correlation\nOne problem with interpreting the variance and covariance is that we don’t have a scale – for example, if \\(\\sigma_{XY}\\) is large and positive, then we’d like to say that \\(X\\) and \\(Y\\) are closely related, but it could be just that the entries of \\(X-\\mu_{X}E\\) and \\(Y-\\mu_{Y}E\\) are large. Here, though, we can really take advantage of the geometric interpretation. Recall that the dot product of two vectors satisfies the formula\n\\[\na \\cdot b = \\|a\\|\\|b\\|\\cos(\\theta)\n\\]\nwhere \\(\\theta\\) is the angle between \\(a\\) and \\(b\\). So\n\\[\n\\cos(\\theta) = \\frac{a\\cdot b}{\\|a\\|\\|b\\|}.\n\\]\nLet’s apply this to the variance and covariance, by noticing that\n\\[\n\\frac{(X-\\mu_{X}E)\\cdot (Y-\\mu_{Y}E)}{\\|(X-\\mu_{X}E)\\|\\|(Y-\\mu_{Y}E)\\|} = \\frac{\\sigma_{XY}}{\\sigma_{XX}\\sigma_{YY}}\n\\]\nso the quantity\n\\[\nr_{XY} = \\frac{\\sigma_{XY}}{\\sigma_{X}\\sigma_{Y}}\n\\tag{2.4}\\]\nmeasures the cosine of the angle between the vectors \\(X-\\mu_{X}E\\) and \\(Y-\\mu_{Y}E\\).\nDefinition: The quantity \\(r_{XY}\\) defined in Equation 2.4 is called the (sample) correlation coefficient between \\(X\\) and \\(Y\\). We have \\(0\\le |r_{XY}|\\le 1\\) with \\(r_{XY}=\\pm 1\\) if and only if the two vectors \\(X-\\mu_{X}\\) and \\(Y-\\mu_{Y}\\) are collinear in \\(\\mathbf{R}^{N}\\).\n*Figure 2.1 illustrates data with different values of the correlation coefficient.\n\n\n\nFigure 2.1: Correlation\n\n\n\n\n2.2.4 The covariance matrix\nIn a typical situation we have many features for each of our (many) samples, that we organize into a data matrix \\(X\\). To recall, each column of \\(X\\) corresponds to a feature that we measure, and each row corresponds to a sample. For example, each row of our matrix might correspond to a person enrolled in a study, and the columns correspond to height (cm), weight (kg), systolic blood pressure, and age (in years):\n\nA sample data matrix \\(X\\)\n\n\nsample\nHt\nWgt\nBp\nAge\n\n\n\n\nA\n180\n75\n110\n35\n\n\nB\n193\n80\n130\n40\n\n\n…\n…\n…\n…\n…\n\n\nU\n150\n92\n105\n55\n\n\n\nIf we have multiple features, as in this example, we might be interested in the variance of each feature and all of their mutual covariances. This “package” of information can be obtained “all at once” by taking advantage of some matrix algebra.\nDefinition: Let \\(X\\) be a \\(N\\times k\\) data matrix, where the \\(k\\) columns of \\(X\\) correspond to different features and the \\(N\\) rows to different samples. Let \\(X_{0}\\) be the centered version of this data matrix, obtained by subtracting the mean \\(\\mu_{i}\\) of column \\(i\\) from all the entries \\(x_{si}\\) in that column. Then the \\(k\\times k\\) symmetric matrix\n\\[\nD_{0} = \\frac{1}{N}X_{0}^{\\intercal}X_{0}\n\\]\nis called the (sample) covariance matrix for the data.\nProposition: The diagonal entries \\(d_{ii}\\) of \\(D_{0}\\) are the variances of the columns of \\(X\\):\n\\[\nd_{ii} = \\sigma_{i}^2 = \\frac{1}{N}\\sum_{s=1}^{N}(x_{si}-\\mu_i)^2\n\\]\nand the off-diagonal entries \\(d_{ij} = d_{ji}\\) are the covariances of the \\(i^{th}\\) and \\(j^{th}\\) columns of \\(X\\):\n\\[\nd_{ij} = \\sigma_{ij} = \\frac{1}{N}\\sum_{s=1}^{N}(x_{si}-\\mu_{i})(x_{sj}-\\mu_{j})\n\\]\nThe sum of the diagonal entries, the trace of \\(D_{0}\\) is the total variance of the data.\nProof: This follows from the definitions, but it’s worth checking the details, which we leave as an exercise.\n\n\n2.2.5 Visualizing the covariance matrix\nIf the number of features in the data is not too large, a density matrix plot provides a tool for visualizing the covariance matrix of the data. A density matrix plot is an \\(k\\times k\\) grid of plots (where \\(k\\) is the number of features). The entry with \\((i,j)\\) coordinates in the grid is a scatter plot of the \\(i^{th}\\) feature against the \\(j^{th}\\) one if \\(i\\not=j\\), and is a histogram of the \\(i^{th}\\) variable if \\(i=j\\).\n*Figure 2.2 is an example of a density matrix plot for a dataset with \\(50\\) samples and \\(2\\) features. This data has been centered, so it can be represented in a \\(50\\times 2\\) data matrix \\(X_{0}\\). The upper left and lower right graphs are scatter plots of the two columns, while the lower left and upper right are the histograms of the columns.\n\n\n\nFigure 2.2: Density Matrix Plot\n\n\n\n\n2.2.6 Linear Combinations of Features (Scores)\nSometimes useful information about our data can be revealed if we combine different measurements together to obtain a “hybrid” measure that captures something interesting. For example, in the Auto MPG dataset that we studied in the section on Linear Regression, we looked at the influence of both vehicle weight \\(w\\) and engine displacement \\(e\\) on gas mileage; perhaps their is some value in considering a hybrid “score” defined as \\[\nS = aw + be\n\\] for some constants \\(a\\) and \\(b\\) – maybe by choosing a good combination we could find a better predictor of gas mileage than using one or the other of the features individually.\nAs another example, suppose we are interested in the impact of the nutritional content of food on weight gain in a study. We know that both calorie content and the level dietary fiber contribute to the weight gain of participants eating this particular food; maybe there is some kind of combined “calorie/fiber” score we could introduce that captures the impact of that food better.\nFinally, when we assign grades in a course, we typically compute a weighted combination of the scores of each student on a series of assignments. Such a combination is another example of a “score” and may help explain the origin of the term.\nDefinition: Let \\(X_{0}\\) be a (centered) \\(N\\times k\\) data matrix giving information about \\(k\\) features for each of \\(N\\) samples. A linear synthetic feature, or a linear score, is a linear combination of the \\(k\\) features. The linear score is defined by constants \\(a_{1},\\ldots, a_{k}\\) so that If \\(y_{1},\\ldots, y_{k}\\) are the values of the features for a particular sample, then the linear score for that sample is\n\\[\nS = a_{1}y_{1}+a_{2}y_{2}+\\cdots+a_{k}y_{k}\n\\]\nLemma: The values of the linear score for each of the \\(N\\) samples can be calculated as\n\\[\n\\left[\\begin{matrix} S_{1} \\\\ \\vdots \\\\ S_{N}\\\\ \\end{matrix}\\right] =\nX_{0}\\left[\n\\begin{matrix} a_{1} \\\\ \\vdots \\\\ a_{k}\\end{matrix}\\right].\n\\tag{2.5}\\]\nProof: Multiplying a matrix by a column vector computes a linear combination of the columns – that’s what this lemma says. Exercise 3 asks you to write out the indices and make sure you believe this.\n\n\n2.2.7 Mean and variance of scores\nWhen we combine features to make a hybrid score, we assume that the features were centered to begin with, so that each features has mean zero. As a result, the mean of the hybrid features is again zero.\nLemma: A linear combination of features with mean zero again has mean zero.\nProof: Let \\(S_{i}\\) be the score for the \\(i^{th}\\) sample, so \\[\nS_{i} = \\sum_{j=1}^{k} x_{ij}a_{j}.\n\\] where \\(X_{0}\\) has entries \\(x_{ij}\\). Then the mean value of the score is \\[\n\\mu_{S} = \\frac{1}{k}\\sum_{i=1}^{N} S_{i} = \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{k} x_{ij}a_{j}.\n\\] Reversing the order of the sum yields \\[\n\\mu_{S} = \\frac{1}{N}\\sum_{j=1}^{k}\\sum_{i=1}^{N} x_{ij}a_{j} = \\sum_{j=1}^{k} a_{j}\\frac{1}{N}(\\sum_{i=1}^{N} x_{ij})=\n\\sum_{j=1}^{k}a_{j}\\mu_{j}=0\n\\] where \\(\\mu_{j}=0\\) is the mean of the \\(j^{th}\\) feature (column) of \\(X_{0}\\).\nThe variance is more interesting, and gives us an opportunity to put the covariance matrix to work. Remember from Equation 2.2 that, since a score \\(S\\) has mean zero, it’s variance is \\(\\sigma_{S}^2=\\frac{1}{N}S\\cdot S\\) – where here the score \\(S\\) is represented by the column vector with entries \\(S_{1},\\ldots S_{k}\\) as in Equation 2.5.\nLemma: The variance of the score \\(S\\) with weights \\(a_1,\\ldots a_k\\) is \\[\n\\sigma_{S}^2 = a^{\\intercal}D_{0}a = \\left[\\begin{matrix}a_{1} & \\cdots & a_{k}\\end{matrix}\\right]D_{0}\n\\left[\\begin{matrix} a_{1} \\\\ \\vdots \\\\ a_{k}\\end{matrix}\\right]\n\\tag{2.6}\\] More generally, if \\(S_{1}\\) and \\(S_{2}\\) are scores with weights \\(a_1,\\ldots, a_k\\) and \\(b_1,\\ldots, b_k\\) respectively, then the covariance \\(\\sigma_{S_{1}S_{2}}\\) is \\[\n\\sigma_{S_{1}S_{2}} = a^{\\intercal}D_{0}b.\n\\]\nProof: From Equation 2.2 and Equation 2.5 we know that \\[\n\\sigma_{S}^2 = \\frac{1}{N}S\\cdot S\n\\] and \\[\nS = X_{0}a.\n\\] Since \\(\\frac{1}{N}S\\cdot S = \\frac{1}{N}S^{\\intercal}S\\), this gives us \\[\n\\frac{1}{N}\\sigma_{S}^2 = \\frac{1}{N}(X_{0}a)^{\\intercal}(X_{0}a) = \\frac{1}{N}a^{\\intercal}X_{0}^{\\intercal}X_{0}a = a^{\\intercal}D_{0}a\n\\] as claimed.\nFor the covariance, use a similar argument with Equation 2.3 and Equation 2.5. writing \\(\\sigma_{S_{1}S_{2}}=\\frac{1}{N}S_{1}\\cdot S_{2}\\) and the fact that \\(S_{1}\\) and \\(S_{2}\\) can be written as \\(X_{0}a\\) and \\(X_{0}b\\).\nThe point of this lemma is that the covariance matrix contains not just the variances and covariances of the original features, but also enough information to construct the variances and covariances for any linear combination of features.\nIn the next section we will see how to exploit this idea to reveal hidden structure in our data.\n\n\n2.2.8 Geometry of Scores\nLet’s return to the dataset that we looked at in Section 2.2.5. We simplify the density matrix plot in Figure 2.3, which shows one of the scatter plots and the two histograms.\nThe scatter plot shows that the data points are arranged in a more or less elliptical cloud oriented at an angle to the \\(xy\\)-axes which represent the two given features. The two individual histograms show the distribution of the two features – each has mean zero, with the \\(x\\)-features distributed between \\(-2\\) and \\(2\\) and the \\(y\\) feature between \\(-4\\) and \\(4\\). Looking just at the two features individually, meaning only at the two histograms, we can’t see the overall elliptical structure.\n\n\n\nFigure 2.3: Simulated Data with Two Features\n\n\nHow can we get a better grip on our data in this situation? We can try to find a “direction” in our data that better illuminates the variation of the data. For example, suppose that we pick a unit vector at the origin pointing in a particular direction in our data. See Figure 2.4.\n\n\n\nFigure 2.4: A direction in the data\n\n\nNow we can orthogonally project the datapoints onto the line defined by this vector, as shown in Figure 2.5.\n\n\n\nFigure 2.5: Projecting the datapoints\n\n\nRecall that if the unit vector is defined by coordinates \\(u=[u_0,u_1]\\), then the orthogonal projection of the point \\(x\\) with coordinates \\((x_0,x_1)\\) is \\((x\\cdot u)u\\). Now \\[\nx\\cdot u = u_0 x_0 + u_1 x_1\n\\] so the coordinates of the points along the line defined by \\(u\\) are the values of the score \\(Z\\) defined by \\(u=[u_0,u_1]\\). Using our work in the previous section, we see that we can find all of these coordinates by matrix multiplication: \\[\nZ = X_0 u\n\\] where \\(X_0\\) is our data matrix. Now let’s add a histogram of the values of \\(Z\\) to our picture:\n\n\n\nFigure 2.6: Distribution of Z\n\n\nThis histogram shows the distribution of the values of \\(Z\\) along the tilted line defined by the unit vector \\(u\\).\nFinally, using our work on the covariance matrix, we see that the variance of \\(Z\\) is given by \\[\n\\sigma_{Z}^2 = \\frac{1}{50}u^{\\intercal}X_{0}^{\\intercal}X_{0}u = u^{\\intercal}D_{0}u\n\\] where \\(D_{0}\\) is the covariance matrix of the data \\(X_{0}\\).\nLemma: Let \\(X_{0}\\) be a \\(N\\times k\\) centered data matrix, and let \\(D_{0}=\\frac{1}{N}X_{0}^{\\intercal}X_{0}\\) be the associated covariance matrix. Let \\(u\\) be a unit vector in “feature space” \\(\\mathbf{R}^{k}\\). Then the score \\(S=X_{0}u\\) can be interpreted as the coordinates of the points of \\(X_{0}\\) projected onto the line generated by \\(u\\). The variance of this score is \\[\n\\sigma^{2}_{S} = u^{\\intercal}D_{0}u = \\sum_{i=1}^{N} s_{i}^2\n\\] where \\(s_{i} = X_{0}[i,:]u\\) is the dot product of the \\(i^{th}\\) row \\(X_{0}[i,:]\\) with \\(u\\). It measures the variability in the data “in the direction of the unit vector \\(u\\)”."
  },
  {
    "objectID": "chapters/02-pca.html#principal-components",
    "href": "chapters/02-pca.html#principal-components",
    "title": "2  Principal Component Analysis",
    "section": "2.3 Principal Components",
    "text": "2.3 Principal Components\n\n2.3.1 Change of variance with direction\nAs we’ve seen in the previous section, if we choose a unit vector \\(u\\) in the feature space and find the projection \\(X_{0}u\\) of our data onto the line through \\(u\\), we get a “score” that we can use to measure the variance of the data in the direction of \\(u\\). What happens as we vary \\(u\\)?\nTo study this question, let’s continue with our simulated data from the previous section, and introduce a unit vector \\[\nu(\\theta) = \\left[\\begin{matrix} \\cos(\\theta) & \\sin(\\theta)\\end{matrix}\\right].\n\\] This is in fact a unit vector, since \\(\\sin^2(\\theta)+\\cos^2(\\theta)=1\\), and it is oriented at an angle \\(\\theta\\) from the \\(x\\)-axis.\nThe variance of the data in the direction of \\(u(\\theta)\\) is given by \\[\n\\sigma_{\\theta}^2 = u(\\theta)^{\\intercal}D_{0}u(\\theta).\n\\]\nA plot of this function for the data we have been considering is in Figure 2.7. As you can see, the variance goes through two full periods with the angle, and it reaches a maximum and minimum value at intervals of \\(\\pi/2\\) – so the two angles where the variance are maximum and minimum are orthogonal to one another.\n\n\n\nFigure 2.7: Change of variance with angle theta\n\n\nThe two directions where the variance is maximum and minimum are drawn on the original data scatter plot in Figure 2.8 .\n\n\n\nFigure 2.8: Data with principal directions\n\n\nLet’s try to understand why this is happening.\n\n\n2.3.2 Directions of extremal variance\nGiven our centered, \\(N\\times i\\) data matrix \\(X_{0}\\), with its associated covariance matrix \\(D_{0}=\\frac{1}{N}X_{0}^{\\intercal}X_{0}\\), we would like to find unit vectors \\(u\\) in \\(\\mathbf{R}^{k}\\) so that \\[\n\\sigma_{u}^{2} = u^{\\intercal}D_{0}u\n\\] reaches its maximum and its minimum. Here \\(\\sigma_{u}^2\\) is the variance of the “linear score” \\(X_{0}u\\) and it represents how dispersed the data is in the “u direction” in \\(\\mathbf{R}^{k}\\).\nIn this problem, remember that the coordinates of \\(u=(u_1,\\ldots, u_{k})\\) are the variables and the symmetric matrix \\(D_{0}\\) is given. As usual, we to find the maximum and minimum values of \\(\\sigma_{u}^{2}\\), we should look at the partial derivatives of \\(\\sigma_{u}^{2}\\) with respect to the variables \\(u_{i}\\) and set them to zero. Here, however, there is a catch – we want to restrict \\(u\\) to being a unit vector, with \\(u\\cdot u =\\sum u_{i}^2=1\\).\nSo this is a constrained optimization problem:\n\nFind extreme values of the function \\[\n\\sigma_{u}^{2} = u^{\\intercal}D_{0}u\n\\]\nSubject to the constraint \\(\\|u\\|^2 = u\\cdot u=1\\) (or \\(u\\cdot u-1=0\\))\n\nWe will use the technique of Lagrange Multipliers to solve such a problem.\nTo apply this method, we introduce the function\n\\[\nS(u, \\lambda) = u^{\\intercal}D_{0}u - \\lambda(u\\cdot u -1)\n\\tag{2.7}\\]\nThen we compute the gradient\n\\[\n\\nabla S = \\left[\\begin{matrix} \\frac{\\partial S}{\\partial u_{1}} \\\\ \\vdots \\\\ \\frac{\\partial S}{\\partial u_{k}} \\\\ \\frac{\\partial S}{\\partial \\lambda}\\end{matrix}\\right]\n\\tag{2.8}\\]\nand solve the system of equations \\(\\nabla S=0\\). Here we have written the gradient as a column vector for reasons that will become clearer shortly.\nComputing all of these partial derivatives looks messy, but actually if we take advantage of matrix algebra it’s not too bad. The following two lemmas explain how to do this.\nLemma: Let \\(M\\) be a \\(N\\times k\\) matrix with constant coefficients and let \\(u\\) be a \\(k\\times 1\\) column vector whose entries are \\(u_1,\\ldots u_{k}\\). The function \\(F(u) = Mu\\) is a linear map from \\(\\mathbf{R}^{k}\\to\\mathbf{R}^{N}\\). Its (total) derivative is a linear map between the same vector spaces, and satisfies \\[\nD(F)(v) = Mv\n\\] for any \\(k\\times 1\\) vector \\(v\\). If \\(u\\) is a \\(1\\times N\\) matrix, and \\(G(u) = uM\\), then \\[\nD(G)(v) = vM\n\\]\nfor any \\(1\\times N\\) vector \\(v\\). (This is the matrix version of the derivative rule that \\(\\frac{d}{dx}(ax)=a\\) for a constant \\(a\\).)\nProof: Since \\(F:\\mathbf{R}^{k}\\to\\mathbf{R}^{N}\\), we can write out \\(F\\) in more traditional function notation as \\[\nF(u) = (F_{1}(u_1,\\ldots, u_k), \\ldots, F_{N}(u_1,\\ldots, u_{k})\n\\] where \\[\nF_{i}(u_1,\\ldots u_k) = \\sum_{j=1}^{k} m_{ij}u_{j}.\n\\] Thus \\(\\frac{\\partial F_{i}}{\\partial u_{j}} = m_{ij}\\). The total derivative \\(D(F)\\) is the linear map with matrix \\[\nD(F)_{ij} = \\frac{\\partial F_{i}}{\\partial u_{j}} = m_{ij}\n\\] and so \\(D(F)=M\\).\nThe other result is proved the same way.\nLemma: Let \\(D\\) be a symmetric \\(k\\times k\\) matrix with constant entries and let \\(u\\) be an \\(k\\times 1\\) column vector of variables \\(u_{1},\\ldots, u_{k}\\). Let \\(F:\\mathbf{R}^{k}\\to R\\) be the function \\(F(u) = u^{\\intercal}Du\\). Then the gradient \\(\\nabla_{u} F\\) is a vector field – that is, a vector-valued function of \\(u\\), and is given by the formula \\[\n\\nabla_{u} F = 2Du\n\\]\nProof: Let \\(d_{ij}\\) be the \\(i,j\\) entry of \\(D\\). We can write out the function \\(F\\) to obtain \\[\nF(u_1,\\ldots, u_{k}) = \\sum_{i=1}^{k} \\sum_{j=1}^{k} u_i d_{ij} u_j.\n\\] Now \\(\\frac{\\partial F}{\\partial u_{i}}\\) is going to pick out only terms where \\(u_{i}\\) appears, yielding: \\[\n\\frac{\\partial F}{\\partial u_{i}} = \\sum_{j=1}^{k} d_{ij}u_{j} + \\sum_{j=1}^{k} u_{j}d_{ji}\n\\] Here the first sum catches all of the terms where the first “u” is \\(u_{i}\\); and the second sum catches all the terms where the second “u” is \\(u_{i}\\). The diagonal terms \\(u_{i}^2d_{ii}\\) contribute once to each sum, which is consistent with the rule that the derivative of \\(u_{i}^2d_{ii} = 2u_{i}d_{ii}\\). To finish the proof, notice that \\[\n\\sum_{j=1}^{k} u_{j}d_{ji} = \\sum_{j=1}^{k} d_{ij}u_{j}\n\\] since \\(D\\) is symmetric, so in fact the two terms are the same Thus \\[\n\\df{u_{i}}F = 2\\sum_{j=1}^{k} d_{ij}u_{j}\n\\] But the right hand side of this equation is twice the \\(i^{th}\\) entry of \\(Du\\), so putting the results together we get \\[\n\\nabla_{u}F = \\left[\\begin{matrix} \\frac{\\partial F}{\\partial u_{1}} \\\\ \\vdots \\\\ \\frac{\\partial F}{\\partial u_{k}}\\end{matrix}\\right] = 2Du.\n\\]\nThe following theorem puts all of this work together to reduce our questions about how variance changes with direction.\n\n\n2.3.3 Critical values of the variance\nTheorem: The critical values of the variance \\(\\sigma_{u}^2\\), as \\(u\\) varies over unit vectors in \\(\\mathbf{R}^{N}\\), are the eigenvalues \\(\\lambda_{1},\\ldots,\\lambda_{k}\\) of the covariance matrix \\(D\\), and if \\(e_{i}\\) is a unit eigenvector corresponding to \\(\\lambda_{i}\\), then \\(\\sigma_{e_{i}}^2 = \\lambda_{i}\\).\nProof: Recall that we introduced the Lagrange function \\(S(u,\\lambda)\\), whose critical points give us the solutions to our constrained optimization problem. As we said in Equation 2.7: \\[\nS(u,\\lambda) = u^{\\intercal}D_{0}u - \\lambda(u\\cdot u - 1) = u^{\\intercal}D_{0}u -\\lambda(u\\cdot u) + \\lambda\n\\] Now apply our Matrix calculus lemmas. First, let’s treat \\(\\lambda\\) as a constant and focus on the \\(u\\) variables. We can write \\(u\\cdot u = u^{\\intercal} I_{N} u\\) where \\(I_{N}\\) is the identity matrix to compute: \\[\n\\nabla_{u} S = 2D_{0}u -2\\lambda u\n\\] For \\(\\lambda\\) we have \\[\n\\df{\\lambda}S = -u\\cdot u +1.\n\\] The critical points occur when \\[\n\\nabla_{u} S = 2(D_{0}-\\lambda)u = 0\n\\] and \\[\n\\df{\\lambda}S = 1-u\\cdot u = 0\n\\] The first equation says that \\(\\lambda\\) must be an eigenvalue, and \\(u\\) an eigenvector: \\[\nD_{0}u = \\lambda u\n\\] while the second says \\(u\\) must be a unit vector \\(u\\cdot u=\\|u\\|^2=1\\). The second part of the result follows from the fact that if \\(e_{i}\\) is a unit eigenvector with eigenvalue \\(\\lambda_{i}\\) then \\[\n\\sigma_{e_{i}}^2 = e_{i}^{\\intercal}D_{0}e_{i} = \\lambda_{i}\\|e_{i}\\|^2=\\lambda_{i}.\n\\]\nTo really make this result pay off, we need to recall some key facts about the eigenvalues and eigenvectors of symmetric matrices. Because these facts are so central to this result, and to other applications throughout machine learning and mathematics generally, we provide proofs in Section 2.5.\n\n\nTable 2.1: Properties of Eigenvalues of Real Symmetric Matrices\n\n\n\n\n\nSummary\n\n\n\n\n1. All of the eigenvalues \\(\\lambda_{1},\\ldots, \\lambda_{l}\\) of \\(D\\) are real. If \\(u^{\\intercal}Du\\ge 0\\) for all \\(u\\in\\mathbf{R}^{k}\\), then all eigenvalues \\(\\lambda_{i}\\) are non-negative. In the latter case we say that \\(D\\) is positive semi-definite.\n\n\n2. If \\(v\\) is an eigenvector for \\(D\\) with eigenvalue \\(\\lambda\\), and \\(w\\) is an eigenvector with a different eigenvalue \\(\\lambda'\\), then \\(v\\) and \\(w\\) are orthogonal: \\(v\\cdot w = 0\\).\n\n\n3. There is an orthonormal basis \\(u_{1},\\ldots, u_{k}\\) of \\(\\mathbf{R}^{k}\\) made up of eigenvectors of \\(D\\) corresponding to the eigenvalues \\(\\lambda_{i}\\).\n\n\n4. Let \\(\\Lambda\\) be the diagonal matrix with entries \\(\\lambda_{1},\\ldots, \\lambda_{N}\\) and let \\(P\\) be the matrix whose columns are made up of the vectors \\(u_{i}\\). Then \\(D = P\\Lambda P^{\\intercal}.\\)\n\n\n\n\nIf we combine our theorem on the critical values with the spectral theorem we get a complete picture. Let \\(D_{0}\\) be the covariance matrix of our data. Since \\[\n\\sigma_{u}^2 = u^{\\intercal}D_{0}u\\ge 0 \\hbox{(it's a sum of squares)}\n\\] we know that the eigenvalues \\(\\lambda_{1}\\ge\\lambda_{2}\\ge \\cdots \\ge \\lambda_{k}\\ge 0\\) are all nonnegative. Choose a corresponding sequence \\(u_{1},\\ldots u_{k}\\) of orthogonal eigenvectors where all \\(\\|u_{i}\\|^2=1\\). Since the \\(u_{i}\\) form a basis of \\(\\mathbf{R}^{N}\\), any score is a linear combination of the \\(u_{i}\\): \\[\nS = \\sum_{i=1}^{k} a_{i}u_{i}.\n\\] Since \\(u_{i}^{\\intercal}D_{0}u_{j} = \\lambda_{j}u_{i}^{\\intercal}u_{j} = 0\\) unless \\(i=j\\), in which case it is \\(\\lambda_{i}\\), we can compute \\[\n\\sigma_{S}^2 = \\sum_{i=1}^{k} \\lambda_{i}a_{i}^2,\n\\] and \\(\\|S\\|^2=\\sum_{i=1}^{k} a_{i}^2\\) since the \\(u_{i}\\) are an orthonormal set. So in these coordinates, our optimization problem is:\n\nmaximize \\(\\sum \\lambda_{i}a_{i}^2\\)\nsubject to the constraint \\(\\sum a_{i}^2 = 1\\).\n\nWe don’t need any fancy math to see that the maximum happens when \\(a_{1}=1\\) and the other \\(a_{j}=0\\), and in that case, the maximum is \\(\\lambda_{1}\\). (If \\(\\lambda_{1}\\) occurs more than once, there may be a whole subspace of directions where the variance is maximal). Similarly, the minimum value is \\(\\lambda_{k}\\) and occurs when \\(a_{k}=1\\) and the others are zero.\n\n\n2.3.4 Subspaces of extremal variance\nWe can generalize the idea of the variance of our data in a particular direction to a higher dimensional version of total variance in a subspace. Suppose that \\(E\\) is a subspace of \\(\\mathbf{R}^{k}\\) and \\(U\\) is a matrix whose columns span \\(E\\) – the columns of \\(U\\) are the weights of a family of scores that span \\(E\\). The values of these scores are \\(XU\\) and the covariance matrix of this projected data is \\[\\frac{1}{N}U^{\\intercal}X^{\\intercal}XU=U^{\\intercal}D_{0}U.\\].\nFinally, the total variance \\(\\sigma_{E}^2\\) of the data projected into \\(E\\) is the sum of the diagonal entries of the matrix\n\\[\n\\sigma^2_{E} = \\mathop{trace}(U^{\\intercal}D_{0}U)\n\\]\nJust as the variance in a given direction \\(u\\) depends on the scaling of \\(u\\), the variance in a subspace depends on the scaling of the columns of \\(U\\). To normalize this scaling, we assume that the columns of \\(U\\) are an orthonormal basis of the subspace \\(E\\).\nNow we can generalize the question asked in Section 2.3.2 by seeking, not just a vector \\(u\\) pointing in the direction of the extremal variance, but instead the subspace \\(U_{s}\\) of dimension \\(s\\) with the property that the total variance of the projection of the data into \\(U_{s}\\) is maximal compared to its projection into other subspaces of that dimension. This is called a subspace of extremal variance.\nTo make this concrete, suppose we consider a subspace \\(E\\) of \\(\\mathbf{R}^{k}\\) of dimension \\(t\\) with basis \\(w_{1},\\ldots, w_{t}\\). Complete this to a basis \\(w_{1},\\ldots, w_{t},w_{t+1},\\ldots, w_{k}\\) of \\(\\mathbf{R}^{k}\\) and then apply the Gram Schmidt Process (see Section 2.5.1) to find an orthonormal basis \\(w'_{1},\\ldots,w'_{s},w'_{s+1},\\ldots, w'_{k}\\) where the \\(w'_{1},\\ldots, w'_{t}\\) are an orthonormal basis for \\(E\\). Let \\(W\\) be the \\(k\\times t\\) matrix whose columns are the \\(w'_{i}\\) for \\(i=1,\\ldots,t\\). The rows of the matrix \\(X_{0}W\\) given the coordinates of the projection of each sample into the subspace \\(E\\) expressed in terms of the scores corresponding to these vectors \\(w'_{i}\\). The total variance of these projections is\n\\[\n\\sigma_{E}^2 = \\sum_{i=1}^{t} \\|X_{0}w'_{i}\\|^2 = \\sum_{i=1}^{t} (w'_{i})^{\\intercal}X_{0}^{\\intercal}X_{0}w'_{i}  = \\sum_{i=1}^{t} (w'_{i})^{\\intercal}D_{0}w'_{i}\n\\]\nIf we want to maximize this, we have the constrained optimization problem of finding \\(w'_{1},\\ldots, w'_{t}\\) so that\n\n\\(\\sum_{i=1}^{t} (w'_{i})^{\\intercal}D_{0}w'_{i}\\) is maximal\nsubject to the constraint that each \\(w_{i}\\) has \\(\\|w'_{i}\\|^2=1\\),\nand that the \\(w'_{i}\\) are orthogonal, meaning \\(w'_{i}\\cdot w'_{j}=0\\) for \\(i\\not=j\\),\nand that the \\(w'_{i}\\) are linearly independent.\n\nThen the span \\(E\\) of these \\(w'_{i}\\) is subspace of extremal variance.\nTheorem: A \\(t\\)-dimensional subspace \\(E\\) is a subspace of extremal variance if and only if it is spanned by \\(t\\) orthonormal eigenvectors of the matrix \\(D_{0}\\) corresponding to the \\(t\\) largest eigenvalues for \\(D_{0}\\).\nProof: We can approach this problem using Lagrange multipliers and matrix calculus if we are careful. Our unknown is \\(k\\times t\\) matrix \\(W\\) whose columns are the \\(t\\) (unknown) vectors \\(w'_{i}\\). The objective function that we are seeking to maximize is \\[\nF = \\mathop{trace}(W^{\\intercal}D_{0}W) = \\sum_{i=1}^{t} (w'_{i})^{\\intercal}D_{0}w_{i}.\n\\] The constraints are the requirements that \\(\\|w'_{i}\\|^2=1\\) and \\(w'_{i}\\cdot w'_{j}=0\\) if \\(i\\not=j\\). If we introduction a matrix of lagrange multipliers \\(\\Lambda=(\\lambda_{ij})\\), where \\(\\lambda_{ij}\\) is the multiplier that goes with the the first of these constraints when \\(i=j\\), and the second when \\(i\\not=j\\), we can express our Lagrange function as: \\[\nS(W,\\Lambda) = \\mathop{trace}(W^{\\intercal}D_{0}W) - (W^{\\intercal}W-I)\\Lambda\n\\] where \\(I\\) is the \\(t\\times t\\) identity matrix.\nTaking the derivatives with respect to the entries of \\(W\\) and of \\(\\Lambda\\) yields the following two equations: \\[\\begin{align*}\nD_{0}W &= W\\Lambda \\\\\nW^{\\intercal}W &= I \\\\\n\\end{align*}\\]\nThe first of these equations says that the space \\(E\\) spanned by the columns of \\(W\\) is invariant under \\(D_{0}\\), while the second says that the columns of \\(W\\) form an orthonormal basis.\nLet’s assume for the moment that we have a matrix \\(W\\) that satisfies these conditions.\nThen it must be the case that \\(\\Lambda\\) is a symmetric, real valued \\(t\\times t\\) matrix, since \\[\nW^{\\intercal}D_{0}W = W^{\\intercal}W\\Lambda = \\Lambda.\n\\] and the matrix on the left is symmetric.\nBy the properties of real symmetric matrices (the spectral theorem), there are orthonormal vectors \\(q_{1},\\ldots q_{t}\\) that are eigenvectors of \\(\\Lambda\\) with corresponding eigenvalues \\(\\tau_{i}\\). If we let \\(Q\\) be the matrix whose columns are the vectors \\(q_{i}\\) and let \\(T\\) be the diagonal \\(t\\times t\\) matrix whose entries are the \\(\\tau_{i}\\), we have \\[\n\\Lambda Q = QT.\n\\]\nIf we go back to our original equations, we see that if \\(W\\) exists such that \\(DW=W\\Lambda\\), then there is a matrix \\(Q\\) with orthonormal columns and a diagonal matrix \\(T\\) such that \\[\nD_{0}WQ = W\\Lambda Q = W Q T.\n\\] In other words, \\(WQ\\) is a matrix whose columns are eigenvectors of \\(D_{0}\\) with eigenvalues \\(\\tau_{i}\\) for \\(i=1,\\ldots, t\\).\nThus we see how to construct an invariant subspace \\(E\\) and a solution matrix \\(W\\). Such an \\(E\\) is spanned by \\(t\\) orthonormal eigenvectors \\(q_{i}\\) with eigenvalues \\(\\tau_{i}\\) of \\(D_{0}\\); and \\(W\\) is is the matrix whose columns are the \\(q_{i}\\). Further, in that case, the total variance associated to \\(E\\) is the sum of the eigenvalues \\(\\tau_{i}\\); to make this as large as possible, we should choose our eigenvectors to correspond to \\(t\\) of the largest eigenvalues of \\(D_{0}\\). This concludes the proof.\n\n\n2.3.5 Definition of Principal Components\nDefinition: The orthonormal unit eigenvectors \\(u_{i}\\) for \\(D_{0}\\) are the principal directions or principal components for the data \\(X_{0}\\).\nTheorem: The maximum variance occurs in the principal direction(s) associated to the largest eigenvalue, and the minimum variance in the principal direction(s) associated with the smallest one. The covariance between scores in principal directions associatedwith different eigenvalues is zero.\nAt this point, the picture in Figure 2.8 makes sense – the red and green dashed lines are the principal directions, they are orthogonal to one another, and the point in the directions where the data is most (and least) “spread out.”\nProof: The statement about the largest and smallest eigenvalues is proved at the very end of the last section. The covariance of two scores corresponding to different eigenvectors \\(u_{i}\\) and \\(u_{j}\\) is \\[u_{i}^{\\intercal}D_{0}u_{j} = \\lambda_{j}(u_{i}\\cdot u_{j}) = 0\\] since the \\(u_{i}\\) and \\(u_{j}\\) are orthogonal.\nSometimes the results above are presented in a slightly different form, and may be referred to, in part, as Rayleigh’s theorem.\nCorollary: (Rayleigh’s Theorem) Let \\(D\\) be a real symmetric matrix and let \\[\nH(v) = \\max_{v\\not = 0}\\frac{v^{\\intercal}Dv}{v^{\\intercal}v}.\n\\] Then \\(H(v)\\) is the largest eigenvalue of \\(D\\). (Similarly, if we replace \\(\\max\\) by \\(\\min\\), then the minimum is the least eigenvalue).\nProof: The maximum of the function \\(H(v)\\) is the solution to the same optimization problem that we considered above.\nExercises.\n\nProve that the two expressions for \\(\\sigma_{X}^2\\) given in Equation 2.1 are the same.\nProve that the covariance matrix is as described in the proposition in Section 2.2.4.\nLet \\(X_{0}\\) be a \\(k\\times N\\) matrix with entries \\(x_{ij}\\) for \\(1\\le i\\le k\\) and \\(1\\le j\\le N\\). If a linear score is defined by the constants \\(a_{1},\\ldots a_{N}\\), check that equation Equation 2.5 holds as claimed.\nWhy is it important to use a unit vector when computing the variance of \\(X_{0}\\) in the direction of \\(u\\)? Suppose \\(v=\\lambda u\\) where \\(u\\) is a unit vector and \\(\\lambda>0\\) is a constant. Let \\(S'\\) be the score \\(X_{0}v\\). How is the variance of \\(S'\\) related to that of \\(S=X_{0}u\\)?"
  },
  {
    "objectID": "chapters/02-pca.html#dimensionality-reduction-via-principal-components",
    "href": "chapters/02-pca.html#dimensionality-reduction-via-principal-components",
    "title": "2  Principal Component Analysis",
    "section": "2.4 Dimensionality Reduction via Principal Components",
    "text": "2.4 Dimensionality Reduction via Principal Components\nThe principal components associated with a dataset separate out directions in the feature space in which the data is most (or least) variable. One of the main applications of this information is to enable us to take data with a great many features – a set of points in a high dimensional space – and, by focusing our attention on the scores corresponding to the principal directions, capture most of the information in the data in a much lower dimensional setting.\nTo illustrate how this is done, let \\(X\\) be a \\(N\\times k\\) data matrix, let \\(X_{0}\\) be its centered version, and let \\(D_{0} = \\frac{1}{N}X_{0}^{\\intercal}X\\) be the associated covariance matrix.\nApply the spectral theorem (proved in Section 2.5) to the covariance matrix to obtain eigenvalues \\(\\lambda_{1}\\ge \\lambda_{2}\\ge\\cdots \\lambda_{k}\\ge 0\\) and associated eigenvectors \\(u_{1},\\ldots, u_{k}\\). The scores \\(S_{i}=X_{0}u_{i}\\) give the values of the data in the principal directions. The variance of \\(S_{i}\\) is \\(\\lambda_{i}\\).\nNow choose a number \\(t<k\\) and consider the vectors \\(S_{1},\\ldots, S_{t}\\). The \\(j^{th}\\) entry in \\(S_{i}\\) is the value of the score \\(S_{i}\\) for the \\(j^{th}\\) data point. Because \\(S_{1},\\ldots, S_{t}\\) capture the most significant variability in the original data, we can learn a lot about our data by considering just these \\(t\\) features of the data, instead of needing all \\(N\\).\nTo illustrate, let’s look at an example. We begin with a synthetic dataset \\(X_{0}\\) which has \\(200\\) samples and \\(15\\) features. The data (some of it) for some of the samples is shown in Table 2.2.\n\n\nTable 2.2: Simulated Data for PCA Analysis\n\n\n\nf-0\nf-1\nf-2\nf-3\nf-4\n...\nf-10\nf-11\nf-12\nf-13\nf-14\n\n\n\n\ns-0\n1.18\n-0.41\n2.02\n0.44\n2.24\n...\n0.32\n0.95\n0.88\n1.10\n0.89\n\n\ns-1\n0.74\n0.58\n1.54\n0.23\n2.05\n...\n0.99\n1.14\n1.56\n0.99\n0.59\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\ns-198\n1.04\n2.02\n1.44\n0.40\n1.33\n...\n0.62\n0.62\n0.54\n1.96\n0.04\n\n\ns-199\n0.92\n2.09\n1.58\n1.19\n1.17\n...\n0.42\n0.85\n0.83\n2.22\n0.90\n\n\n\n\nThe full dataset is a \\(200\\times 15\\) matrix; it has \\(3000\\) numbers in it and we’re not really equipped to make sense of it. We could try some graphing – for example, Figure 2.9 shows a scatter plot of two of the features plotted against each other.\n\n\n\nFigure 2.9: Scatter Plot of Two Features\n\n\nUnfortunately there’s not much to see in Figure 2.9 – just a blob – because the individual features of the data don’t tell us much in isolation, whatever structure there is in this data arises out of the relationship between different features.\nIn Figure 2.10 we show a “density grid” plot of the data. The graph in position \\(i,j\\) shows a scatter plot of the \\(i^{th}\\) and \\(j^{th}\\) columns of the data, except in the diagonal positions, where in position \\(i,i\\) we plot a histogram of column \\(i\\). There’s not much structure visible; it is a lot of blobs.\n\n\n\nFigure 2.10: Density Grid Plot of All Features\n\n\nSo let’s apply the theory of principal components. We use a software package to compute the eigenvalues and eigenvectors of the matrix \\(D_{0}\\). The \\(15\\) eigenvalues \\(\\lambda_{1}\\ge \\cdots \\ge \\lambda_{15}\\) are plotted, in descending order, in Figure 2.11 .\n\n\n\nFigure 2.11: Eigenvalues of the Covariance Matrix\n\n\nThis plot shows that the first \\(4\\) eigenvalues are relatively large, while the remaining \\(11\\) are smaller and not much different from each other. We interpret this as saying that most of the variation in the data is accounted for by the first four principal components. We can even make this quantitative. The total variance of the data is the sum of the eigenvalues of the covariance matrix – the trace of \\(D_{0}\\) – and in this example that sum is around \\(5\\). The sum of the first \\(4\\) eigenvalues is about \\(4\\), so the first four eignvalues account for about \\(4/5\\) of the total variance, or about \\(80\\%\\) of the variation of the data.\nNow let’s focus in on the two largest eigenvalues \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\) and their corresponding eigenvectors \\(u_{1}\\) and \\(u_{2}\\). The \\(200\\times 1\\) column vectors \\(S_{1}=X_{0}u_{1}\\) and \\(S_{2}=X_{0}u_{2}\\) are the values of the scores associated with these two eigenvectors. So for each data point (each row of \\(X_{0}\\)) we have two values (the corresponding entries of \\(S_{1}\\) and \\(S_{2}\\).) In Figure 2.12 we show a scatter plot of these scores.\n\n\n\nFigure 2.12: Scatter Plot of Scores in the First Two Principal Directions\n\n\nNotice that suddenly some structure emerges in our data! We can see that the 200 points are separated into five clusters, distinguished by the values of their scores! This ability to find hidden structure in complicated data, is one of the most important applications of principal components.\nIf we were dealing with real data, we would now want to investigate the different groups of points to see if we can understand what characteristics the principal components have identified.\n\n2.4.1 Loadings\nThere’s one last piece of the PCA puzzle that we are going to investigate. In Figure 2.12, we plotted our data points in the coordinates given by the first two principal components. In geometric terms, we took the cloud of \\(200\\) points in \\(\\mathbf{R}^{15}\\) given by the rows of \\(X_{0}\\) and projected those points into the two dimensional plane spanned by the eigenvectors \\(u_{1}\\) and \\(u_{2}\\), and then plotted the distribution of the points in that plane.\nMore generally, suppose we take our dataset \\(X_{0}\\) and consider the first \\(t\\) principal components corresponding to the eigenvectors \\(u_{1},\\ldots, u_{t}\\). The projection of the data into the space spanned by these eigenvectors is the represented by the \\(S = k\\times t\\) matrix \\(X_{0}U\\) where \\(U\\) is the \\(k\\times t\\) matrix whose columns are the eigenvectors \\(u_{i}\\). Each row of \\(S\\) gives the values of the score arising from \\(u_{i}\\) in the \\(i^{th}\\) column for \\(i=1,\\ldots, t\\).\nThe remaining question that we wish to consider is: how can we see some evidence of the original features in subspace? We can answer this by imagining that we had an artificial sample \\(x\\) that has a measurement of \\(1\\) for the \\(i^{th}\\) feature and a measurement of zero for all the other features. The corresponding point is represented by a \\(1\\times k\\) row vector with a \\(1\\) in position \\(i\\). The projection of this synthetic sample into the span of the first \\(t\\) principal components is the \\(1\\times t\\) vector \\(xU\\). Notice, however, that \\(xU\\) is just the \\(i^{th}\\) row of the matrix \\(U\\). This vector in the space spanned by the \\(u_{i}\\) is called the “loading” of the \\(i^{th}\\) feature in the principal components.\nThis is illustrated in Figure 2.13, which shows a line along the direction of the loading corresponding to the each feature added to the scatter plot of the data in the plane spanned by the first two principal components. One observation one can make is that some of the features are more “left to right”, like features \\(7\\) and \\(8\\), while others are more “top to bottom”, like \\(6\\). So points that lie on the left side of the plot have smaller values of features \\(7\\) and \\(8\\), while those at the top of the plot have larger values of feature \\(6\\).\n\n\n\nFigure 2.13: Loadings in the Principal Component Plane\n\n\n\n\n2.4.2 The singular value decomposition\nThe singular value decomposition is a slightly more detailed way of looking at principal components. Let \\(\\Lambda\\) be the diagonal matrix of eigenvalues of \\(D_{0}\\) and let \\(P\\) be the \\(k\\times k\\) orthogonal matrix whose columns are the principal components. Then we have \\[\nD_{0} =\\frac{1}{N}X_{0}^{\\intercal}X_{0}= P\\Lambda P^{\\intercal}.\n\\] Consider the \\(N\\times k\\) matrix \\[\nX_{0}P = A.\n\\]\nAs we saw in the previous section, the columns of \\(A\\) give the projection of the data into the \\(k\\) principal directions. Then\n\\[A^{\\intercal}A=P^{\\intercal}X_{0}^{\\intercal}X_{0}P=N\\Lambda.\\]\nIn other words, the columns of \\(A\\) are orthogonal and the diagonal entries of \\(A^{\\intercal}A\\) are \\(N\\) times the variance of the data in the various principal directions.\nNow we are going to tinker with the matrix \\(A\\) in order to make an \\(N\\times N\\) orthogonal matrix. The first modification we make is to normalize the columns of \\(A\\) so that they have length \\(1\\). We do this by setting \\[\nA_{1} = A(N\\Lambda)^{-1/2}.\n\\] Then \\(A_{1}^{\\intercal}A_{1}\\) is the identity, so the columns of \\(A_{1}\\) are orthonormal. Here we are assuming that the eigenvalues of \\(D_{0}\\) are nonzero – this isn’t strictly necessary, and we could work around this, but for simplicity we will assume it’s true. It amounts to the assumption that the independent variables are not linearly related, as we’ve seen before.\nThe second modification is to extend \\(A_{1}\\) to an \\(N\\times N\\) matrix. The \\(k\\) columns of \\(A_1\\) span only a \\(k\\)-dimensional subspace of the \\(N\\)-dimensional space where the feature vectors lie.\nComplete the subspace by finding an orthogonal complement to it – that is, find \\(N-k\\) mutually orthogonal unit vectors all orthogonal to the column space of \\(A_1\\). By adding these vectors to \\(A\\) as columns, create an extended \\(N\\times N\\) matrix \\(\\tilde{A}_1\\) which is orthogonal.\nNotice that \\(\\tilde{A}_{1}^{\\intercal}A\\) is an \\(N\\times k\\) matrix whose upper \\(k\\times k\\) block is \\((N\\Lambda)^{1/2}\\) and whose final \\(N-k\\) rows are all zero. We call this matrix \\(\\tilde{\\Lambda}\\).\nTo maintain consistency with the traditional formulation, we let \\(U=\\tilde{A}_{1}^{\\intercal}\\) and then we have the following proposition.\nProposition: We have a factorization \\[\nX_{0} = U\\tilde{\\Lambda}P^{\\intercal}\n\\tag{2.9}\\] where \\(U\\) and \\(P\\) are orthogonal matrices of size \\(N\\times N\\) and \\(k\\times k\\) respectively, and \\(\\tilde{\\Lambda}\\) is an \\(N\\times k\\) diagonal matrix. This is called the “singular value decomposition” of \\(X_{0}\\), and the entries of \\(\\tilde{\\Lambda}\\) are called the singular values. If we let \\(u_1,\\ldots, u_k\\) be the first \\(k\\) rows of \\(U\\), then the \\(k\\) column vectors \\(u_{i}^{\\intercal}\\) are an orthonormal basis for the feature space spanned by the columns of \\(X_{0}\\), and they point in the “principal directions” for the data matrix \\(X_{0}\\).\nIn this section we take a slight detour and apply what we’ve learned about the covariance matrix, principal components, and the singular value decomposition to the original problem of linear regression that we studied in Chapter 1.\nIn this setting, in addition to our centered data matrix \\(X_{0}\\), we have a vector \\(Y\\) of target values and we find the “best” approximation \\[\n\\hat{Y} = X_{0}M\n\\] using the least squares method. As we showed in Chapter 1, the optimum \\(M\\) is found as \\[\nM = (X_{0}^{\\intercal}X_{0})^{-1}X^{\\intercal}Y = ND_{0}^{-1}X_0^{\\intercal}Y\n\\] and the predicted values \\(\\hat{Y}\\) are \\[\n\\hat{Y} = NX_{0}D_{0}^{-1}X_0^{\\intercal}Y.\n\\]\nGeometrically, we understood this process as defining \\(\\hat{Y}\\) to be the orthogonal projection of \\(Y\\) into the subspace spanned by the columns of \\(X_{0}\\).\nLet’s use the decomposition (see Equation 2.9 ) \\(X_{0}=U\\tilde{\\Lambda}P^{\\intercal}\\) in this formula. First, notice that \\[\nX_{0}^{\\intercal}X_{0}= P\\tilde{\\Lambda}^{\\intercal}U^{\\intercal}U\\tilde{\\Lambda}P^{\\intercal} = P\\tilde{\\Lambda}^{\\intercal}\\tilde{\\Lambda}P^{\\intercal}.\n\\]\nThe middle term \\(\\tilde{\\Lambda}^{\\intercal}\\tilde{\\Lambda}\\) is the \\(k\\times k\\) matrix \\(\\Lambda\\) whose diagonal entries are \\(N\\lambda_{i}\\) where \\(\\lambda_{i}\\) are the eigenvalues of the covariance matrix \\(D_{0}\\). Assuming these are all nonzero (which is tantamount to the assumption that the covariance matrix is invertible), we obtain \\[\n\\hat{Y} = NU\\tilde{\\Lambda}P^{\\intercal}P\\Lambda^{-1}P^{\\intercal}P\\tilde{\\Lambda}^{\\intercal}U^{\\intercal}Y.\n\\] There is a lot of cancellation here, and in the end what’s left is \\[\n\\hat{Y}=UEU^{\\intercal}Y\n\\] where \\(E\\) is and \\(N\\times N\\) matrix whose upper \\(k\\times k\\) block is the identity and whose remaining entries are zero. Rearranging a bit more we have \\[\nU^{\\intercal}\\hat{Y} = EU^{\\intercal}Y.\n\\]\nTo unpack this equation, let \\(u_{1},\\ldots, u_{N}\\) be the rows of the matrix \\(U\\). Since \\(U\\) is an orthogonal matrix, the column vectors \\(u_{i}^{\\intercal}\\) are an orthonormal basis for the \\(N\\) dimensional space where the columns of \\(X_{0}\\) lie. We can write the target vector \\(Y\\) \\[\nY = \\sum_{j=1}^{N} (u_{j}\\cdot Y)u_{j}^{\\intercal}.\n\\]\nThen the projection \\(\\hat{Y}\\) of \\(Y\\) into the subspace spanned by the data is obtained by dropping the last \\(N-k\\) terms in the sum:\n\\[\n\\hat{Y}=\\sum_{j=1}^{k} (u_{j}\\cdot Y)u_{j}^{\\intercal}\n\\]"
  },
  {
    "objectID": "chapters/02-pca.html#sec-spectraltheorem",
    "href": "chapters/02-pca.html#sec-spectraltheorem",
    "title": "2  Principal Component Analysis",
    "section": "2.5 Eigenvalues and Eigenvectors of Real Symmetric Matrices (The Spectral Theorem)",
    "text": "2.5 Eigenvalues and Eigenvectors of Real Symmetric Matrices (The Spectral Theorem)\nNow that we’ve shown how to apply the theory of eigenvalues and eigenvectors of symmetric matrices to extract principal directions from data, and to use those principal directions to find structure, we will give a proof of the properties that we summarized in Table 2.1.\nA key tool in the proof is the Gram-Schmidt orthogonalization process.\n\n2.5.1 Gram-Schmidt\nProposition (Gram-Schmidt Process): Let \\(w_{1},\\ldots, w_{k}\\) be a collection of linearly independent vectors in \\(\\mathbf{R}^{N}\\) and let \\(W\\) be the span of the \\(w_{i}\\). Let \\(u_{1} = w_{1}\\) and let \\[\nu_{i} = w_{i} - \\sum_{j=1}^{i-1} \\frac{w_{i}\\cdot u_{j}}{u_{j}\\cdot u_{j}}u_{j}\n\\] for \\(i=2,\\ldots, k\\). Then\n\nThe vectors \\(u_{i}\\) are orthogonal: \\(u_{i}\\cdot u_{j}=0\\) unless \\(i=j\\).\nThe vectors \\(u_{i}\\) span \\(W\\).\nEach \\(u_{i}\\) is orthogonal to the all of \\(w_{1},\\ldots, w_{i-1}\\).\nThe vectors \\(u'_{i} = u_{i}/\\|u_{i}\\|\\) are orthonormal.\n\nProof: This is an inductive exercise, and we leave it to you to work out the details.\n\n\n2.5.2 The spectral theorem\nTheorem: Let \\(D\\) be a real symmetric \\(N\\times N\\) matrix. Then:\n\nAll of the \\(N\\) eigenvalues \\(\\lambda_1\\ge \\lambda_2\\ge \\cdots \\ge \\lambda_{N}\\) are real. If \\(u^{\\intercal}Du\\ge 0\\) for all \\(u\\in\\mathbf{R}^{N}\\), then all eigenvalues \\(\\lambda_{i}\\ge 0\\).\nThe matrix \\(D\\) is diagonalizable – that is, it has \\(N\\) linearly independent eigenvectors.\nIf \\(v\\) and \\(w\\) are eigenvectors corresponding to eigenvalues \\(\\lambda\\) and \\(\\lambda'\\), with \\(\\lambda\\not=\\lambda'\\), then \\(v\\) and \\(w\\) are orthogonal: \\(v\\cdot w=0\\).\nThere is an orthonormal basis \\(u_{1},\\ldots, u_{N}\\) of \\(\\mathbf{R}^{N}\\) made up of eigenvectors for the eigenvalues \\(\\lambda_{i}\\).\nLet \\(\\Lambda\\) be the diagonal matrix with entries \\(\\lambda_{1},\\ldots, \\lambda_{N}\\) and let \\(P\\) be the matrix whose columns are made up of the eigenvectors \\(u_{i}\\). Then \\(D=P\\Lambda P^{\\intercal}\\).\n\nProof: First of all, we use the fact that any matrix has at least one eigenvector with associated eigenvalue. This is a theorem from linear algebra that relies on the fundamental theorem of algebra. With this result available, we start by proving part 1. Suppose that \\(\\lambda\\) is an eigenvalue of \\(D\\). Let \\(u\\) be a corresponding nonzero eigenvector. Then \\(Du=\\lambda u\\) and \\(D\\overline{u}=\\overline{\\lambda}\\overline{u}\\), where \\(\\overline{u}\\) is the vector whose entries are the conjugates of the entries of \\(u\\) (and \\(\\overline{D}=D\\) since \\(D\\) is real). Now we have \\[\n\\overline{u}^{\\intercal}Du = \\lambda \\overline{u}\\cdot u = \\lambda\\|u\\|^2\n\\] and \\[\nu^{\\intercal}D\\overline{u} = \\overline{\\lambda}u\\cdot \\overline{u} = \\overline{\\lambda}\\|u\\|^2.\n\\] But the left hand side of both of these equations are the same (take the transpose and use the symmetry of \\(D\\)) so we must have \\(\\lambda\\|u\\|^2 = \\overline{\\lambda}\\|u\\|^2\\) so \\(\\lambda=\\overline{\\lambda}\\), meaning \\(\\lambda\\) is real.\nIf we have the additional property that \\(u^{\\intercal}Du\\ge 0\\) for all \\(u\\), then in particular \\(u_{i}^{\\intercal}Du_{i} = \\lambda\\|u\\|^2\\ge 0\\), and since \\(\\|u\\|^2> 0\\) we must have \\(\\lambda\\ge 0\\).\nProperty \\(2\\) is in some ways the most critical fact. We know from the general theory of the characteristic polynomial, and the fundamental theorem of algebra, that \\(D\\) has \\(N\\) complex eigenvalues, although some may be repeated. However, it may not be the case that \\(D\\) has \\(N\\) linearly independent eigenvectors – it may not be diagonalizable. So we will establish that any symmetric matrix over the real numbers is diagonalizable.\nA one-by-one matrix is automatically symmetric and diagonalizable. In the \\(N\\)-dimensional case, we know, at least, that \\(D\\) has at least one eigenvector, and real one at that by part \\(1\\), and this gives us a place to begin an inductive argument.\nLet \\(v_{N}\\not=0\\) be an eigenvector with eigenvalue \\(\\lambda\\) and normalized so that \\(\\|v_{N}\\|^2=1\\),\nand extend this to a basis \\(v_{1},\\ldots v_{N}\\) of \\(\\mathbf{R}^{N}\\). Apply the Gram-Schmidt process to construct an orthonormal basis of \\(\\mathbf{R}^{N}\\) \\(u_{1},\\ldots, u_{N}\\) so that \\(u_{N}=v_{N}\\).\nAny vector \\(v\\in\\mathbf{R}^{N}\\) is a linear combination \\[\nv = \\sum_{i=1}^{N} a_{i}u_{i}\n\\] and, since the \\(u_{i}\\) are orthonormal, the coefficients can be calculated as \\(a_{i}=(u_{i}\\cdot v)\\).\nUsing this, we can find the matrix \\(D'\\) of the linear map defined by our original matrix \\(D\\) in this new basis. By definition, if \\(d'_{ij}\\) are the entries of \\(D'\\), then\n\\[\nDu_{i} = \\sum_{j=1}^{N} d'_{ij} u_{j}\n\\]\nand so\n\\[\nd'_{ij} = u_{j}\\cdot Du_{i} = u_{j}^{\\intercal}Du_{i}.\n\\]\nSince \\(D\\) is symmetric, \\(u_{j}^{\\intercal}Du_{i} =u_{i}^{\\intercal}Du_{j}\\) and so \\(d'_{ij}=d'_{ji}\\). In other words, the matrix \\(D'\\) is still symmetric. Furthermore,\n\\[\nd'_{Ni} = u_{i}\\cdot Du_{N} = u_{i}\\cdot \\lambda u_{N} = \\lambda (u_{i}\\cdot u_{N})\n\\]\nsince \\(u_{N}=v_{N}\\). Since the \\(u_{i}\\) are an orthonormal basis, we see that \\(d'_{iN}=0\\) unless \\(i=N\\), and \\(d'_{NN}=\\lambda\\).\nIn other words, the matrix \\(D'\\) has a block form: \\[\nD' = \\left(\\begin{matrix} *&* & \\cdots &*  & 0 \\\\ \\vdots & \\vdots & \\ddots   & \\vdots & \\vdots \\\\\n* & *& \\cdots &*  & 0 \\\\\n0 & 0 & \\cdots &0 &\\lambda \\end{matrix}\\right)\n\\] and the block denoted by \\(*\\)’s is symmetric. If we call that block \\(D_{*}\\), the inductive hypothesis tells us that the symmetric matrix \\(D_{*}\\) is diagonalizable, so it has a basis of eigenvectors \\(u'_{1},\\ldots, u'_{N-1}\\) with eigenvalues \\(\\lambda_{1},\\ldots, \\lambda_{N-1}\\); this gives us a basis for the subspace of \\(\\mathbf{R}^{N}\\) spanned by \\(u_{1},\\ldots, u_{N-1}\\) which, together with \\(u_{N}\\) gives us a basis of \\(\\mathbf{R}^{N}\\) consisting of eigenvectors of \\(D\\).\nThis finishes the proof of Property \\(2\\).\nFor property \\(3\\), compute \\[\nv^{\\intercal}Dw = \\lambda'(v\\cdot w)=w^{\\intercal}Dv = \\lambda (w\\cdot v).\n\\] Since \\(\\lambda\\not=\\lambda'\\), we must have \\(v\\cdot w=0\\).\nFor property \\(4\\), if the eigenvalues are all distinct, this is a consequence of property \\(2\\) – you have \\(N\\) eigenvectors, scaled to length \\(1\\), for different eigenvalues, and by \\(2\\) they are orthogonal. So the only complication is the case where some eigenvalues are repeated. If \\(\\lambda\\) occurs \\(r\\) times, then you have \\(r\\) linearly independent vectors \\(u_{1},\\ldots, u_{r}\\) that span the \\(\\lambda\\) eigenspace. The Gram-Schmidt process allows you to construct an orthonormal set that spans this eigenspace, and while this orthonormal set isn’t unique, any one of them will do.\nFor property \\(5\\), let \\(e_{i}\\) be the column vector that is zero except for a \\(1\\) in position \\(i\\). The product \\(e_{j}^{\\intercal}De_{i}=d_{ij}\\). Let’s write \\(e_{i}\\) and \\(e_{j}\\) in terms of the orthonormal basis \\(u_{1},\\ldots u_{N}\\): \\[\ne_{i} = \\sum_{k=1}^{N} (e_{i}\\cdot u_{k})u_k \\hbox{ and } e_{j} = \\sum_{k=1}^{N}(e_{j}\\cdot u_{k})u_{k}.\n\\] Using this expansion, we compute \\(e_{j}^{\\intercal}De_{i}\\) in a more complicated way: \\[\ne_{j}^{\\intercal}De_{i} = \\sum_{r=1}^{N}\\sum_{s=1}^{N} (e_{j}\\cdot u_{r})(e_{i}\\cdot u_{s})(u_{r}^{\\intercal}Du_{s}).\n\\] But \\(u_{r}^{\\intercal}Du_{s}=\\lambda_{s}(u_{r}\\cdot u_{s})=0\\) unless \\(r=s\\), in which case it equals \\(\\lambda_{r}\\), so \\[\ne_{j}^{\\intercal}De_{i} = \\sum_{r=1}^{N} \\lambda_{r}(e_{j}\\cdot u_{r})(e_{i}\\cdot u_{r}).\n\\] On the other hand, \\[\nP^{\\intercal}e_{i} = \\left[\\begin{matrix} (e_{i}\\cdot u_{1})\\\\ (e_{i}\\cdot u_{2})\\\\ \\vdots \\\\(e_{i}\\cdot u_{N})\\end{matrix}\\right]\n\\] and \\[\n\\Lambda P^{\\intercal}e_{i} = \\left[\\begin{matrix} \\lambda_{1}(e_{i}\\cdot u_{i})\\\\ \\lambda_{2}(e_{i}\\cdot u_{2})\\\\ \\vdots \\\\ \\lambda_{N}(e_{i}\\cdot u_{N})\\end{matrix}\\right]\n\\] Therefore the \\(i,j\\) entry of \\(P\\Lambda P^{\\intercal}\\) is \\[\n(e_{j}^{\\intercal}P)\\Lambda (P^{\\intercal}e_{j}) = \\sum_{r=1}^{N} \\lambda_{r}(e_{i}\\cdot u_{r})(e_{j}\\cdot u_{r}) = d_{ij}\n\\] so the two matrices \\(D\\) and \\(P\\Lambda P^{\\intercal}\\) are in fact equal.\nExercises:\n\nProve the rest of the first lemma in Section 2.4.2.\nProve the Gram-Schmidt Process has the claimed properties in Section 2.5.1."
  },
  {
    "objectID": "chapters/03-probability.html#introduction",
    "href": "chapters/03-probability.html#introduction",
    "title": "3  Probability and Bayes Theorem",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nProbability theory is one of the three central mathematical tools in machine learning, along with multivariable calculus and linear algebra. Tools from probability allow us to manage the uncertainty inherent in data collected from real world experiments, and to measure the reliability of predictions that we might make from that data. In these notes, we will review some of the basic terminology of probability and introduce Bayesian inference as a technique in machine learning problems.\nThis will only be a superficial introduction to ideas from probability. For a thorough treatment, see this open-source introduction to probability. For a more applied emphasis, I recommend the excellent online course Probabilistic Systems Analysis and Applied Probability and its associated text [1]."
  },
  {
    "objectID": "chapters/03-probability.html#probability-basics",
    "href": "chapters/03-probability.html#probability-basics",
    "title": "3  Probability and Bayes Theorem",
    "section": "3.2 Probability Basics",
    "text": "3.2 Probability Basics\nThe theory of probability begins with a set \\(X\\) of possible events or outcomes, together with a “probability” function \\(P\\) on (certain) subsets of \\(X\\) that measures “how likely” that combination of events is to occur.\nThe set \\(X\\) can be discrete or continuous. For example, when flipping a coin, our set of possible events would be the discrete set \\(\\{H,T\\}\\) corresponding to the possible events of flipping heads or tails. When measuring the temperature using a thermometer, our set of possible outcomes might be the set of real numbers, or perhaps an interval in \\(\\mathbb{R}\\). The thermometer’s measurement is random because it is affected by, say, electronic noise, and so its reading is the true temperature perturbed by a random amount.\nThe values of \\(P\\) are between \\(0\\), meaning that the event will not happen, and \\(1\\), meaning that it is certain to occur. As part of our set up, we assume that the total chance of some event from \\(X\\) occurring is \\(1\\), so that \\(P(X)=1\\); and the chance of “nothing” happening is zero, so \\(P(\\emptyset)=0\\). And if \\(U\\subset X\\) is some collection, then \\(P(U)\\) is the chance of an event from \\(U\\) occurring.\nThe last ingredient of this picture of probability is additivity. Namely, we assume that if \\(U\\) and \\(V\\) are subsets of \\(X\\) that are disjoint, then \\[\nP(U\\cup V)=P(U)+P(V).\n\\] Even more generally, we assume that this holds for (countably) infinite collections of disjoint subsets \\(U_1,U_2,\\ldots\\), where \\[\nP(U_1\\cup U_2\\cup\\cdots)=\\sum_{i=1}^{\\infty} P(U_i)\n\\]\nDefinition: The combination of a set \\(X\\) of possible outcomes and a probability function \\(P\\) on subsets of \\(X\\) that satisfies \\(P(X)=1\\), \\(0\\le P(U)\\le 1\\) for all \\(U\\), and is additive on countable disjoint collections of subsets of \\(X\\) is called a (naive) probability space. \\(X\\) is called the sample space and the subsets of \\(X\\) are called events.\nWarning: The reason for the term “naive” in the above definition is that, if \\(X\\) is an uncountable set such as the real numbers \\(\\mathbb{R}\\), then the conditions in the definition are self-contradictory. This is a deep and rather surprising fact. To make a sensible definition of a probability space, one has to restrict the domain of the probability function \\(P\\) to certain subsets of \\(X\\). These ideas form the basis of the mathematical subject known as measure theory. In these notes we will work with explicit probability functions and simple subsets such as intervals that avoid these technicalities.\n\n3.2.1 Discrete probability examples\nThe simplest probability space arises in the analysis of coin-flipping. As mentioned earlier, the set \\(X\\) contains two elements \\(\\{H,T\\}\\). The probability function \\(P\\) is determined by its value \\(P(\\{H\\})=p\\), where \\(0\\le p\\le 1\\), which is the chance of the coin yielding a “head”. Since \\(P(X)=1\\), we have \\(P(\\{T\\})=1-p\\).\nOther examples of discrete probability spaces arise from dice-rolling and playing cards. For example, suppose we roll two six-sided dice. There are \\(36\\) possible outcomes from this experiment, each equally likely. If instead we consider the sum of the two values on the dice, our outcomes range from \\(2\\) to \\(12\\) and the probabilities of these outcomes are given by\n\n\n\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n1/36\n1/18\n1/12\n1/9\n5/36\n1/6\n5/36\n1/9\n1/12\n1/18\n1/36\n\n\n\nA traditional deck of \\(52\\) playing cards contains \\(4\\) aces. Assuming that the chance of drawing any card is the same (and is therefore equal to \\(1/52\\)), the probability of drawing an ace is \\(4/52=1/13\\) since \\[\nP(\\{A_{\\clubsuit},A_{\\spadesuit},A_{\\heartsuit},A_{\\diamondsuit}\\}) = 4P(\\{A_{\\clubsuit}\\})=4/52=1/13\n\\]\n\n\n3.2.2 Continuous probability examples\nWhen the set \\(X\\) is continuous, such as in the temperature measurement, we measure \\(P(U)\\), where \\(U\\subset X\\), by giving a “probability density function” \\(f:X\\to \\mathbb{R}\\) and declaring that \\[\nP(U) = \\int_{U}f(x) dX.\n\\] Notice that our function \\(f(x)\\) has to satisfy the condition \\[\nP(X)=\\int_{X} f(x)dX = 1.\n\\]\nFor example, in our temperature measurement example, suppose the “true” outside temperature is \\(t_0\\), and our thermometer gives a reading \\(t\\). Then a good model for the random error is to assume that the error \\(x=t-t_0\\) is governed by the density function \\[\nf_\\sigma(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-x^2/2\\sigma^2}\n\\] where \\(\\sigma\\) is a parameter. In a continuous situation such as this one, the probability of any particular outcome in \\(X\\) is zero since \\[\nP(\\{t\\})=\\int_{t}^{t}f_{\\sigma}(x)dx = 0\n\\] Still, the shape of the density function does tell you where the values are concentrated – values where the density function is larger are more likely than those where it is smaller.\nWith this density function, and x=\\(t-t_0\\), the error in our measurement is given by \\[\nP(|t-t_0|<\\delta)=\\int_{-\\delta}^{\\delta} \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-x^2/2\\sigma^2} dx\n\\tag{3.1}\\]\nThe parameter \\(\\sigma\\) (called the standard deviation) controls how tightly the thermometer’s measurement is clustered around the true value \\(t_0\\); when \\(\\sigma\\) is large, the measurements are scattered widely, when small, they are clustered tightly. See Figure 3.1.\n\n\n\nFigure 3.1: Normal Density"
  },
  {
    "objectID": "chapters/03-probability.html#conditional-probability-and-bayes-theorem",
    "href": "chapters/03-probability.html#conditional-probability-and-bayes-theorem",
    "title": "3  Probability and Bayes Theorem",
    "section": "3.3 Conditional Probability and Bayes Theorem",
    "text": "3.3 Conditional Probability and Bayes Theorem\nThe theory of conditional probability gives a way to study how partial information about an event informs us about the event as a whole. For example, suppose you draw a card at random from a deck. As we’ve seen earlier, the chance that card is an ace is \\(1/13\\). Now suppose that you learn that (somehow) that the card is definitely not a jack, king, or queen. Since there are 12 cards in the deck that are jacks, kings, or queens, the card you’ve drawn is one of the remaining 40 cards, which includes 4 aces. Thus the chance you are holding an ace is now \\(4/40=1/10\\).\nIn terms of notation, if \\(A\\) is the event “my card is an ace” and \\(B\\) is the event “my card is not a jack, queen, or king” then we say that the probability of \\(A\\) given \\(B\\) is \\(1/10\\). The notation for this is \\[\nP(A|B) = 1/10.\n\\]\nMore generally, if \\(A\\) and \\(B\\) are events from a sample space \\(X\\), and \\(P(B)>0\\), then \\[\nP(A|B) = \\frac{P(A\\cap B)}{P(B)},\n\\] so that \\(P(A|B)\\) measures the chance that \\(A\\) occurs among those situations in which \\(B\\) occurs.\n\n3.3.1 Bayes Theorem\nBayes theorem is a foundational result in probability.\nTheorem: Bayes Theorem says \\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}.\n\\]\nIf we use the definition of conditional probability given above, this is straightforward: \\[\n\\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B\\cap A)}{P(B)} = P(A|B).\n\\]\n\n\n3.3.2 An example\nTo illustrate conditional probability, let’s consider what happens when we administer the most reliable COVID-19 test, the PCR test, to an individual drawn from the population at large. There are two possible test results (positive and negative) and two possible true states of the person being tested (infected and not infected). Suppose I go to the doctor and get a COVID test which comes back positive. What is the probability that I actually have COVID?\nLet’s let \\(S\\) and \\(W\\) stand for infected (sick) and not infected (well), and let \\(+/-\\) stand for test positive or negative. Note that there are four possible outcomes of our experiment:\n\ntest positive and infected (S+) – this is a true positive.\ntest positive and not infected (W+) – this is a false positive.\ntest negative and infected (S-) – this is a false negative.\ntest negative and not infected (W-) – this is a true negative.\n\nThe CDC says that the chance of a false positive – that is, the percentage of samples from well people that incorrectly yields a positive result – is about one-half of one percent, or 5 in 1000.\nIn other words, \\[\nP(+|W) = P(W+)/P(W) = 5/1000=1/200\n\\]\nOn the other hand, the CDC tells us that chance of a false negative is 1 in 4, so \\[\nP(-|S) = P(S-)/P(S) = .25.\n\\] Since \\(P(S-)+P(S+)=P(S).\\) since every test is either positive or negative, we have \\[\nP(+|S) = .75.\n\\]\nSuppose furthermore that the overall incidence of COVID-19 in the population is p. In other words, \\(P(S)=p\\) so \\(P(W)=1-p\\). Then \\[P(S+)=P(S)P(+|S)=.75p\\] and \\[\nP(W+)=P(W)P(+|W)=.005(1-p).\n\\] Putting these together we get \\(P(+)=.005+.745p\\)\nWhat I’m interested in is \\(P(S|+)\\) – the chance that I’m sick, given that my test result was positive. By Bayes Theorem, \\[\nP(S|+)=\\frac{P(+|S)P(S)}{P(+)}=.75p/(.005+.745p)=\\frac{750p}{5+745p}.\n\\]\nAs Figure 3.2 shows, if the population incidence is low then a positive test is far from conclusive. Indeed, if the overall incidence of COVID is one percent, then a positive test result only implies a 60 percent chance that I am in fact infected.\nJust to fill out the picture, we have \\[\nP(-) = P(S-)+P(W-)=(P(S)-P(S+))+(P(W)-P(W+))\n\\] which yields \\[\nP(-)=1-.005+.005p-.75p = .995-.745p.\n\\] Using Bayes Theorem, we obtain \\[\nP(S|-) = \\frac{P(-|S)P(S)}{P(-)} = .25p/(.995-.745p) =\\frac{250p}{995-745p}.\n\\] In this case, even though the false negative rate is pretty high (25 percent) overall, if the population incidence is one percent, then the probability that you’re sick given a negative result is only about \\(.25\\) percent. So negative results are very likely correct!\n\n\n\nFigure 3.2: P(S|+) vs P(S)"
  },
  {
    "objectID": "chapters/03-probability.html#independence",
    "href": "chapters/03-probability.html#independence",
    "title": "3  Probability and Bayes Theorem",
    "section": "3.4 Independence",
    "text": "3.4 Independence\nIndependence is one of the fundamental concepts in probability theory. Conceptually, two events are independent if the occurrence of one has does not influence the likelihood of the occurrence of the other. For example, successive flips of a coin are independent events, since the result of the second flip doesn’t have anything to do with the result of the first. On the other hand, whether or not it rains today and tomorrow are not independent events, since the weather tomorrow depends (in a complicated way) on the weather today.\nWe can formalize this idea of independence using the following definition.\nDefinition: Let \\(X\\) be a sample space and let \\(A\\) and \\(B\\) be two events. Then \\(A\\) and \\(B\\) are independent if \\(P(A\\cap B)=P(A)P(B)\\). Equivalently, \\(A\\) and \\(B\\) are independent if \\(P(A|B)=P(A)\\) and \\(P(B|A)=P(B)\\).\n\n3.4.1 Examples\n\n3.4.1.1 Coin Flipping\nSuppose our coin has a probability of heads given by a real number \\(p\\) between \\(0\\) and \\(1\\), and we flip our coin \\(N\\) times. What is the chance of gettting \\(k\\) heads, where \\(0\\le k\\le N\\)? Any particular sequence of heads and tails containing \\(k\\) heads and \\(N-k\\) tails has probability \\[\nP(\\hbox{a particular sequence of $k$ heads among $N$ flips}) = p^{k}(1-p)^{N-k}.\n\\] In addition, there are \\(\\binom{N}{k}\\) sequences of heads and tails containing \\(k\\) heads. Thus the probability \\(P(k,N)\\) of \\(k\\) heads among \\(N\\) flips is \\[\nP(k,N) = \\binom{N}{k}p^{k}(1-p)^{N-k}.\n\\tag{3.2}\\]\nNotice that the binomial theorem gives us \\(\\sum_{k=0}^{N} P(k,N) =1\\) which is a reassuring check on our work.\nThe probability distribution on the set \\(X=\\{0,1,\\ldots,N\\}\\) given by \\(P(k,N)\\) is called the binomial distribution with parameters \\(N\\) and \\(p\\).\n\n\n3.4.1.2 A simple ‘mixture’\nNow let’s look at an example of events that are not independent. Suppose that we have two coins, with probabilities of heads \\(p_1\\) and \\(p_2\\) respectively; and assume these probabilities are different. We play the a game in which we first choose one of the two coins (with equal chance) and then flip it twice. Is the result of the second flip independent of the first? In other words, is \\(P(HH)=P(H)^2\\)?\nThis type of situation is called a ‘mixture distribution’ because the probability of a head is a “mixture” of the probability coming from the two different coins.\nThe chance that the first flip is a head is \\((p_1+p_2)/2\\) because it’s the chance of picking the first coin, and then getting a head, plus the chance of picking the second, and then getting a head. The chance of getting two heads in a row is \\((p_1^2+p_2^2)/2\\) because it’s the chance, having picked the first coin, of getting two heads, plus the chance, having picked the second, of getting two heads.\nSince \\[\n\\frac{p_1^2+p_2^2}{2}\\not=\\left(\\frac{p_1+p_2}{2}\\right)^2\n\\] we see these events are not independent.\nIn terms of conditional probabilities, the chance that the second flip is a head, given that the first flip is, is computed as: \\[\nP(HH|H) = \\frac{p_1^2+p_2^2}{p_1+p_2}.\n\\] From the Cauchy-Schwartz inequality one can show that \\[\n\\frac{p_1^2+p_2^2}{p_1+p_2}>\\frac{p_1+p_2}{2}.\n\\]\nWhy should this be? Why should the chance of getting a head on the second flip go up given that the first flip was a head? One way to think of this is that the first coin flip contains a little bit of information about which coin we chose. If, for example \\(p_1>p_2\\), and our first flip is heads, then it’s just a bit more likely that we chose the first coin. As a result, the chance of getting another head is just a bit more likely than if we didn’t have that information. We can make this precise by considering the conditional probability \\(P(p=p_1|H)\\) that we’ve chosen the first coin given that we flipped a head. From Bayes’ theorem:\n\\[\nP(p=p_1|H) = \\frac{P(H|p=p_1)P(p=p_1)}{P(H)}=\\frac{p_1}{p_1+p_2}=\\frac{1}{1+(p_2/p_1)}>\\frac{1}{2}\n\\] since \\((1+(p_2/p_1))<2\\).\nExercise: Push this argument a bit further. Let \\(p_1=\\max(p_1,p_2)\\) Let \\(P_N\\) be the conditional probability of getting heads assuming that the first \\(N\\) flips were heads. Show that \\(P_N\\to p_1\\) as \\(N\\to\\infty\\). All those heads piling up make it more and more likely that you’re flipping the first coin and so the chance of getting heads approaches \\(p_1\\).\n\n\n3.4.1.3 An example with a continuous distribution\nSuppose that we return to our example of a thermometer which measures the ambient temperature with an error that is distributed according to the normal distribution, as in Equation 3.1. Suppose that we make 10 independent measurements \\(t_1,\\ldots, t_{10}\\) of the true temperature \\(t_0\\). What can we say about the distribution of these measurements?\nIn this case, independence means that \\[\nP=P(|t_1-t_0|<\\delta,|t_2-t_0|<\\delta,\\ldots) = P(|t_1-t_0|<\\delta)P(|t_2-t_0|<\\delta)\\cdots P(|t_{10}-t_{0}|<\\delta)\n\\] and therefore \\[\nP = \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^{10}\\int_{-\\delta}^{\\delta}\\cdots\\int_{-\\delta}^{\\delta}\ne^{-(\\sum_{i=1}^{10} x_i^2)/2\\sigma^2} dx_1\\cdots dx_{10}\n\\]\nOne way to look at this is that the vector \\(\\mathbf{e}\\) of errors \\((|t_1-t_0|,\\ldots,|t_{10}-t_0|)\\) is distributed according to a multivariate gaussian distribution: \\[\nP(\\mathbf{e}\\in U) =\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^{10}\\int_{U}\ne^{-\\|x\\|^2/2\\sigma^2} d\\mathbf{x}\n\\tag{3.3}\\]\nwhere \\(U\\) is a region in \\(\\mathbf{R}^{10}\\).\nThe multivariate gaussian can also describe situations where independence does not hold. For simplicity, let’s work in two dimensions and consider the probability density on \\(\\mathbf{R}^{2}\\) given by \\[\nP(\\mathbf{e}\\in U) = A\\int_{U} e^{-(x_1^2-x_1x_2+x_2^2)/2\\sigma^2} d\\mathbf{x}.\n\\] where the constant \\(A\\) is chosen so that \\[\nA\\int_{\\mathbf{R}^{2}}e^{-(x_1^2-x_1x_2+x_2^2)/2\\sigma^2}d\\mathbf{x} = 1.\n\\]\nThis density function as a “bump” concentrated near the origin in \\(\\mathbf{R}^{2}\\), and its level curves are a family of ellipses centered at the origin. See Figure 3.3 for a plot of this function with \\(\\sigma=1\\).\n\n\n\nFigure 3.3: Multivariate Gaussian\n\n\nIn this situation we can look at the conditional probability of the first variable given the second, and see that the two variables are not independent. Indeed, if we fix \\(x_2\\), then the distribution of \\(x_1\\) depends on our choice of \\(x_2\\). We could see this by a calculation, or we can just look at the graph: if \\(x_2=0\\), then the most likely values of \\(x_1\\) cluster near zero, while if \\(x_2=1\\), then the most likely values of \\(x_1\\) cluster somewhere above zero."
  },
  {
    "objectID": "chapters/03-probability.html#random-variables-mean-and-variance",
    "href": "chapters/03-probability.html#random-variables-mean-and-variance",
    "title": "3  Probability and Bayes Theorem",
    "section": "3.5 Random Variables, Mean, and Variance",
    "text": "3.5 Random Variables, Mean, and Variance\nTypically, when we are studying a random process, we aren’t necessarily accessing the underlying events, but rather we are making measurements that provide us with some information about the underlying events. For example, suppose our sample space \\(X\\) is the set of throws of a pair of dice, so \\(X\\) contains the \\(36\\) possible combinations that can arise from the throws. What we are actually interested is the sum of the values of the two dice – that’s our “measurement” of this system. This rather vague notion of a measurement of a random system is captured by the very general idea of a random variable.\nDefinition: Let \\(X\\) be a sample space with probability function \\(P\\). A random variable on \\(X\\) is a function \\(f:X\\to \\mathbb{R}\\).\nGiven a random variable \\(f\\), we can use the probability measure to decide how likely \\(f\\) is to take a particular value, or values in a particular set by the formula \\[\nP(f(x)\\in U) = P(f^{-1}(U))\n\\]\nIn the dice rolling example, the random variable \\(S\\) that assigns their sum to the pair of values obtained on two dice is a random variable. Those values lie between \\(2\\) and \\(12\\) and we have \\[\nP(S=k) = P(S^{-1}(\\{k\\}))=P(\\{(x,y): x+y=k\\})\n\\] where \\((x,y)\\) runs through \\(\\{1,2,\\ldots,6\\}^{2}\\) representing the two values and \\(P((x,y))=1/36\\) since all throws are equally likely.\nLet’s look at a few more examples, starting with what is probably the most fundamental of all.\nDefinition: Let \\(X\\) be a sample space with two elements, say \\(H\\) and \\(T\\), and suppose that \\(P(H)=p\\) for some \\(0\\le p\\le 1\\). Then the random variable that satisfies \\(f(H)=1\\) and \\(f(T)=0\\) is called a Bernoulli random variable with parameter \\(p\\).\nIn other words, a Bernoulli random variable gives the value \\(1\\) when a coin flip is heads, and \\(0\\) for tails.\nNow let’s look at what we earlier called the binomial distribution.\nDefinition: Let \\(X\\) be a sample space consisting of strings of \\(H\\) and \\(T\\) of length \\(N\\), with the probability of a particular string \\(S\\) with \\(k\\) heads and \\(N-k\\) tails given by \\[\nP(S)=p^{k}(1-p)^{N-k}\n\\] for some \\(0\\le p\\le 1\\). In other words, \\(X\\) is the sample space consisting of \\(N\\) independent flips of a coin with probability of heads given by \\(p\\).\nLet \\(f:X\\to \\mathbb{R}\\) be the function which counts the number of \\(H\\) in the string. We can express \\(f\\) in terms of Bernoulli random variables; indeed, \\[\nf=X_1+\\ldots+X_N\n\\] where each \\(X_i\\) is a Bernoulli random variable with parameter \\(p\\).\nNow \\[\nP(f=k) = \\binom{N}{k}p^{k}(1-p)^{N-k}\n\\] since \\(f^{-1}(\\{k\\})\\) is the number of elements in the subset of strings of \\(H\\) and \\(T\\) of length \\(N\\) containing exactly \\(k\\) \\(H\\)’s. This is our old friend the binomial distribution. So a binomial distribution is the distribution of the sum of \\(N\\) independent Bernoulli random variables.\nFor an example with a continuous random variable, suppose our sample space is \\(\\mathbf{R}^{2}\\) and the probability density is the simple multivariate normal \\[\nP(\\mathbf{x}\\in U) = \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2\\int_{U} e^{-\\|\\mathbf{x}\\|^2/2} d\\mathbf{x}.\n\\] Let \\(f\\) be the random variable \\(f(\\mathbf{x})=\\|\\mathbf{x}\\|\\). The function \\(f\\) measures the Euclidean distance of a randomly drawn point from the origin. The set \\[U=f^{-1}([0,r))\\subseteq\\mathbf{R}^{2}\\] is the circle of radius \\(r\\) in \\(\\mathbf{R}^{2}\\). The probability that a randomly drawn point lies in this circle is \\[\nP(f<r) = \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2\\int_{U} e^{-\\|\\mathbf{x}\\|^2/2} d\\mathbf{x}.\n\\]\nWe can actually evaluate this integral in closed form by using polar coordinates. We obtain \\[\nP(f<r) = \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2\\int_{\\theta=0}^{2\\pi}\\int_{\\rho=0}^{r} e^{-\\rho^2/2}\\rho d\\rho d\\theta.\n\\] Since \\[\n\\frac{d}{d\\rho}e^{-\\rho^2/2}=-\\rho e^{-\\rho^2/2}\n\\] we have \\[\\begin{align*}\nP(f<r)&=-\\frac{1}{2\\pi}\\theta e^{-\\rho^2/2}|_{\\theta=0}^{2\\pi}|_{\\rho=0}^{r}\\cr\n&=1-e^{-r^2/2}\\cr\n\\end{align*}\\]\nThe probability density associated with this random variable is the derivative of \\(1-e^{-r^2/2}\\) \\[\nP(f\\in [a,b])=\\int_{r=a}^{b} re^{-r^2/2} dr\n\\] as you can see by the fundamental theorem of calculus. This density is drawn in Figure 3.4 where you can see that the points are clustered at a distance of \\(1\\) from the origin.\n\n\n\nFigure 3.4: Density of the Norm\n\n\n\n3.5.1 Independence and Random Variables\nWe can extend the notion of independence from events to random variables.\nDefinition: Let \\(f\\) and \\(g\\) be two random variables on a sample space \\(X\\) with probability \\(P\\). Then \\(f\\) and \\(g\\) are independent if, for all intervals \\(U\\) and \\(V\\) in \\(\\mathbb{R}\\), the events \\(f^{-1}(U)\\) and \\(g^{-1}(V)\\) are independent.\nFor discrete probability distributions, this means that, for all \\(a,b\\in\\mathbb{R}\\), \\[\nP(f=a\\hbox{\\ and\\ }g=b)=P(f=a)P(g=b).\n\\]\nFor continous probability distributions given by a density function \\(P(x)\\), independence can be more complicated to figure out.\n\n\n3.5.2 Expectation, Mean and Variance\nThe most fundamental tool in the study of random variables is the concept of “expectation”, which is a fancy version of average. The word “mean” is a synonym for expectation – the mean of a random variable is the same as its expectation or “expected value.”\nDefinition: Let \\(X\\) be a sample space with probability measure \\(P\\). Let \\(f:X\\to \\mathbb{R}\\) be a random variable. Then the expectation or expected value \\(E[f]\\) of \\(f\\) is \\[\nE[f] = \\int_X f(x)dP.\n\\] More specifically, if \\(X\\) is discrete, then \\[\nE[f] = \\sum_{x\\in X} f(x)P(x)\n\\] while if \\(X\\) is continuous with probability density function \\(p(x)dx\\) then \\[\nE[f] = \\int_{X} f(x)p(x)dx.\n\\]\nIf \\(f\\) is a Bernoulli random variable with parameter \\(p\\), then \\[\nE[f] = 1\\cdot p+0\\cdot (1-p) = p\n\\]\nIf \\(f\\) is a binomial random variable with parameters \\(p\\) and \\(N\\), then \\[\nE[f] = \\sum_{i=0}^{N} i\\binom{N}{i}p^{i}(1-p)^{N-i}\n\\] One can evaluate this using some combinatorial tricks, but it’s easier to apply this basic fact about expectations.\nProposition: Expectation is linear: \\(E[aX+bY]=aE[X]+bE[Y]\\) for random variables \\(X,Y\\) and constants \\(a\\) and \\(b\\).\nThe proof is an easy consequence of the expression of \\(E\\) as a sum (or integral).\nSince a binomial random variable \\(Z\\) with parameters \\(N\\) and \\(p\\) is the sum of \\(N\\) Bernoulli random variables, its expectation is \\[\nE[X_1+\\cdots+X_N]=Np.\n\\]\nA more sophisticated property of expectation is that it is multiplicative when the random variables are independent.\nProposition: Let \\(f\\) and \\(g\\) be two independent random variables. Then \\(E[fg]=E[f]E[g]\\).\nProof: Let’s suppose that the sample space \\(X\\) is discrete. By definition, \\[\nE[f]=\\sum_{x\\in X}f(x)P(x)\n\\] and we can rewrite this as \\[\nE[f]=\\sum_{a\\in\\mathbf{R}} aP(\\{x: f(x)=a\\}).\n\\] Let \\(Z\\subset\\mathbb{R}\\) be the range of \\(f\\). Then \\[\\begin{align*}\nE[fg]&=\\sum_{a\\in Z} aP(\\{x: fg(x)=a\\}) \\\\\n&=\\sum_{a\\in Z}\\sum_{(u,v)\\in\\genfrac{}{}{0pt}{}{\\mathbf{Z}^{2}}{uv=a}}aP(\\{x:f(x)=u\\hbox{\\ and\\ }g(x)=v\\}) \\\\\n&=\\sum_{a\\in Z}\\sum_{\\genfrac{}{}{0pt}{}{\\mathbf{Z}^{2}}{uv=a}}uvP(\\{x:f(x)=u\\})P(\\{x:g(x)=v\\}) \\\\\n&=\\sum_{u\\in Z}uP(\\{x:f(x)=u\\})\\sum_{v\\in Z}vP(\\{x:f(x)=v\\}) \\\\\n&=E[f]E[g]\n\\end{align*}\\]\n\n3.5.2.1 Variance\nThe variance of a random variable is a measure of its dispersion around its mean.\nDefinition: Let \\(f\\) be a random variable. Then the variance is the expression \\[\n\\sigma^2(f) = E[(f-E[f])^2]=E[f^2]-(E[f])^2\n\\] The square root of the variance is called the “standard deviation.”\nThe two formulae for the variance arise from the calculation \\[\nE[(f-E[f])^2]=E[(f^2-2fE[f]+E[f]^2)]=E[f^2]-2E[f]^2+E[f]^2=E[f^2]-E[f]^2.\n\\]\nTo compute the variance of the Bernoulli random variable \\(f\\) with parameter \\(p\\), we first compute \\[\nE[f^2]=p(1)^2+(1-p)0^2=p.\n\\] Since \\(E[f]=p\\), we have \\[\n\\sigma^2(f)=p-p^2=p(1-p).\n\\]\nIf \\(f\\) is the binomial random variable with parameters \\(N\\) and \\(p\\), we can again use the fact that \\(f\\) is the sum of \\(N\\) Bernoulli random variables \\(X_1+\\cdots+X_n\\) and compute\n\\[\\begin{align*}\nE[(\\sum_{i}X_i)^2]-E[\\sum_{i} X_{i}]^2 &=E[\\sum_{i} X_i^2+\\sum_{i,j}X_{i}X_{j}]-N^2p^2\\\\\n&=Np+N(N-1)p^2-N^2p^2 \\\\\n&=Np(1-p)\n\\end{align*}\\]\nwhere we have used the fact that the square \\(X^2\\) of a Bernoulli random variable is equal to \\(X\\).\nFor a continuous example, suppose that we consider a sample space \\(\\mathbb{R}\\) with the normal probability density \\[\nP(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-x^2/2\\sigma^2}dx.\n\\]\nThe mean of the random variable \\(x\\) is \\[\nE[x] =\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} xe^{-x^2/2\\sigma^2}dx=0\n\\]\nsince the function being integrated is odd. The variance is\n\\[\nE[x^2] = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} x^2e^{-x^2/2\\sigma^2}dx.\n\\]\nThe trick to evaluating this integral is to consider the derivative:\n\\[\n\\frac{d}{d\\sigma}\\left[\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-x^2/(2\\sigma^2)}dx\\right]=0\n\\]\nwhere the result is zero since the quantity being differentiated is a constant (namely \\(1\\)). Sorting through the resulting equation leads to the fact that\n\\[\nE[x^2]=\\sigma^2\n\\]\nso that the \\(\\sigma^2\\) parameter in the normal distribution really is the variance of the associated random variable."
  },
  {
    "objectID": "chapters/03-probability.html#models-and-likelihood",
    "href": "chapters/03-probability.html#models-and-likelihood",
    "title": "3  Probability and Bayes Theorem",
    "section": "3.6 Models and Likelihood",
    "text": "3.6 Models and Likelihood\nA statistical model is a mathematical model that accounts for data via a process that incorporates random behavior in a structured way. We have seen several examples of such models in our discussion so far. For example, the Bernoulli process that describes the outcome of a series of coin flips as independent choices of heads or tails with probability \\(p\\) is a simple statistical model; our more complicated mixture model in which we choose one of two coins at random and then flip that is a more complicated model.\nOur description of the variation in temperature measurements as arising from perturbations from the true temperature by a normally distributed amount is another example of a statistical model, this one involving a continuous random variable.\nWhen we apply a mathematical model to understand data, we often have a variety of parameters in the model that we must adjust to get the model to best “fit” the observed data. For example, suppose that we observe the vibrations of a block attached to a spring. We know that the motion is governed by a second order linear differential equation, but the dynamics depend on the mass of the block, the spring constant, and the damping coefficient. By measuring the dynamics of the block over time, we can try to work backwards to figure out these parameters, after which we will be able to predict the block’s motion into the future.\n\n3.6.1 Maximum Likelihood (Discrete Case)\nTo see this process in a statistical setting, let’s return to the simple example of a coin flip. The only parameter in our model is the probability \\(p\\) of getting heads on a particular flip. Suppose that we flip the coin \\(100\\) times and get \\(55\\) heads and \\(45\\) tails. What can we say about \\(p\\)?\nWe will approach this question via the “likelihood” function for our data. We ask: for a particular value of the parameter \\(p\\), how likely is this outcome? From Equation 3.2 we have \\[\nP(55H,45T)=\\binom{100}{55}p^{55}(1-p)^{45}.\n\\]\nThis function is plotted in Figure 3.5. As you can see from that plot, it is extremely unlikely that we would have gotten \\(55\\) heads if \\(p\\) was smaller than \\(.4\\) or greater than \\(.7\\), while the most likely value of \\(p\\) occurs at the maximum value of this function, and a little calculus tells us that this point is where \\(p=.55\\). This most likely value of \\(p\\) is called the maximum likelihood estimate for the parameter \\(p\\).\n\n\n\nFigure 3.5: Likelihood Plot\n\n\n\n\n3.6.2 Maximum Likelihood (Continuous Case)\nNow let’s look at our temperature measurements where the error is normally distributed with variance parameter \\(\\sigma^2\\). As we have seen earlier, the probability density of errors \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\) of \\(n\\) independent measurements is \\[\nP(\\mathbf{x}) = \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^{n}e^{-\\|\\mathbf{x}\\|^2/(2\\sigma^2)}d\\mathbf{x}.\n\\] (see Equation 3.3). What should we use as the parameter \\(\\sigma\\)? We can ask which choice of \\(\\sigma\\) makes our data most likely. To calculate this, we think of the probability of a function of \\(\\sigma\\) and use Calculus to find the maximum. It’s easier to do this with the logarithm.\n\\[\n\\log P(\\mathbf{x})=\\frac{-\\|\\mathbf{x}\\|^2}{2\\sigma^2}-n\\log{\\sigma}+C\n\\] where \\(C\\) is a constant that we’ll ignore. Taking the derivative and setting it to zero, we obtain \\[\n-\\|\\mathbf{x}\\|^2\\sigma^{-3}-n\\sigma^{-1}=0\n\\] which gives the formula \\[\n\\sigma^2=\\frac{\\|\\mathbf{x}\\|^2}{n}\n\\]\nThis should look familiar! The maximum likelihood estimate of the variance is the mean-squared-error.\n\n\n3.6.3 Linear Regression and likelihood\nIn our earlier lectures we discussed linear regression at length. Our introduction of ideas from probability give us new insight into this fundamental tool. Consider a statistical model in which certain measured values \\(y\\) depend linearly on \\(x\\) up to a normally distributed error: \\[\ny=mx+b+\\epsilon\n\\] where \\(\\epsilon\\) is drawn from the normal distribution with variance \\(\\sigma^2\\).\nThe classic regression setting has us measuring a collection of \\(N\\) points \\((x_i,y_i)\\) and then asking for the “best” \\(m\\), \\(b\\), and \\(\\sigma^2\\) to explain these measurements. Using the likelihood perspective, each value \\(y_i-mx_i-b\\) is an independent draw from the normal distribution with variance \\(\\sigma^2\\), exactly like our temperature measurements in the one variable case.\nThe likelihood (density) of those draws is therefore \\[\nP = \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^Ne^{-\\sum_{i}(y_i-mx_i-b)^2/(2\\sigma^2)}.\n\\] What is the maximum likelihood estimate of the parameters \\(m\\), \\(b\\), and \\(\\sigma^2\\)?\nTo find this we look at the logarithm of \\(P\\) and take derivatives. \\[\n\\log(P) = -N\\log(\\sigma) -\\frac{1}{2\\sigma^2}\\sum_{i}(y_i-mx_i-b)^2.\n\\]\nAs far as \\(m\\) and \\(b\\) are concerned, the minimum comes from the derivatives with respect to \\(m\\) and \\(b\\) of \\[\n\\sum_{i}(y_i-mx_i-b)^2.\n\\] In other words, the maximum likelihood estimate \\(m_*\\) and \\(b_*\\) for \\(m\\) and \\(b\\) are exactly the ordinary least squares estimates.\nAs far as \\(\\sigma^2\\) is concerned, we find just as above that the maximum likelihood estimate \\(\\sigma^2_*\\) is the mean squared error \\[\n\\sigma^2_*=\\frac{1}{N}\\sum_{i}(y_i-m_*x_i-b_*)^2.\n\\]\nThe multivariate case of regression proposes a model of the form \\[\nY=X\\beta+\\epsilon\n\\] and a similar calculation again shows that the least squares estimates for \\(\\beta\\) are the maximum likelihood values for this model."
  },
  {
    "objectID": "chapters/03-probability.html#bayesian-inference",
    "href": "chapters/03-probability.html#bayesian-inference",
    "title": "3  Probability and Bayes Theorem",
    "section": "3.7 Bayesian Inference",
    "text": "3.7 Bayesian Inference\nWe conclude our review of ideas from probability by examining the Bayesian perspective on data.\nSuppose that we wish to conduct an experiment to determine the temperature outside our house. We begin our experiment with a statistical model that is supposed to explain the variability in the results. The model depends on some parameters that we wish to estimate. For example, the parameters of our experiment might be the ‘true’ temperature \\(t_*\\) and the variance \\(\\sigma^2\\) of the error.\nFrom the Bayesian point of view, at the beginning of this experiment we have an initial sense of what the temperature is likely to be, expressed in the form of a probability distribution. This initial information is called the prior distribution.\nFor example, if we know that it’s December in Connecticut, our prior distribution might say that the temperature is more likely to be between 20 and 40 degrees Fahrenheit and is quite unlikely to be higher than 60 or lower than 0. So our prior distribution might look like Figure 3.6.\n\n\n\nFigure 3.6: Prior Distribution on Temperature\n\n\nIf we really have no opinion about the temperature other than its between say, \\(-20\\) and \\(100\\) degrees, our prior distribution might be uniform over that range, as in Figure 3.7.\n\n\n\nFigure 3.7: Uniform Prior\n\n\nThe choice of a prior will guide the interpretation of our experiments in ways that we will see shortly.\nThe next step in our experiment is the collection of data. Suppose we let \\(\\mathbf{t}=(t_1,t_2,\\ldots, t_n)\\) be a random variable representing \\(n\\) independent measurements of the temperature. We consider the joint distribution of the parameters \\(t_*\\) and \\(\\sigma^2\\) and the possible measurements \\(\\mathbf{t}\\): \\[\nP(\\mathbf{t},t_*,\\sigma^2)=\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)^{n}e^{-\\|\\mathbf{t}-t_*\\mathbf{e}\\|^2/(2\\sigma^2)}\n\\] where \\(\\mathbf{e}=(1,1,\\ldots, 1)\\).\nThe conditional probability \\(P(t_{*},\\sigma^2|\\mathbf{t})\\) is the distribution of the values of \\(t_*\\) and \\(\\sigma^2\\)givena value of the \\(\\mathbf{t}\\). This is what we hope to learn by our experiment – namely, if we make a particular measurement, what does it tell us about \\(t_*\\) and \\(\\sigma^2\\)?\nNow suppose that we actually make some measurements, and so we obtain a specific set of values \\(\\mathbf{t}_0\\) for \\(\\mathbf{t}\\).\nBy Bayes Theorem, \\[\nP(t_{*},\\sigma^2|\\mathbf{t}=\\mathbf{t}_0) = \\frac{P(\\mathbf{t}=\\mathbf{t}_0|t_{*},\\sigma^2)P(t_{*},\\sigma^2)}{P(\\mathbf{t}=\\mathbf{t}_0)}\n\\] We interpret this as follows:\n\nthe left hand side \\(P(t_{*},\\sigma^2|\\mathbf{t}=\\mathbf{t}_0)\\) is called the posterior distribution and is the distribution of \\(t_{*}\\) and \\(\\sigma^2\\) obtained by updating our prior knowledge with the results of our experiment.\nThe probability \\(P(\\mathbf{t}=\\mathbf{t}_{0}|t_{*},\\sigma^2)\\) is the probability of obtaining the measurements we found for a particular value of the parameters \\(t_{*}\\) and \\(\\sigma^2\\).\nThe probability \\(P(t_{*},\\sigma^2)\\) is the prior distribution on the parameters that reflects our initial impression of the distribution of these parameters.\nThe denominator \\(P(\\mathbf{t}=\\mathbf{t}_{0})\\) is the total probability of the results that we obtained, and is the integral over the distribution of the parameters weighted by their prior probability: \\[\nP(\\mathbf{t}=\\mathbf{t}_{0})=\\int_{t_{*},\\sigma^2}P(\\mathbf{t}=\\mathbf{t}_{0}|t_{*},\\sigma^2)P(t_{*},\\sigma^2)\n\\]\n\n\n3.7.1 Bayesian experiments with the normal distribution\nTo illustrate these Bayesian ideas, we’ll consider the problem of measuring the temperature, but for simplicity let’s assume that we fix the variance in our error measurements at \\(1\\) degree. Let’s use the prior distribution on the true temperature that I proposed in Figure 3.6, which is a normal distribution with variance \\(15\\) “shifted” to be centered at \\(30\\): \\[\nP(t_*)=\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)e^{-(t_*-30)^2/30}.\n\\] The expected value \\(E[t]\\) – the mean of the this distribution – is \\(30\\).\nSince the error in our measurements is normally distributed with variance \\(1\\), we have \\[\nP(t-t_{*})=\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)e^{-(t-t_{*})^2/2}\n\\] or as a function of the absolute temperature, we have \\[\nP(t,t_{*}) = \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)e^{-(t-t_*)^2/2}.\n\\]\nNow we make a bunch of measurements to obtain \\(\\mathbf{t}_0=(t_1,\\ldots, t_n)\\). We have \\[\nP(\\mathbf{t}=\\mathbf{t}_0|t_{*}) = \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^ne^{-\\|\\mathbf{t}-t_*\\mathbf{e}\\|^2/2}.\n\\]\nThe total probability \\(T=P(\\mathbf{t}=\\mathbf{t_0})\\) is hard to calculate, so let’s table that for a while. The posterior probability is \\[\nP(t_{*}|\\mathbf{t}=\\mathbf{t}_{0}) = \\frac{1}{T}\n\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^ne^{-\\|\\mathbf{t}-t_*\\mathbf{e}\\|^2/2}\n\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)e^{-(t_*-30)^2/30}.\n\\]\nLeaving aside the multiplicative constants for the moment, consider the exponential \\[\ne^{-(\\|\\mathbf{t}-t_{*}\\mathbf{e}\\|^2/2+(t_{*}-30)^2)/30}.\n\\] Since \\(\\mathbf{t}\\) is a vector of constants – it is a vector of our particular measurements – the exponent \\[\n\\|\\mathbf{t}-t_{*}\\mathbf{e}\\|^2/2+(t_{*}-30)^2/30 = (t_{*}-30)^2/30+\\sum_{i} (t_{i}-t_{*})^2/2\n\\] is a quadratic polynomial in \\(t_{*}\\) that simplifies: \\[\n(t_{*}-30)^2/30+\\sum_{i} (t_{i}-t_{*})^2/2 = At_{*}^2+Bt_{*}+C.\n\\] Here \\[\nA=(\\frac{1}{30}+\\frac{n}{2}),\n\\] \\[\nB=-2-\\sum_{i} t_{i}\n\\] \\[\nC=30+\\frac{1}{2}\\sum_{i} t_{i}^2.\n\\]\nWe can complete the square to write \\[\nAt_{*}^2+Bt_{*}+C = (t_{*}-U)^2/2V +K\n\\] where \\[\nU=\\frac{2+\\sum_{i}t_{i}}{\\frac{1}{15}+n}\n\\] and \\[\nV=\\frac{1}{\\frac{1}{15}+n}.\n\\] So up to constants that don’t involve \\(t_{*}\\), the posterior density is of the form \\[\ne^{(t_{*}-U)^2/2V}\n\\] and since it is a probability density, the constants must work out to give total integral of \\(1\\). Therefore the posterior density is a normal distribution centered at \\(U\\) and with variance \\(V\\). Here \\(U\\) is called theposterior meanand \\(V\\) theposterior variance.\nTo make this explicit, suppose \\(n=5\\) and we measured the following temperatures: \\[\n40, 41,39, 37, 44\n\\] The mean of these observations is \\(40.2\\) and the variance is \\(5.4\\).\nA calculation shows that the posterior mean is \\(40.1\\) and the posterior variance is \\(0.2\\). Comparing the prior with the posterior, we obtain the plot in Figure 3.8. The posterior has a sharp peak at \\(40.1\\) degrees. This value is just a bit smaller than the mean of the observed temperatures which is \\(40.2\\) degrees. This difference is caused by the prior – our prior distribution said the temperature was likely to be around \\(30\\) degrees, and so the prior pulls the observed mean a bit towards the prior mean taking into account past experience. Because the variance of the prior is large, it has a relatively small influence on the posterior.\nThe general version of the calculation above is summarized in this proposition.\nProposition: Suppose that our statistical model for an experiment proposes that the measurements are normally distributed around an (unknown) mean value of \\(\\mu\\) with a (fixed) variance \\(\\sigma^2\\). Suppose further that our prior distribution on the unknown mean \\(\\mu\\) is normal with mean \\(\\mu_0\\) and variance \\(\\tau^2\\). Suppose we make measurements \\[\ny_1,\\ldots, y_n\n\\] with mean \\(\\overline{y}\\). Then the posterior distribution of \\(\\mu\\) is again normal, with posterior variance \\[\n\\tau'^2 = \\frac{1}{\\frac{1}{\\tau^2}+\\frac{n}{\\sigma^2}}\n\\] and posterior mean \\[\n\\mu' = \\frac{\\frac{\\mu_0}{\\tau^2}+\\frac{n}{\\sigma^2}\\overline{y}}{\\frac{1}{\\frac{1}{\\tau^2}+\\frac{n}{\\sigma^2}}}\n\\]\nSo the posterior mean is a sort of weighted average of the sample mean and the prior mean; and as \\(n\\to\\infty\\), the posterior mean approaches the sample mean – in other words, as you get more data, the prior has less and less influence on the results of the experiment.\n\n\n\nFigure 3.8: Prior and Posterior\n\n\n\n\n3.7.2 Bayesian coin flipping\nFor our final example in this fast overview of ideas from probability, we consider the problem of deciding whether a coin is fair. Our experiment consists of \\(N\\) flips of a coin with unknown probability \\(p\\) of heads, so the data consists of the number \\(h\\) of heads out of the \\(N\\) flips. To apply Bayesian reasoning, we need a prior distribution on \\(p\\). Let’s first assume that we have no reason to prefer one value of \\(p\\) over another, and so we choose for our prior the uniform distribution on \\(p\\) between \\(0\\) and \\(1\\).\nWe wish to analyze \\(P(p|h)\\), the probability distribution of \\(p\\) given \\(h\\) heads out of \\(N\\) flips. Bayes Theorem gives us: \\[\nP(p|h) = \\frac{P(h|p)P(p)}{P(h)}\n\\] where \\[\nP(h|p) = \\binom{N}{h}p^{h}(1-p)^{N-h}\n\\] and \\[\nP(h)=\\int_{p=0}^{1} P(h|p)P(p) dp = \\binom{N}{h}\\int_{p=0}^{1} p^{h}(1-p)^{N-h}dp\n\\] is a constant which insures that \\[\\int_{p}P(p|h)dp=1.\\]\nWe see that the posterior distribution \\(P(p|h)\\) is proportional to the polynomial function \\[\nP(p|h)\\propto p^{h}(1-p)^{N-h}.\n\\] As in Section 3.6.1, we see that this function peaks at \\(h/N\\). This is called the maximum a posteriori estimate for \\(p\\).\nAnother way to summarize the posterior distribution \\(P(p|h)\\) is to look at the expected value of \\(p\\). This is called the posterior mean of \\(p\\). To compute it, we need to know the normalization constant in the expression for \\(P(p|h)\\), and for that we can take advantage of the properties of a special function \\(B(a,b)\\) called the Beta-function: \\[\nB(a,b) = \\int_{p=0}^{1} p^{a-1}(1-p)^{b-1} dp.\n\\]\nProposition: If \\(a\\) and \\(b\\) are integers, then \\(B(a,b)=\\frac{a+b}{ab}\\frac{1}{\\binom{a+b}{a}}\\).\nProof: Using integration by parts, one can show that \\[\nB(a,b)=\\frac{a-1}{b}B(a-1,b+1)\n\\] and a simple calculation shows that \\[\nB(1,b) = \\frac{1}{b}.\n\\] Let \\[\nH(a,b)=\\frac{a+b}{ab}\\frac{1}{\\binom{a+b}{a}} = \\frac{(a-1)!(b-1)!}{(a+b-1)!}\n\\] Then it’s easy to check that \\(H\\) satsifies the same recurrences as \\(B(a,b)\\), and that \\(H(1,b)=1/b\\). So the two functions agree by induction.\nUsing this Proposition, we see that \\[\nP(p|h) = \\frac{p^{h}(1-p)^{N-h}}{B(h+1,N-h+1)}\n\\] and \\[\nE[p] = \\frac{\\int_{p=0}^{1} p^{h+1}(1-p)^{N-h}dp}{B(h+1,N-h+1)}=\\frac{B(h+2,N-h+1)}{B(h+1,N-h+1)}.\n\\] Sorting through this using the formula for \\(B(a,b)\\) we obtain \\[\nE[p]=\\frac{h+1}{N+2}.\n\\]\nSo if we obtained \\(55\\) heads out of \\(100\\) flips, the maximum a posteriori estimate for \\(p\\) is \\(.55\\), while the posterior mean is \\(56/102=.549\\) – just a bit less.\nNow suppose that we had some reason to believe that our coin was fair. Then we can choose a prior probability distribution that expresses this. For example, we can choose \\[\nP(p) = \\frac{1}{B(5,5)}p^{4}(1-p)^{4}.\n\\] Here we use the Beta function to guarantee that \\(\\int_{0}^{1}P(p)dp=1\\). We show this prior distribution in Figure 3.9.\n\n\n\nFigure 3.9: Beta(5,5) Prior\n\n\nIf we redo our Bayes theorem calculation, we find that our posterior distribution is \\[\nP(p|h) \\propto p^{h+4}(1-p)^{N-h+4}\n\\] and relying again on the Beta function for normalization we have \\[\nP(p|h) = \\frac{1}{B(h+5,N-h+5)}p^{h+4}(1-p)^{N-h+4}\n\\] Here the maximum a posterior estimate for \\(p\\) is \\(h+4/N+8\\) while our posterior mean is \\[\n\\frac{B(h+6,N-h+5)}{B(h+5,N-h+5)} = \\frac{h+5}{N+10}.\n\\]\nIn the situation of \\(55\\) heads out of \\(100\\), the maximum a posteriori estimate is \\(.546\\) and the posterior mean is \\(.545\\). These numbers have been pulled just a bit towards \\(.5\\) because our prior knowledge makes us a little bit biased towards \\(p=.5\\).\n\n\n3.7.3 Bayesian Regression (or Ridge Regression)\nIn this chapter we return to our discussion of linear regression and introduce some Bayesian ideas. The combination will lead us to the notion of “ridge” regression, which is a type of linear regression that includes a prior distribution on the coefficients that indicates our preference for smaller rather than larger coefficients. Introduction of this prior leads to a form of linear regression that is more resilient in situations where the independent variables are less independent than we would hope.\nBefore introducing these Bayesian ideas, let us recall from Section 3.6.3 that ordinary least squares yields the parameters that give the “most likely” set of parameters for a model of the form \\[\nY=XM + \\epsilon\n\\] where the error \\(\\epsilon\\) is normally distributed with mean \\(0\\) and variance \\(\\sigma^2\\), and the mean squared error becomes the maximum likelihood estimate of the variance \\(\\sigma^2\\).\nTo put this into a Bayesian perspective, we notice that the linear regression model views \\(Y-XM\\) as normally distributed given \\(M\\). That is, we see the probability \\(P(Y-XM|M)\\) as normal with variance \\(\\sigma^2\\).\nThen we introduce a prior distribution on the coefficients \\(M\\), assuming that they, too, are normally distributed around zero with variance \\(\\tau^2\\). This means that ab initio we think that the coefficients are likely to be small.\nFrom Bayes Theorem, we then have \\[\nP(M|Y,X) = \\frac{P(Y,X|M)P(M)}{P(Y,X)}\n\\] and in distribution terms we have \\[\nP(M|Y,X) = Ae^{\\|Y-XM\\|^2/\\sigma^2}e^{-\\|M\\|^2/\\tau^{2}}\n\\] where \\(A\\) is a normalizing constant.\nThe first thing to note from this expression is that the posterior distribution for the \\(M\\) parameters for regression are themselves normally distributed.\nThe maximum likelihood estimate \\(M_{r}\\) for the parameters \\(M\\) occurs when \\(P(M|Y,X)\\) is maximum, which we find by taking the derivatives. Using the matrix algebra developed in our linear regression chapter, we obtain the equation \\[\n(X^{\\intercal}Y-(X^{\\intercal}X)M_r)/\\sigma^2-M_r/\\tau^{2}=0\n\\] or \\[\n(X^{\\intercal}X+s)M_r=X^{\\intercal}Y\n\\tag{3.4}\\] where \\(s=\\sigma^2/\\tau^2\\).\nTherefore the ridge coefficients are given by the equation \\[\nM_{r}=(X^{\\intercal}X+s)^{-1}X^{\\intercal}Y\n\\tag{3.5}\\]\n\n3.7.3.1 Practical aspects of ridge regression\nUsing ridge regression leads to a solution to the least squares problem in which the regression coefficients are biased towards being smaller. Beyond this, there are a number of implications of the technique which affect its use in practice.\nFirst, we can put the Bayesian derivation of the ridge regression formulae in the background and focus our attention on Equation 3.5. We can treat the parameter \\(s\\) (which must be non-negative) as “adjustable”.\nOne important consideration when using ridge regression is that Equation 3.5 is not invariant if we scale \\(X\\) and \\(Y\\) by a constant. This is different from “plain” regression where we consider the equation \\(Y=XM\\). In that case, rescaling \\(X\\) and \\(Y\\) by the same factor leaves the coefficients \\(M\\) alone. For this reason, ridge regression is typically used on centered, standardized coordinates. In other words, we replace each feature \\(x_i\\) by \\((x_i-\\mu_i)/\\sigma_i\\) where \\(\\mu_i\\) and \\(\\sigma_i\\) are the sample mean and standard deviation of the \\(i^{th}\\) feature, and we replace our response variables \\(y_i\\) similarly by \\((y-\\mu)/\\sigma\\) where \\(\\mu\\) and \\(\\sigma\\) are the mean and standard deviation of the \\(y\\)-values. Then we find \\(M\\) using Equation 3.5, perhaps experimenting with different values of \\(s\\), using our centered and standardized variables.\nTo emphasize that we are using centered coordinates, we write \\(X_{0}\\) for our data matrix instead of \\(X\\). Recall that the matrix \\(X_0^{\\intercal}X_0\\) that enters into Equation 3.4 is \\(ND_{0}\\) where \\(D_{0}\\) is the covariance matrix. Therefore in ridge regression we have replaced \\(ND_0\\) by \\(ND_0+s\\). Since \\(D_0\\) is a real symmetric matrix, as we’ve seen in Chapter 2 it is diagonalizable so that \\(AD_0A^{-1}\\) is diagonal for an orthogonal matrix \\(A\\) and has eigenvalues \\(\\lambda_1\\ge \\ldots\\ge \\lambda_k\\) which are the variances of the data along the principal directions.\nOne effect of using ridge regression is that the eigenvalues of \\(ND_{0}+s\\) are always at least \\(s>0\\), so the use of ridge regression avoids the possibility that \\(D_{0}\\) might not be invertible. In fact, a bit more is true. Numerical analysis tells us that when considering the problem of computing the inverse of a matrix, we should look at its condition number, which is the ratio \\(\\lambda_1/\\lambda_k\\) of the largest to the smallest eigenvalue.\nIf the condition number of a matrix is large, then results from numerical analysis show that it is almost singular and its inverse becomes very sensitive to small changes in the entries of the matrix. However, the eigenvalues of \\(ND_0+s\\) are \\(N\\lambda_{i}+s\\) and so the condition number becomes \\((N\\lambda_1+s)/(N\\lambda_k+s)\\). For larger values of \\(\\lambda\\), this condition number shrinks, and so the inverse of the matrix \\(ND_0+s\\) becomes better behaved than \\(ND_{0}\\). In this way, ridge regression helps to improve the numerical stability of the linear regression algorithm.\nA second way to look at Ridge regression is to go back to the discussion of the singular value decomposition of the matrix \\(X_{0}\\) in section Section 2.4.2. There we showed that the SVD of \\(X_{0}\\) yields an expression \\[\nX_{0}=U\\tilde{\\Lambda}P^{\\intercal}\n\\] where \\(U\\) and \\(P\\) are orthogonal matrices and \\(\\Lambda\\) is an \\(N\\times k\\) matrix whose upper block is diagonal with eigenvalues \\(\\sqrt{N\\lambda_{i}}\\). The rows of \\(U\\) gave us an orthonormal basis that allowed us to write the predicted vector \\(\\hat{Y}\\) as a projection: \\[\n\\hat{Y}=\\sum_{i=1}^{k} (u_j\\cdot Y)u_{j}^{\\intercal}.\n\\]\nIf we repeat this calculation, but using the ridge regression formula, we obtain \\[\n\\hat{Y}_{r}=X_{0}M_r = U\\tilde{\\Lambda}P^{\\intercal}(P\\tilde{\\Lambda}^{\\intercal}U^{\\intercal}U\\tilde{\\Lambda}P^{\\intercal}+s)^{-1}P\\tilde{\\Lambda}^{\\intercal}U^{\\intercal}Y.\n\\] Since \\(P\\) is orthogonal, \\(P^{\\intercal}=P^{-1}\\), so \\[\nP^{\\intercal}(P\\tilde{\\Lambda}^2P^{\\intercal}+s)^{-1}P=P^{-1}(P(\\Lambda+s)P^{-1})P=(\\Lambda+s)^{-1}\n\\] and \\(\\Lambda+s\\) is a \\(k\\times k\\) diagonal matrix with entries \\(N\\lambda_{i}+s\\).\nPutting the pieces together we see that \\[\n\\hat{Y}_{r}=U\\tilde{\\Lambda}(\\Lambda+s)^{-1}\\tilde{\\Lambda}U^{\\intercal}Y.\n\\]\nIn the language of orthogonal projection, this means that \\[\n\\hat{Y}_{r} = \\sum_{i=1}^{k} \\frac{N\\lambda_{i}}{N\\lambda_{i}+s}(u_j\\cdot Y)u_{j}^{\\intercal}.\n\\]\nIn other words, the predicted value computed by ridge regression is obtained by projecting \\(Y\\) into the space spanned by the feature vectors, but weighting the different principal components by \\(N\\lambda_{i}/(N\\lambda_{i}+s)\\). With this weighting, the principal components with smaller variances are weighted less than those with larger variances. For this reason, ridge regression is sometimes called a shrinkage method.\n\n\n\n\n[1] Bertsekas, D. P. and Tsitsiklis, J. N. (2008). Introduction to probability. Athena Scientific."
  },
  {
    "objectID": "chapters/04-naive-bayes.html#introduction",
    "href": "chapters/04-naive-bayes.html#introduction",
    "title": "4  The Naive Bayes classification method",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn our discussion of Bayes Theorem, we looked at a situation in which we had a population consisting of people infected with COVID-19 and people not infected, and we had a test that we could apply to determine which class an individual belonged to. Because our test was not 100 percent reliable, a positive test result didn’t guarantee that a person was infected, and we used Bayes Theorem to evaluate how to interpret the positive test result. More specifically, our information about the test performance gave us the the conditional probabilities of positive and negative test results given infection status – so for example we were given \\(P(+|\\mathrm{infected})\\), the chance of getting a positive test assuming the person is infected – and we used Bayes Theorem to determine \\(P(\\mathrm{infected}|+)\\), the chance that a person was infected given a positive test result.\nThe Naive Bayes classification method is a generalization of this idea. We have data that belongs to one of two classes, and based on the results of a series of tests, we wish to decide which class a particular data point belongs to. For one example, we are given a collection of product reviews from a website and we wish to classify those reviews as either “positive” or “negative.” This type of problem is called “sentiment analysis.” For another, related example, we have a collection of emails or text messages and we wish to label those that are likely “spam” emails. In both of these examples, the “test” that we will apply is to look for the appearance or absence of certain key words that make the text more or less likely to belong to a certain class. For example, we might find that a movie review that contains the word “great” is more likely to be positive than negative, while a review that contains the word “boring” is more likely to be negative.\nThe reason for the word “naive” in the name of this method is that we will derive our probabilities from empirical data, rather than from any deeper theory. For example, to find the probability that a negative movie review contains the word “boring”, we will look at a bunch of reviews that our experts have said are negative, and compute the proportion of those that contain the word boring. Indeed, to develop our family of tests, we will rely on a training set of already classified data from which we can determine estimates of probabilities that we need."
  },
  {
    "objectID": "chapters/04-naive-bayes.html#an-example-dataset",
    "href": "chapters/04-naive-bayes.html#an-example-dataset",
    "title": "4  The Naive Bayes classification method",
    "section": "4.2 An example dataset",
    "text": "4.2 An example dataset\nTo illustrate the Naive Bayes algorithm, we will work with the “Sentiment Labelled Sentences Data Set” ([1]). This dataset contains 3 files, each containing 1000 documents labelled \\(0\\) or \\(1\\) for “negative” or “positive” sentiment. There are 500 of each type of document in each file. One file contains reviews of products from amazon.com; one contains yelp restaurant reviews, and one contains movie reviews from imdb.com.\nLet’s focus on the amazon reviews data. Here are some samples:\nSo there is no way for me to plug it in here \n    in the US unless I go by a converter.   0\nGood case, Excellent value. 1\nGreat for the jawbone.  1\nTied to charger for conversations lasting more than \n    45 minutes.MAJOR PROBLEMS!! 0\nThe mic is great.   1\nI have to jiggle the plug to get it to line up right to \n    get decent volume.  0\nIf you have several dozen or several hundred contacts, then \n    imagine the fun of sending each of them one by one. 0\nIf you are Razr owner...you must have this! 1\nNeedless to say, I wasted my money. 0\nWhat a waste of money and time!.    0\nAs you can see, each line consists of a product review followed by a \\(0\\) or \\(1\\); in this file the review is separated from the text by a tab character."
  },
  {
    "objectID": "chapters/04-naive-bayes.html#bernoulli-tests",
    "href": "chapters/04-naive-bayes.html#bernoulli-tests",
    "title": "4  The Naive Bayes classification method",
    "section": "4.3 Bernoulli tests",
    "text": "4.3 Bernoulli tests\nWe will describe the “Bernoulli” version of a Naive Bayes classifier for this data. The building block of this method is a test based on a single word. For example, let’s consider the word great among all of our amazon reviews. It turns out that great occurs \\(5\\) times in negative reviews and \\(92\\) times in positive reviews among our \\(1000\\) examples. So it seems that seeing the word great in a review makes it more likely to be positive. The appearances of great are summarized in Table 4.1 . We write ~great for the case where great does not appear.\n\n\nTable 4.1: Ocurrences of great by type of review\n\n\n\n+\n-\ntotal\n\n\n\n\ngreat\n92\n5\n97\n\n\n~great\n408\n495\n903\n\n\ntotal\n500\n500\n1000\n\n\n\n\nIn this data, positive and negative reviews are equally likely so \\(P(+)=P(-)=.5\\) From this table, and Bayes Theorem, we obtain the empirical probabilities (or “naive” probabilities).\n\\[\nP(\\mathbf{great} | +) = \\frac{92}{500} = .184\n\\]\nand\n\\[\nP(\\mathbf{great}) = \\frac{97}{1000} = .097\n\\]\nTherefore\n\\[\nP(+|\\mathbf{great}) = \\frac{.184}{.097}(.5) = 0.948.\n\\]\nIn other words, if you see the word great in a review, there’s a 95% chance that the review is positive.\nWhat if you do not see the word great? A similar calculation from the table yields\n\\[\nP(+|\\sim\\mathbf{great}) = \\frac{408}{903} = .452\n\\]\nIn other words, not seeing the word great gives a little evidence that the review is negative (there’s a 55% chance it’s negative) but it’s not that conclusive.\nThe word waste is associated with negative reviews. It’s statistics are summarized in Table 4.2 .\n\n\nTable 4.2: Ocurrences of waste by type of review\n\n\n\n+\n-\ntotal\n\n\n\n\nwaste\n0\n14\n14\n\n\n~waste\n500\n486\n986\n\n\ntotal\n500\n500\n1000\n\n\n\n\nBased on this data, the “naive” probabilities we are interested in are:\n\\[\\begin{align*}\nP(+|\\mathbf{waste}) &= 0\\\\\nP(+|\\sim\\mathbf{waste}) &= .51\n\\end{align*}\\]\nIn other words, if you see waste you definitely have a negative review, but if you don’t, you’re only slightly more likely to have a positive one.\nWhat about combining these two tests? Or using even more words? We could analyze our data to count cases in which both great and waste occur, in which only one occurs, or in which neither occurs, within the two different categories of reviews, and then use those counts to estimate empirical probabilities of the joint events. But while this might be feasible with two words, if we want to use many words, the number of combinations quickly becomes huge. So instead, we make a basic, and probably false, assumption, but one that makes a simple analysis possible.\nAssumption: We assume that the presence or absence of the words great and waste in a particular review (positive or negative) are independent events. More generally, given a collection of words \\(w_1,\\ldots, w_k\\), we assume that their occurences in a given review are independent events.\nIndependence means that we have \\[\\begin{align*}\nP(\\mathbf{great},\\mathbf{waste}|\\pm) &= P(\\mathbf{great}|\\pm)P(\\mathbf{waste}|\\pm)\\\\\nP(\\mathbf{great},\\sim\\mathbf{waste}|\\pm) &= P(\\mathbf{great}|\\pm)P(\\sim\\mathbf{waste}|\\pm)\\\\\n&\\vdots \\\\\n\\end{align*}\\]\nSo for example, if a document contains the word great and does not contain the word waste, then the probability of it being a positive review is: \\[\nP(+|\\mathbf{great},\\sim\\mathbf{waste}) = \\frac{P(\\mathbf{great}|+)P(\\sim\\mathbf{waste}|+)P(+)}{P(\\mathbf{great},\\sim\\mathbf{waste})}\n\\] while the probability of it being a negative review is \\[\nP(-|\\mathbf{great},\\sim\\mathbf{waste}) = \\frac{P(\\mathbf{great}|-)P(\\sim\\mathbf{waste}|-)P(-)}{P(\\mathbf{great},\\sim\\mathbf{waste})}\n\\] Rather than compute these probabilities (which involves working out the denominators), let’s just compare them. Since they have the same denominators, we just need to compare numerators, which we call \\(L\\) for likelihood: Using the data from Table 4.1 and Table 4.2 , we obtain: \\[\nL(+|\\mathbf{great},\\sim\\mathbf{waste}) = (.184)(1)(.5) = .092\n\\] and \\[\nL(-|\\mathbf{great},\\sim\\mathbf{waste}) = (.01)(.028)(.5) = .00014\n\\] so our data suggests strongly that this is a positive review."
  },
  {
    "objectID": "chapters/04-naive-bayes.html#feature-vectors",
    "href": "chapters/04-naive-bayes.html#feature-vectors",
    "title": "4  The Naive Bayes classification method",
    "section": "4.4 Feature vectors",
    "text": "4.4 Feature vectors\nTo generalize this, suppose that we have extracted keywords \\(w_1,\\ldots, w_k\\) from our data and we have computed the empirical values \\(P(w_{i}|+)\\) and \\(P(w_{i}|-)\\) by counting the fraction of positive and negative reviews that contain the word \\(w_{i}\\):\n\\[\nP(w_{i}|\\pm) = \\frac{\\hbox{ number of $\\pm$ reviews that mention $w_{i}$}}{\\hbox{ number of $\\pm$ reviews total}}\n\\]\nNotice that we only count reviews, not ocurrences, so that if a word occurs multiple times in a review it only contributes 1 to the count. That’s why this is called the Bernoulli Naive Bayes – we are thinking of each keyword as yielding a yes/no test on each review.\nGiven a review, we look to see whether each of our \\(k\\) keywords appears or does not. We encode this information as a vector of length \\(k\\) containing \\(0\\)’s and \\(1\\)’s indicating the absence or presence of the \\(k\\)th keyword. Let’s call this vector the feature vector for the review.\nFor example, if our keywords are \\(w_1=\\mathbf{waste}\\), \\(w_2=\\mathbf{great}\\), and \\(w_3=\\mathbf{useless}\\), and our review says\nThis phone is useless, useless, useless!  What a waste!\nthen the associated feature vector is \\(f=(1,0,1)\\).\nFor the purposes of classification of our reviews, we are going to forget entirely about the text of our reviews and work only with the feature vectors. From an abstract perspective, then, by choosing our \\(k\\) keywords, our “training set” of \\(N\\) labelled reviews can be replaced by an \\(N\\times k\\) matrix \\(X=(x_{ij})\\) with entries \\(0\\) or \\(1\\), where \\(x_{ij}=1\\) if and only if the \\(j^{th}\\) keyword appears in the \\(i^{th}\\) review.\nThe labels of \\(0\\) or \\(1\\) for unfavorable or favorable reviews can also be packaged up into a \\(N\\times 1\\) vector \\(Y\\) that serves as our “target” variable.\nSetting things up this way lets us express the computations of our probabilities \\(P(w_{i}|\\pm)\\) in vector form. In fact, \\(Y^{\\intercal}X\\) is the sum of the rows of \\(X\\) corresponding to positive reviews, and therefore, letting \\(N_{\\pm}\\) denote the number of \\(\\pm\\) reviews, \\[\nP_{+} = \\frac{1}{N_{+}}Y^{\\intercal}X = \\left[\\begin{array}{cccc} P(w_{1}|+)& P(w_{2}|+) & \\cdots &P(w_{k}|+)\\end{array}\\right].\n\\] Similarly, since \\(Y\\) and \\(X\\) have zero and one entries only, if we write \\(1-Y\\) and \\(1-X\\) for the matrices obtained by replacing every entry \\(z\\) by \\(1-z\\) in each matrix, we have: \\[\nP_{-} = \\frac{1}{N_{-}}(1-Y)^{\\intercal}X =  \\left[\\begin{array}{cccc} P(w_{1}|-)& P(w_{2}|-) & \\cdots &P(w_{k}|-)\\end{array}\\right].\n\\]\nNote that the number of positive reviews is \\(N_{+}=Y^{\\intercal}Y\\) and the number of negative ones is \\(N_{-}=N-N_{+}\\). Since \\(P(+)\\) is the fraction of positive reviews among all reviews, we can compute it as \\(P(+)=\\frac{1}{N}Y^{\\intercal}Y\\), and \\(P(-)=1-P(+)\\)."
  },
  {
    "objectID": "chapters/04-naive-bayes.html#likelihood",
    "href": "chapters/04-naive-bayes.html#likelihood",
    "title": "4  The Naive Bayes classification method",
    "section": "4.5 Likelihood",
    "text": "4.5 Likelihood\nIf a review has an associated feature vector \\(f=(f_1,\\ldots, f_k)\\), then by independence the probability of that feature vector ocurring within one of the \\(\\pm\\) classes is \\[\nP(f|\\pm) = \\prod_{i: f_{i}=1} P(w_{i}|\\pm)\\prod_{i: f_{i}=0}(1-P(w_{i}|\\pm))\n\\] which we can also write \\[\nP(f|\\pm) = \\prod_{i=1}^{k} P(w_{i}|\\pm)^{f_{i}}(1-P(w_{i}|\\pm))^{(1-f_{i})}.\n\\tag{4.1}\\]\nThese products aren’t practical to work with – they are often the product of many, many small numbers and are therefore really tiny. Therefore it’s much more practical to work with their logarithms. \\[\n\\log P(f|\\pm) = \\sum_{i=1}^{k} f_{i}\\log P(w_{i}|\\pm) + (1-f_{i})\\log(1-P(w_{i}|\\pm))\n\\tag{4.2}\\]\nIf we have a group of reviews \\(N\\) organized in a matrix \\(X\\), where each row is the feature vector associated to the corresponding review, then we can compute all of this at once. We’ll write \\(\\log P_{\\pm}=\\log P(X|\\pm)\\) as the row vector whose \\(i^{th}\\) entry is \\(\\log P(f_{i}|\\pm)\\):\n\\[\n\\log P(X|\\pm) = X(\\log P_{\\pm})^{\\intercal}+(1-X)(\\log (1-P_{\\pm}))^{\\intercal}.\n\\tag{4.3}\\]\nBy Bayes Theorem, we can express the chance that our review with feature vector \\(f\\) is positive or negative by the formula: \\[\n\\log P(\\pm|f) = \\log P(f|\\pm)+\\log P(\\pm) - \\log P(f)\n\\] where \\[\nP(\\pm) = \\frac{\\hbox{ the number of $\\pm$ reviews}}{\\hbox{ total number of reviews}}\n\\] and \\(P(f)\\) is the fraction of reviews with the given feature vector. (Note: in practice, some of these probabilities will be zero, and so the log will not be defined. A common practical approach to dealing with this is to introduce a “fake document” into both classes in which every vocabulary word appears – this guarantees that the frequency matrix will have no zeros in it).\nA natural classification rule would be to say that a review is positive if \\(\\log P(+|f)>\\log P(-|f)\\), and negative otherwise. In applying this, we can avoid computing \\(P(f)\\) by just comparing \\(\\log P(f|+)+\\log P(+)\\) and \\(\\log P(f|-)+\\log P(-)\\) computed using Equation 4.2. Then we say:\n\na review is positive if \\(\\log P(f|+)+\\log P(+)>\\log P(f|-)+\\log P(-)\\) and negative otherwise.\n\nAgain we can exploit the matrix structure to do this for a bunch of reviews at once. Using Equation 4.3 and the vectors \\(P_{\\pm}\\) we can compute column vectors corresponding to both sides of our decision inequality and subtract them. The positive entries indicate positive reviews, and the negative ones, negative reviews."
  },
  {
    "objectID": "chapters/04-naive-bayes.html#the-bag-of-words",
    "href": "chapters/04-naive-bayes.html#the-bag-of-words",
    "title": "4  The Naive Bayes classification method",
    "section": "4.6 The Bag of Words",
    "text": "4.6 The Bag of Words\nIn our analysis above, we thought of the presence or absence of certain key words as a set of independent tests that provided evidence of whether our review was positive or negative. This approach is suited to short pieces of text, but what about longer documents? In that case, we might want to consider not just the presence or absence of words, but the frequency with which they appear. Multinomial Naive Bayes, based on the “bag of words” model, is a classification method similar to Bernoulli Naive Bayes but which takes term frequency into account.\nLet’s consider, as above, the problem of classifying documents into one of two classes. We assume that we have a set of keywords \\(w_1,\\ldots, w_k\\). For each class \\(\\pm\\), we have a set of probabilities \\(P(w_i|\\pm)\\) with the property that \\[\n\\sum_{i=1}^{k}P(w_{i}|\\pm)=1.\n\\]\nThe “bag of words” model says that we construct a document of length \\(N\\) in, say, the \\(+\\) class by independently drawing a word \\(N\\) times from the set \\(w_1,\\ldots, w_k\\) with probabilities \\(P(w_{i}|+)\\). The name “bag of words” comes from thinking of each class as having an associated bag containing the words \\(w_1,\\ldots, w_k\\) with relative frequencies given by the probabilities, and generating a document by repeatedly drawing a word from the bag.\nIn the Multinomial Naive Bayes method, we estimate the probabilities \\(P(w_{i}|\\pm)\\) by counting the number of times each word occurs in a document of the given class: \\[\nP(w_{i}|\\pm) = \\frac{\\hbox{ number of times word $i$ occurs in $\\pm$ documents}}{\\hbox{ total number of words in $\\pm$ documents}}\n\\] This is the “naive” part of the algorithm. Package up these probabilities in vectors: \\[\nP_{\\pm} = \\left[\\begin{array}{ccc} P(w_{1}|\\pm) & \\cdots & P(w_{k}|\\pm)\\end{array}\\right].\n\\]\nAs in the Bernoulli case, we often add a fake document to each class where all of the words occur once, in order to avoid having zero frequencies when we take a logarithm later.\nNow, given a document, we associate a feature vector \\(\\mathbf{f}\\) whose \\(i^{th}\\) entry is the frequency with which word \\(i\\) appears in that document. The probability of obtaining a particular document with feature vector \\(\\mathbf{f}=(f_1,\\ldots, f_k)\\) from the bag of words associated with class \\(\\pm\\) is given by the “multinomial” distribution: \\[\nP(\\mathbf{f}|\\pm)=\\frac{N!}{f_1!f_2!\\cdots f_k!} \\prod_{i=1}^{k} P(w_{i}|\\pm)^{f_{i}}\n\\] which generalizes the binomial distribution to multiple choices. The constant will prove irrelevant, so let’s call the interesting part \\(L_{\\pm}\\): \\[\nL(\\mathbf{f}|\\pm)= \\prod_{i=1}^{k} P(w_{i}|\\pm)^{f_{i}}\n\\]\nFrom Bayes Theorem, we have \\[\nP(\\pm|\\mathbf{f}) = \\frac{P(\\mathbf{f}|\\pm)P(\\pm)}{P(\\mathbf{f})}\n\\] where \\(P(\\pm)\\) is estimated by the fraction of documents (total) in each class.\nWe classify our document by considering \\(P(\\pm|\\mathbf{f})\\) and concluding:\n\na document with feature vector \\(\\mathbf{f}\\) is in class \\(+\\) if \\(\\log P(+|\\mathbf{f})>\\log P(-|\\mathbf{f})\\).\n\nIn this comparison, both the constant (the multinomial coefficient) and the denominator cancel out, so we only need to compare \\(\\log L(\\mathbf{f}|+)+\\log P(+)\\) with \\(\\log L(\\mathbf{f}|-)+\\log P(-)\\) We have \\[\n\\log L(\\mathbf{f}|\\pm) = \\sum_{i=1}^{k} f_{i}\\log P(w_{i}|\\pm)\n\\] or, in vector form, \\[\n\\log P(\\mathbf{f}|\\pm) = \\mathbf{f}\\log P_{\\pm}^{\\intercal}\n\\]\nTherefore, just as in the Bernoulli case, we can package up our document \\(i\\) as an \\(N\\times k\\) data matrix \\(X\\), where position \\(ij\\) gives the number of times word \\(j\\) occurs in document \\(i\\). Then we can compute the vector \\[\n\\hat{Y} = X\\log P_{+}^{\\intercal} + \\log P(+)-X\\log P_{-}^{\\intercal} - \\log P(-)\n\\] and assign those documents where \\(\\hat{Y}>0\\) to the \\(+\\) class and the rest to the \\(-\\) class."
  },
  {
    "objectID": "chapters/04-naive-bayes.html#other-applications",
    "href": "chapters/04-naive-bayes.html#other-applications",
    "title": "4  The Naive Bayes classification method",
    "section": "4.7 Other applications",
    "text": "4.7 Other applications\nWe developed the Naive Bayes method for sentiment analysis, but once we chose a set of keywords our training data was reduced to an \\(N\\times k\\) matrix \\(X\\) of \\(0/1\\) entries, together with an \\(N\\times 1\\) target column vector \\(Y\\). Then our classification problem is to decide whether a given vector of \\(k\\) entries, all \\(0\\) or \\(1\\), is more likely to carry a \\(0\\) or \\(1\\) label. All of the parameters we needed for Naive Bayes – the various probabilities – can be extracted from the matrix \\(X\\).\nFor example, suppose we have a collection of images represented as black/white pixels in a grid that belong to one of two classes. For example, we might have \\(28x28\\) bitmaps of handwritten zeros and ones that are labelled, and we wish to construct a classifier that can decide whether a new \\(28x28\\) bitmap is a zero or one. An example of such a bitmap is given in Figure 4.1. We can view each \\(28x28\\) bitmap as a vector of length \\(784\\) with \\(0/1\\) entries and apply the same approach outlined above. However, there are other methods that are more commonly used for this problem, such as logistic regression and neural networks.\n\n\n\nFigure 4.1: Handwritten 0\n\n\n\n\n\n\n[1] U.C. Irvine ML Repository. Sentiment Labelled Sentences Data Set.Available at https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences."
  },
  {
    "objectID": "chapters/05-1-gradient-descent.html#introduction",
    "href": "chapters/05-1-gradient-descent.html#introduction",
    "title": "5  Gradient Descent",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nA common mathematical theme throughout machine learning is the problem of finding the minimum or maximum value of a function. For example, in linear regression, we find the “best-fitting” linear function by identifying the parameters that minimize the mean squared error. In principal component analysis, we try to identify the scores which have the greatest variation for the given set of data, and for this we needed to maximize a function using Lagrange multipliers. In later lectures, we will see many more examples where we construct the “best” function for a particular task by minimizing some kind of error between our constructed function and the true observed values.\nIn our discussion of PCA and linear regression, we were able to give analytic formulae for the solution to our problems. These solutions involved (in the case of linear regression) inverting a matrix, and in the case of PCA, finding eigenvalues and eigenvectors. These are elegant mathematical results, but at that time we begged the question of how to actually compute these quantities of interest in an efficient way. In this section, we will discuss the technique known as gradient descent, which is perhaps the simplest approach to minimizing a function using calculus, and which is at the foundation of many practical machine learning algorithms."
  },
  {
    "objectID": "chapters/05-1-gradient-descent.html#the-key-idea",
    "href": "chapters/05-1-gradient-descent.html#the-key-idea",
    "title": "5  Gradient Descent",
    "section": "5.2 The Key Idea",
    "text": "5.2 The Key Idea\nSuppose that we have a function \\(f(x_0,\\ldots, x_{k-1})\\) and we wish to find its minimum value. In Calculus classes, we are taught to take the derivates of the function and set them equal to zero, but for anything other than the simplest functions this problem is not solvable in practice. In real life, we use iterative methods to find the minimum of the function \\(f\\).\nThe main tool in this approach is a fact from multivariate calculus.\nProposition: Let \\(f(x_0,\\ldots, x_{k-1})\\) be a function and let \\(\\nabla f\\) be its gradient. Then at each point \\(x\\) in \\(\\R^{k}\\), the gradient \\((\\nabla f)(x)\\) is a vector that points in the direction in which \\(f\\) is increasing most rapidly from \\(x\\) and \\((-\\nabla f)(x)\\) points in the direction in which \\(f\\) is decreasing most rapidly. If \\(\\nabla f=0\\) at \\(x\\) then \\(x\\) is a critical point of \\(f\\).\nThis fact arises from thinking about the directional derivative of a function.\nThe directional derivative \\(D_{v}f\\) measures the rate of change of \\(f\\) as one moves with velocity vector \\(v\\) from the point \\(x\\) and it is defined as \\[\nD_{v}f(x) = \\frac{d}{dt}f(x+tv)|_{t=0}\n\\] From the chain rule, we can compute that \\[\nD_{v}f(x) = \\sum_{i=0}^{k-1} \\frac{\\partial f}{\\partial x_{i}}\\frac{dx_{i}}{dt} = (\\nabla f)\\cdot v\n\\] where \\[\n\\nabla f = \\left[\\frac{\\partial f}{\\partial x_{i}}\\right]_{i=0}^{k-1}\n\\] is the gradient of \\(f\\).\nThe directional derivative \\(D_{v}(f)=(\\nabla f)\\cdot v\\) measures the rate of change of \\(f\\) if we travel with velocity \\(v\\) from a point \\(x\\). To remove the dependence on the magnitude of \\(v\\) (since obviously \\(f\\) will change more quickly if we travel more quickly in a given direction), we scale \\(v\\) to be a unit vector. Then, since \\[\n\\nabla f\\cdot v=\\|\\nabla f\\|\\|v\\|\\cos\\theta=\\|\\nabla f\\|\\cos \\theta\n\\] where \\(\\theta\\) is the angle between \\(v\\) and \\(\\nabla f\\), the dot product giving the rate is maximized when \\(v\\) is parallel to \\(\\nabla f\\). If \\(v\\) is opposite to \\(\\nabla f\\), the dot product is minimized."
  },
  {
    "objectID": "chapters/05-1-gradient-descent.html#the-algorithm",
    "href": "chapters/05-1-gradient-descent.html#the-algorithm",
    "title": "5  Gradient Descent",
    "section": "5.3 The Algorithm",
    "text": "5.3 The Algorithm\nTo exploit the fact that the gradient points in the direction of most rapid increase of our function \\(f\\), we adopt the following strategy. Starting from a point \\(x\\), compute the gradient \\(\\nabla f\\) of \\(f\\). Take a small step in the direction of the gradient – that should increase the value of \\(f\\). Then do it again, and again; each time, you move in the direction of increasing \\(x\\), but at some point the gradient becomes very small and you stop moving much. At that moment, you quit. This is called “gradient ascent.”\nIf we want to minimize, not maximize, our function, then we want to move opposite to the gradient in small steps. This is the more common formulation.\n\nAlgorithm 5.1 (Gradient Descent Algorithm) Given a function \\(f:\\mathbb{R}^{k}\\to \\mathbb{R}\\), to find a point where it is mimized, choose:\n\na starting point \\(c^{(0)}\\),\na small constant \\(\\nu\\) (called the learning rate)\nand a small constant \\(\\epsilon\\) (the tolerance).\n\nIteratively compute \\[\nc^{(n+1)}=c^{(n)} -\\nu\\nabla f(c^{(n)})\n\\] until \\(|c^{(n+1)}-c^{(n)}|<\\epsilon\\).\nThen \\(c^{(n+1)}\\) is an (approximate) critical point of \\(f\\).\n\n\n\n\nFigure 5.1: Gradient Descent Illustrated\n\n\nThe behavior of gradient descent, at least when all goes well, is illustrated in Figure 5.1 for the function \\[\nf(x,y) = 1.3e^{-2.5((x-1.3)^2+(y-0.8)^2))}-1.2e^{-2((x-1.8)^2)+(y-1.3)^2)}.\n\\] Figure 5.1 is a contour plot, with the black lines at constant height and the colors indicating the height of the function. This function has two “pits” or “wells” indicated by the darker, “cooler” colored regions. The red line shows the path that the gradient descent algorithm takes, from a higher, “hotter” region to a lower cooler one.\nTo get a little more perspective on gradient descent, consider the one-dimensional case, with \\[\nf(x)=3x^4+4x^3-12x^2+5.\n\\] This is a quartic polynomial whose graph has two local minima and a local maximum, depicted in Figure 5.2.\n\n\n\nFigure 5.2: A quartic polynomial\n\n\nIn this case the gradient is just the derivative \\[\nf'(x)=12x^3+12x^2-24x\n\\] and the iteration is \\[\nc^{(n+1)} = c^{(n)}-12\\nu((c^{(n)})^3+(c^{(n)})^2-2c^{(n)}).\n\\]\nFrom this simple example we can see the power and also the pitfalls of this method. Suppose we choose \\(x_0=.5\\), \\(\\nu=.01\\), and do \\(30\\) iterations of the main loop in our algorithm. The result is shown in Figure 5.3 .\n\n\n\nFigure 5.3: Gradient descent to a local minimum\n\n\nAs we hope, the red dots quickly descend to the bottom of the “valley” at the point \\(x=1\\). However, this valley is only a local minimum of the function; the true minimum is at \\(x=-2\\). Gradient descent can’t see that far away point and so we don’t find the true minimum of the function. One way to handle this is to run gradient descent multiple times with random starting coordinates and then look for the minimum value it finds among all of these tries.\nGradient descent can fail more spectacularly if we choose an unfortunate combination of learning rate and starting point."
  },
  {
    "objectID": "chapters/05-logistic-regression.html#likelihood-and-logistic-regression",
    "href": "chapters/05-logistic-regression.html#likelihood-and-logistic-regression",
    "title": "6  Logistic Regression",
    "section": "6.1 Likelihood and Logistic Regression",
    "text": "6.1 Likelihood and Logistic Regression\nIn applications, our goal is to choose the parameters of a logistic model to accurately predict the likelihood of the event under study occurring as a function of the measured parameter. Let’s imagine that we collected the data that we generated above, without knowing that it’s source was a logistic model. So Table 6.1 shows the number of times the event occurred, for each of the measured values of the \\(x\\) parameter.\n\n\nTable 6.1: Sample Data\n\n\n\\(x\\)\n-3\n-2\n-1\n0\n1\n2\n3\n\n\n\n\nOccurrences (out of 100)\n10\n18\n38\n50\n69\n78\n86\n\n\n\n\nOur objective now is to find a logistic model which best explains this data. Concretely, we need to estimate the coefficients \\(a\\) and \\(b\\) that yield \\[\np(x) = \\frac{1}{1+e^{-ax-b}}\n\\tag{6.2}\\]\nwhere the resulting probabilities best estimate the data. As we have seen, this notion of “best” can have different interpretations. For example, we could approach this from a Bayesian point of view, adopt a prior distribution on the parameters \\(a\\) and \\(b\\), and use the data to obtain this prior and obtain a posterior distribution on \\(a\\) and \\(b\\). For this first look at logistic regression, we will instead adopt a “maximum likelihood” notion of “best” and ask what is the most likely choice of \\(a\\) and \\(b\\) to yield this data.\nTo apply the maximum likelihood approach, we need to ask “for (fixed, but unknown) values of \\(a\\) and \\(b\\), what is the likelihood that a logistic model with those parameters would yield the data we have collected?” Each column in Table 6.1 represents \\(100\\) Bernoulli trials with a fixed probability \\(p(x)\\). So, for example, the chance \\(q\\) of obtaining \\(10\\) positive results with \\(x=-3\\) is given by \\[\nq(-3)=C p(-3)^{10}(1-p(-3))^{90}\n\\] where \\(C\\) is a constant (it would be a binomial coefficient). Combining this for different values of \\(x\\), we see that the likelihood of the data is the product \\[\nL(a,b) = C' p(-3)^{10}(1-p(-3))^{90}p(-2)^{18}(1-p(-2))^{82}\\cdots p(3)^{86}(1-p(3))^{14}\n\\] where \\(C'\\) is another constant. Each \\(p(x)\\) is a function of the parameters \\(a\\) and \\(b\\), so all together this is a function of those two parameters. Our goal is to maximize it.\nOne step that simplifies matters is to consider the logarithm of the likelihood: \\[\n\\log L (a,b)= \\sum_{i=0}^{6} \\left[ x_{i}\\log(p(x_{i})) + (100-x_{i})\\log(1-p(x_{i}))\\right] +C''\n\\] where \\(C''\\) is yet another constant. Since our ultimate goal is to maximize this, the value of \\(C''\\) is irrelevant and we can drop it.\n\n6.1.1 Another point of view on logistic regression\nIn Table 6.1 we summarize the results of our experiments in groups by the value of the \\(x\\) parameter. We can think of the data somewhat differently, by instead considering each event separately, corresponding to a parameter value \\(x\\) and an outcome \\(0\\) or \\(1\\). From this point of view the data summarized in Table 6.1 would correspond to a vector with \\(700\\) rows. The first \\(100\\) rows (corresponding to the first column of the table) would have first entry \\(-3\\), the next \\(100\\) would have \\(-2\\), or so on. So our parameter values form a vector \\(X\\). Meanwhile, the outcomes form a vector \\(Y\\) with entries \\(0\\) or \\(1\\).\nMore generally, imagine we are studying our advertising data and, for each potential customer, we record how many times they saw our ad. We create a vector \\(X\\) whose entries are these numbers. Then we create another vector \\(Y\\), of the same length, whose entries are either \\(0\\) or \\(1\\) depending of whether or not the customer purchased our product.\nOne way to think about logistic regression in this setting is that we are trying to fit a function that, given the value \\(x_i\\), tries to yield the corresponding value \\(y_i\\). However, instead of finding a deterministic function, as we did in linear regression, instead we try to fit a logistic function that captures the likelihood that the \\(y\\)-value is a \\(1\\) given the \\(x\\)-value. This “curve-fitting perspective” is why this is considered a regression problem.\nIf, as above, we think of each row of the matrix as an independent trial, then the chance that \\(y_i=1\\) is \\(p(x_i)\\) and the chance that \\(y_i=0\\) is \\(1-p(x_i)\\), where \\(p(x)\\) is given by the logistic function as in Equation 6.2. The likelihood of the results we obtained is therefore: \\[\nL(a,b) = C \\prod_{i=0}^{N-1} p(x_i)^{y_i}(1-p(x_i))^{(1-y_i)}\n\\] where \\(C\\) is a constant and we are exploiting the trick that, since \\(y_i\\) is either zero or one, \\(1-y_i\\) is correspondingly one or zero. Thus only \\(p(x_i)\\) or \\(1-p(x_i)\\) occurs in each term of the product. If we group the terms according to \\(x_i\\) we obtain our earlier formula for \\(L(a,b)\\).\nThis expresssion yields an apparently similar formula for the log-likelihood (up to an irrelevant constant): \\[\n\\log L(X,a,b) = \\sum_{i=0}^{N-1} y_i\\log p(x_i) + (1-y_i)\\log (1-p(x_i)).\n\\] Using vector notation, this can be further simplified, where again we drop irrelevant constants: \\[\n\\log L(X,a,b) = Y\\cdot\\log p(X) + (1-Y)\\cdot \\log(1-p(X)).\n\\] To be absolutely concrete, in this formula, \\(p(X)\\) is a vector \\[\np(X)=[p(x_i)]_{i=0}^{N-1} = \\left[\\frac{1}{1+e^{-ax_i-b}} \\right]_{i=0}^{N-1}\n\\] so its entries are functions of the unknown parameters \\(a\\) and \\(b\\).\nWe might naively try to maximize this by taking the derivatives with respect to \\(a\\) and \\(b\\) and setting them to zero, but this turns out to be impractical. So we need a different approach to finding the parameters \\(a\\) and \\(b\\) which maximize this likelihood function. We will return to this problem later, but before we do so we will look at some generalizations and broader applications of the logistic model.\n\n\n6.1.2 Logistic regression with multiple features\nThe next generalization we can consider of the logistic model is the situation where the log-odds of our event of interest depend linearly on multiple parameters. In other words, we have \\[\n\\log\\frac{p}{1-p} = m_0 x_0 + m_1 x _1 + \\cdots + m_{k-1} x_{k-1} + b\n\\] where the \\(a_i\\) and \\(b\\) are constants. Under this model, notice that the incremental effects of changes to the different parameters \\(x_i\\) have independent effects on the probability. So, for example, if \\(x_1\\) were the number of times our potential customer saw an online advertisement and \\(x_2\\) were the number of times they saw a print advertisement, by adopting this model we are assuming that the impact of seeing more online ads is completely unrelated to the impact of seeing more print ads.\nThe probability is again given by a sigmoid function \\[\np(x_1,\\ldots, x_k) = \\frac{1}{1+e^{-\\sum_{i=0}^{k-1} m_i x_i +b}}\n\\]\nThis model has an \\(N\\times k\\) feature matrix whose rows are the values \\(x_0,\\ldots, x_{k-1}\\) for each sample. The outcome of our experimemt is recorded in an \\(N\\times 1\\) column vector \\(Y\\) whose entries are \\(0\\) or \\(1\\). The likelihood function is formally equivalent to what we computed in the case of a single feature, but it will be useful to be a bit careful about vector notation.\nFollowing the same pattern we adopted for linear regression, let \\(X\\) be the \\(N\\times (k+1)\\) matrix whose first \\(k\\) columns contain the values \\(x_i\\) for each sample, and whose last column is all \\(1\\). Rename the “intercept” variable as \\(a_{k+1}\\) and organize these parameters into a \\((k+1)\\times 1\\) matrix \\(M\\). Then \\[\np(X)=\\sigma(XM)\n\\] and our likelihood becomes \\[\n\\log L(M) = Y\\cdot \\log\\sigma(XM) + (1-Y)\\cdot(1-\\log\\sigma(XM)).\n\\tag{6.3}\\]"
  },
  {
    "objectID": "chapters/05-logistic-regression.html#finding-the-maximum-likelihood-solution-by-gradient-descent",
    "href": "chapters/05-logistic-regression.html#finding-the-maximum-likelihood-solution-by-gradient-descent",
    "title": "6  Logistic Regression",
    "section": "6.2 Finding the maximum likelihood solution by gradient descent",
    "text": "6.2 Finding the maximum likelihood solution by gradient descent\nGiven a set of features \\(X\\) and targets \\(Y\\) for a logistic model, we now want to find the values \\(M\\) so that the log-likelihood of the model for those paramters, given the data, is maximized. While in linear regression we could find a nice closed form solution to this problem, the presence of the non-linear function \\(\\sigma(x)\\) in the likelihood makes that impossible for logistic regression. Thus we need to use a numerical approximation. The most straightforward such method is called gradient descent. It is at the foundation of many numerical optimization algorithms, and so while we will develop it here for logistic regression we will have other opportunities to apply it and we will discuss it more thoroughly on its own later."
  },
  {
    "objectID": "chapters/05-logistic-regression.html#gradient-descent-and-logistic-regression",
    "href": "chapters/05-logistic-regression.html#gradient-descent-and-logistic-regression",
    "title": "6  Logistic Regression",
    "section": "6.3 Gradient Descent and Logistic Regression",
    "text": "6.3 Gradient Descent and Logistic Regression\nWe can use gradient descent, as discussed in Chapter 5, to find the maximum likelihood set of parameters for our logistic model. As we saw earlier, in Equation 6.3, we have the log likelihood function \\[\n\\log L(M) = Y\\cdot \\log\\sigma(XM) + (1-Y)\\cdot\\log(1-\\sigma(XM))\n\\] where \\(Y\\) are the target \\(0/1\\) values, \\(X\\) is our \\(N\\times (k+1)\\) data matrix whose last column is all ones, and \\(M\\) is the \\(k+1\\times 1\\) column vector of unknown parameters. Since gradient descent is naturally a minimizing algorithm, we will minimize the function \\(-L(M)\\).\nThe key piece of information that we need is the gradient \\(-\\nabla L\\), where the variables are the entries of \\(M\\). The complicating features is the presence of the nonlinear function \\(\\sigma\\), so let’s start with a simple observation about this function.\nLemma: The logistic function \\(\\sigma(x)\\) satisfies the differential equation \\[\n\\frac{d\\sigma}{dx} = \\sigma(x)(1-\\sigma(x)).\n\\]\nProof: Since \\[\n\\sigma(x)= \\frac{1}{1+e^{-x}},\n\\] \\[\n1-\\sigma(x) = \\frac{e^{-x}}{1+e^{-x}}.\n\\] Then we calculate \\[\\begin{aligned}\n\\frac{d\\sigma}{dx}&=\\left(\\frac{1}{(1+e^{-x})}\\right)^2e^{-x} \\\\\n                  &= \\left(\\frac{1}{1+e^{-x}}\\right)\\left(\\frac{e^{-x}}{1+e^{-x}}\\right)\\\\\n                  &=\\sigma(x)(1-\\sigma(x)) \\\\\n\\end{aligned}\n\\] which is what we claimed.\nWe apply this differential equation to compute the gradient of \\(L\\).\n\nTheorem 6.1 Proposition: The gradient \\(-\\nabla L(M)\\) is given by \\[\n-\\nabla \\log L(M) = X^{\\intercal}(\\sigma(XM)-Y).\n\\] Notice that the right side of this equation yields a \\((k+1)\\times 1\\) column vector. The entries of this vector are the partial derivatives with respect to the coefficients \\(m_{i}\\) for \\(i=0,\\ldots, k\\).\n\nProof: This is yet another exercise in the chain rule and keeping track of indices. Let’s first look at the term \\(Y\\cdot \\log\\sigma(XM)\\). Writing it out, we have \\[\nY\\cdot \\log\\sigma(XM)=\\sum_{i=0}^{N-1}y_{i}\\log\\sigma(\\sum_{j=0}^{k}x_{ij}m_{j}).\n\\] Applying \\(\\partial/\\partial m_{s}\\) to this yields \\[\n\\sum_{i=0}^{N-1}y_{i}(1-\\sigma(\\sum_{j=0}^{k}x_{ij}m_{j}))x_{is}\n\\] where we’ve used the chain rule and the differential equation for \\(\\sigma\\) discussed above. At the same time, we can apply \\(\\partial/\\partial m_{s}\\) to the second term \\((1-Y)\\cdot\\log(1-\\sigma(XM))\\) and obtain \\[\n-\\sum_{i=0}^{N-1}(1-y_{i})\\sigma(\\sum_{j=0}^{k}x_{ij}m_{j})x_{is}.\n\\] The term \\(\\sum_{i=0}^{N-1} y_{i}\\sigma(\\sum_{j=0}^{k}x_{ij}m_{j})x_{is}\\) cancels, yielding \\[\n\\frac{\\partial L(M)}{m_{s}} = -\\sum_{i=0}^{N-1} (y_{i}-\\sigma(\\sum_{j=0}^{k}x_{ij}m_{j}))x_{is}.\n\\]\nSince our weights \\(M\\) are naturally a \\((k+1)\\times 1\\) column vector, looked at properly this is our desired formula: \\[\n-\\nabla \\log L(M) = X^{\\intercal}(\\sigma(XM)-Y).\n\\] Since the right side is an \\((k+1)\\times N\\) matrix times an \\(N\\times 1\\) column vector, the result is a \\((k+1)\\times 1\\) column vector whose entries are the partial derivatives of \\(-\\log L(M)\\) with respect to the weights \\(m_{s}\\).\n\n6.3.1 Gradient Descent on our synthetic data\nNow we can apply gradient descent to find a maximum likelihood logistic model for the sample data that we generated from the logistic model and reported in Table 6.1. With the probability given as \\[\np(x) = \\frac{1}{1+e^{-ax-b}}\n\\] we make an initial guess of \\(a=1\\) and \\(b=0\\) set a learning rate \\(\\nu=.001\\), and run the gradient descent algorithm for \\(30\\) iterations. We plot the negative log-likelihood for this algorithm one the left in Figure 6.2, where we see that it drops swiftly to a minimum value. The corresponding parameter values are \\(a=.6717\\) and \\(b=-.0076\\), and the fit of the the corresponding logistic curve to the observed data is shown on the right in Figure 6.2.\n\n\n\nFigure 6.2: Max Likelihood Gradient Descent for Logistic Fitting\n\n\nThe parameters used to generate the data are close to this; they were \\(a=log(2)=\\).6931$ and \\(b=0\\).\n\n\n6.3.2 Gradient Descent and Logistic Regression on “real” data\nWe conclude this first look at logistic regression and gradient descent by analyzing some simple real data. This dataset consists of about \\(2200\\) customers who patronize a certain food store. Among the features in the data set is a field giving the total dollars spent at the store by a customer; we will study that feature and its relationship to the question of whether or not the customer accepted a special offer from the store. (see [1] for the original data source).\n\n\n\nFigure 6.3: Food Marketing Data: Histograms of Expenditures and Response\n\n\nThe two plots in Figure 6.3 summarize the data. The first plot is a histogram showing the amounts spent by the customers; the second shows the distribution of responses.\nWe would like to know how expenditures increase the likelihood of customers accepting our offer. We therefore fit a logistic model to the data. The result is shown in Figure 6.4.\n\n\n\nFigure 6.4: Logistic Model for Food Marketing"
  },
  {
    "objectID": "chapters/05-logistic-regression.html#logistic-regression-and-classification",
    "href": "chapters/05-logistic-regression.html#logistic-regression-and-classification",
    "title": "6  Logistic Regression",
    "section": "6.4 Logistic Regression and classification",
    "text": "6.4 Logistic Regression and classification\nBeyond the kind of probability prediction that we have discussed up to this point, logistic regression is one of the most powerful techniques for attacking the classification problem. Let’s start our discussion with a sample problem that is a simplified version of one of the most famous machine learning benchmark problems, the MNIST (Modified National Institute of Science and Technology) dataset of handwritten numerals. This dataset consists of \\(60000\\) labelled grayscale images of handwritten digits from \\(0\\) to \\(9\\). Each image is stored as a \\(28x28\\) array of integers from \\(0\\) to \\(255\\). Each cell of the array corresponds to a “pixel” in the image, and the contents of that cell is a grayscale value. See [2] for the a more detailed description of how the dataset was constructed.\nIn Figure 6.5 is a picture of a handwritten “1” from the MNIST dataset.\n\n\n\nFigure 6.5: Handwritten One from MNIST\n\n\nClassification Problem for MNIST: Given a \\(28x28\\) array of grayscale values, determine which digit is represented.\nAt first glance, this does not look like a logistic regression problem. To make the connection clearer, let’s simplify the problem and imagine that our database contains only labelled images of zeros and ones – we’ll worry about how to handle the full problem later. So now our task is to determine which images are zeros, and which are ones.\nOur approach will be to view each image as a vector of length \\(784=28*28\\) by stringing the pixel values row by row into a one dimensional vector, which following our conventions yields a matrix of size \\(N\\times 784\\) where \\(N\\) is the number of images. Since we may also need an “intercept”, we add a column of \\(1\\)’s to our images yielding a data matrix \\(X\\) of size \\(N\\times 785\\). The labels \\(y\\) form a column vector of size \\(N\\) containing zeros and ones.\nWe will also simplify the data but converting the gray-scale images to monochrome by converting gray levels up to \\(128\\) as “white” and beyond \\(128\\) as “black”.\nThe logistic regression approach asks us to find the “best” vector \\(M\\) so that, for a given image vector \\(x\\) (extended by adding a one at the end), the function \\[\np(x)=\\frac{1}{1+e^{-xM}}\n\\] is close to \\(1\\) if \\(x\\) represents a one, and is close to zero if \\(x\\) represents zero. Essentially we think of \\(p(x)\\) as giving the probability that the vector \\(x\\) represents an image of a one. If we want a definite choice, then we can set a threshold value \\(p_0\\) and say that the image \\(x\\) is a one if \\(p(x)>p_0\\) and zero otherwise. The natural choice of \\(p_0=.5\\) amounts to saying that we choose the more likely of the two options under the model.\nSince we are applying the logistic model we are assuming:\n\nthat the value of each pixel in the image contributes something towards the chance of the total image being one;\nand the different pixels have independent, and additive effects on the odds of getting a one.\n\nIf we take this point of view, then we can ask for the vector \\(M\\) that is most likely to account for the labellings, and we can use our maximum likelihood gradient descent method to find \\(M\\).\nThis approach is surprisingly effective. With the MNIST zeros and ones, and the gradient descent method discussed above, one can easily find \\(M\\) so that the logistic model predicts the correct classification with accuracy in the high 90% range.\n\n6.4.1 Weights as filters\nOne interesting aspect of using logistic regression on images for classification is that the we can interpret the optimum set of coefficients \\(M\\) as a kind of filter for our images. Remember that \\(M\\) is a vector with \\(785\\) entries, the last of which is an “intercept”.\nThe logistic model says that, for an image vector \\(x\\), the log-odds that the image is a one is given by \\[\n\\log \\frac{p}{1-p} = \\sum_{i=0}^{783} M_{i}x_{i} + M_{784}.\n\\] This means that if the value of \\(M_{i}\\) is positive, then large values in the \\(i^{th}\\) pixel increase the chance that our image is a one; while if \\(M_{i}\\) is negative, large values decrease the chance. If \\(M_{i}\\) is negative, the reverse is true. However, the values \\(x_{i}\\) are the gray scale “darkness” of the image, so the entries of \\(M\\) emphasize or de-emphasize dark pixels according to whether that dark pixel is more or less likely to occur in a one compared to a zero.\nThis observation allows us to interpret the weights \\(M\\) as a kind of “filter” for the image. In fact, if we rescale the entries of \\(M\\) (omitting the intercept) so that they lie between \\(0\\) and \\(255\\), we can arrange them as a \\(28\\times 28\\) array and plot them as an image. The result of doing this for a selection of MNIST zeros and ones is shown on the left in Figure 6.6. The red (or positive) weights in the middle of the image tell us that if those pixels are dark, the image is more likely to be a one; the blue (or negative) weights scattered farther out tell us that if those pixels are dark, the image is more likely to be a zero.\n\n\n\nFigure 6.6: Rescaled weights (blue is negative). Top: 0 vs 1. Bottom: 3 vs 8.\n\n\nWhat’s important to notice here is that we did not design this “filter” by hand, based on our understanding of the differences between a handwritten zero and one; instead, the algorithm “learned” the “best” filter to optimize its ability to distinguish these digits.\nHere’s another example. Suppose we redo the MNIST problem above, but we try to distinguish 3’s from 8’s.\nWe have about 4500 of each digit, and we label the 3’s with zero and the 8’s with one. Then we use our maximum likelihood optimization. In this case, the filter is shown on the bottom in Figure 6.6."
  },
  {
    "objectID": "chapters/05-logistic-regression.html#multiclass-logistic-regression",
    "href": "chapters/05-logistic-regression.html#multiclass-logistic-regression",
    "title": "6  Logistic Regression",
    "section": "6.5 Multiclass Logistic Regression",
    "text": "6.5 Multiclass Logistic Regression\nOne could attack the problem of classifying the ten distinct classes of digits by, for example, labelling all of the zeros as class zero and everything else as class one, and finding a set of weights that distinguishes zero from everything else. Then, in turn, one could do the same for each of the other digits. Given an unknown image, this would yield a set of probabilities from which one could choose the most likely class. This type of classification is called “one vs rest”, for obvious reasons. It seems more natural, however, to construct a model that, given an image, assigns a probability that it belongs to each of the different possibilities. It is this type of multiclass logistic regression that we will study now.\nOur goal is to build a model that, given an unknown image, returns a vector of ten probabilities, each of which we can interpret as the chance that our unknown image is in fact of a particular digit. If we know the image’s class, then it’s probability vector should be nine zeros with a single one in the position corresponding to the digit. So, for example, if our image is of a two, then the vector of probabilities\n\\[\n\\left[ \\begin{matrix} p_0 & p_1 & p_2 &\\cdots & p_8 & p_9\\\\\\end{matrix}\\right]=\\left[\\begin{matrix} 0 &0 & 1 & \\cdots & 0 & 0\\\\\\end{matrix}\\right]\n\\]\nwhere \\(p_i\\) is the probability that our image is the digit \\(i\\). Notice also that the probabilities \\(p_i\\) must sum to one. We encode the class membership of our samples by constructing an \\(N\\times r\\) matrix \\(Y\\), each row of which has a one in column \\(j\\) if that sample belongs to class \\(j\\), and zeros elsewhere. This type of representation is sometimes called “one-hot” encoding.\nSo let’s assume we have \\(N\\) data points, each with \\(k\\) features, and a one-hot encoded, \\(N\\times r\\) matrix of labels \\(Y\\) encoding the data into \\(r\\) classes. As usual, we add an “extra” feature, which is the constant \\(1\\) for each sample, to account for the “intercept”. So our data matrix will be \\(N\\times (k+1)\\).\nOur goal will be to find a \\((k+1)\\times r\\) matrix of “weights” \\(M\\) so that, for each sample, we compute \\(r\\) values, given by the rows of the matrix \\(XM\\). These \\(r\\) values are linear functions of the features, but we need probabilities. In the one-dimensional case, we used the logistic function \\(\\sigma\\) to convert our linear function to probabilities. In this higher dimensional case we use a generalization of \\(\\sigma\\) called the “softmax” function.\nDefinition: Let \\(F:\\mathbf{R}^r\\to\\mathbf{R}^{r}\\) be the function \\[\nF(z_1,\\ldots, z_r) = \\sum_{j=1}^{r} e^{z_{i}}\n\\] and let \\(\\sigma:\\mathbf{R}^{r}\\to \\mathbf{R}^{r}\\) be the function \\[\n\\sigma(z_1,\\ldots, z_n) = \\left[\\begin{matrix} \\frac{e^{z_1}}{F} & \\cdots & \\frac{e^{z_{r}}}{F}\\end{matrix}\\right].\n\\] Notice that the coordinates of the vector \\(\\sigma(z_1,\\ldots,z_n)\\) are all between \\(0\\) and \\(1\\), and their sum is one.\nOur multiclass logistic model will say that the probability vector that gives the probabilities that a particular sample belongs to a particular class is given by the rows of the matrix \\(\\sigma(XM)\\), where \\(\\sigma(XM)\\) means applying the function \\(\\sigma\\) to each row of the \\(N\\times r\\) matrix \\(XM\\). For later computation, if:\n\n\\(x=X[i,:]\\) is the \\(k+1\\)-entry feature vector of a single sample – a row of the data matrix \\(X\\)\n\\(m_{j}=M[:,j]\\) is the \\(k+1\\)-entry column vector corresponding to the \\(j^{th}\\) column of \\(M\\),\n\nthen the probability vector \\([p_{t}]_{t=1}^{r}\\) has entries \\[\np_{t}(x;M) = \\frac{e^{x\\cdot m_{t}}}{\\sum_{s=1}^{r} e^{x\\cdot m_{s} }}.\n\\]\n\n6.5.1 Multiclass logistic regression - the likelihood\nThe probability vector \\([p_{t}(x;M)]\\) encodes the probabilities that the \\(x\\)-value belongs to each of the possible classes. That is, \\[\np_{j}(x;M)=\\hbox{The chance that x is in class j}.\n\\]\nWe have captured the class membership of the samples in a \\((k+1)\\times r\\) matrix \\(Y\\) which is “one-hot” encoded. Each row of this matrix has is zero in each place, except in the “correct” class, where it is one. Let \\(y=Y[i,:]\\) be the \\(i^{th}\\) row of this matrix, so it is an \\(r\\)-entry row vector which is \\(1\\) in the position giving the “correct” class for our sample \\(x\\).\nSo we can represent the chance that sample \\(j\\) belongs to class \\(i\\) as \\[\nP(\\hbox{ sample i in class j})=\\prod_{s=1}^{r} p_{s}(x;M)^{y_{s}}.\n\\] Taking the logarithm, we find \\[\n\\log P = \\sum_{s=1}^{r} y_{s}\\log p_{s}(x;M).\n\\]\nSince each sample is independent, the total likelihood is the product of these probabilites, and the log-likelihood the corresponding sum: \\[\n\\log L(M) = \\sum_{X,Y} \\sum_{s=1}^{r} y_{s}\\log p_{s}(x;M).\n\\] where the sum is over the \\(N\\) rows of \\(X\\) and \\(Y\\). This is equivalent to the matrix expression \\[\n\\log L(M) = \\mathrm{trace}(Y^{\\intercal}\\log P)=\\mathrm{trace}(Y^{\\intercal}\\log\\sigma(XM))\n\\]\nThis is the multiclass generalization of Equation 6.3. To see the connection, notice that, in the case where we have only two classes, \\(y_1=1-y_0\\) and \\(p_{1}(x;M)=1-p_{0}(x;M)\\), so this sum is the same as in the two class situation.\n\n\n6.5.2 Multiclass logistic regression - the gradient.\nTo find the “best-fitting” multiclass logistic model by gradient descent, we need an expression for the gradient of the likelihood \\(L(M)\\). As with all of these calculations, this is an exercise in the chain rule. We start with the formula \\[\np_{s}(x;M) = \\frac{e^{x\\cdot m_s}}{\\sum_{t=1}^{r} e^{x\\cdot m_{t}}}\n\\] The gradient of this is made up of the derivatives with respect to the \\(m_{bq}\\) where \\(b=0,\\ldots, k\\) and \\(q=1,\\dots, r\\) so its natural to think of this gradient as a \\((k+1)\\times r\\) matrix, the same shape as \\(M\\). Remember that each \\(m_s\\) is the \\(s^{th}\\) column of \\(M\\) so is made up of \\(m_{bs}\\) for \\(b=0,\\ldots, k\\).\nLooking at \\[\n\\frac{\\partial p_{s}}{\\partial m_{bq}}\n\\] there are two cases to consider. The first is when \\(q\\) and \\(s\\) are different, so the numerator of \\(p_{s}\\) doesn’t involve \\(m_{pq}\\). In this case the derivative is \\[\n\\frac{\\partial p_{s}}{\\partial m_{bq}}=-\\frac{e^{x\\cdot m_{s}}e^{x\\cdot m_{q}}x_b}{(\\sum_{t=1}^{r} e^{x\\cdot m_{t}})^2}=-p_{s}p_{q}x_{b}\n\\] In vector terms: \\[\n[\\frac{\\partial p_{s}}{\\partial m_{bq}}]_{b=1}^{k}=-p_{q}p_{s}[x_{b}]_{b=1}^{k}\n\\] as an equality of \\(k+1\\)-entry row vectors. This can be written more simply as a vector equation: \\[\n\\frac{\\partial p_{s}}{\\partial m_{q}}=-p_{q}p_{s}x.\\qquad (q\\not=s).\n\\] When \\(q=s\\), we have \\[\n\\frac{\\partial p_{s}}{\\partial m_{bs}}\n=\\frac{e^{x\\cdot m_{bs}}x_b}{\\sum_{t=1}^{r}e^{x\\cdot m_{t}}}-\\frac{e^{x\\cdot m_{s}}e^{x\\cdot m_{s}}x_b}{(\\sum_{t=1}^{r} e^{x\\cdot m_{t}})^2}=p_{s}(1-p_{s})x_b\n\\] or in vector terms \\[\n\\frac{\\partial p_{s}}{\\partial m_{s}}=p_{s}(1-p_{s})x^{\\intercal}.\n\\]\nImportant: The gradient on the left is properly seen as a column vector (because \\(m_{s}\\) is a column of the matrix \\(M\\), with \\(k+1\\) entries), and since \\(x\\) is a row of the data matrix, so to keep the indices straight, we need \\(x^{\\intercal}\\) on the right.\nNow we can use these formulae together with the expression for \\(\\log L(M)\\) to obtain the gradient. Using the vector form, we have \\[\n\\frac{\\partial \\log L(M)}{\\partial m_{q}} = \\sum_{X,Y}\\sum_{s=1}^{r} y_{s}\\frac{\\partial \\log p_{s}}{m_{q}}.\n\\] Using our computations above, the chain rule, and the derivative of the logarithm, this is the sum \\[\n\\frac{\\partial \\log L(M)}{\\partial m_{q}} =\\sum_{X,Y}\\sum_{s=1}^{r} y_{s}(I_{qs}-p_{q})x^{\\intercal}\n\\] where \\(I_{qs}=1\\) if \\(q=s\\) and zero otherwise.\nNow \\(y_{s}I_{qs}\\) is zero unless \\(s=q\\), and the sum \\(\\sum_{s=1}^{r} y_{s}=1\\), so this simplifies further to \\[\n\\frac{\\partial \\log L(M)}{\\partial m_{q}} = \\sum_{X,Y} (y_{q}-p_{q})x^{\\intercal}.\n\\] This is equivalent to the matrix expression \\[\n\\nabla \\log L(M) = X^{\\intercal}(Y-P)=X^{\\intercal}(Y-\\sigma(XM)).\n\\tag{6.4}\\]\nCompare Equation 6.4 to Theorem 6.1 and we see that the form is identical whether in the two-class or multi-class case if we set things up properly.\n\nAlgorithm 6.1 (Multiclass Gradient Descent) Given:\n\nan \\(N\\times(k+1)\\) data matrix \\(X\\) whose last column is all \\(1\\),\nan \\(N\\times r\\) matrix \\(Y\\) that “one-hot” encodes the labels of the classification problem;\na random \\((k+1)\\times r\\) matrix \\(M\\) of initial guesses for the parameters\na “learning rate” \\(\\nu\\),\n\nIterate: \\[\nM=M+\\nu X^{\\intercal}(Y-\\sigma(XM))\n\\] until \\(M\\) changes by less than some tolerance."
  },
  {
    "objectID": "chapters/05-logistic-regression.html#batch-descent",
    "href": "chapters/05-logistic-regression.html#batch-descent",
    "title": "6  Logistic Regression",
    "section": "6.6 Batch Descent",
    "text": "6.6 Batch Descent\nA look at the formulae for the gradient (see Equation 6.4) tells us that each iteration of the algorithm requires us to multiply the entire data matrix times the weights (to compute \\(XM\\)) and then again to multiply by \\(X^{\\intercal}\\). In practice, \\(X\\) may have a very large number of rows, and working with the entire matrix may be impractical.\nOne simple solution to this is to work with the data in batches. Each main iteration of gradient descent is made up of smaller steps, each of which works with a subset of the data matrix. These smaller steps could be single rows of the data matrix, or submatrices.\n\n\n\n\n[1] Jack Daoud. Marketing Analytics.Available at https://www.kaggle.com/datasets/jackdaoud/marketing-data.\n\n\n[2] LeCun, Y., Cortes, C. and Burges, C. The MNIST Database.Available at http://yann.lecun.com/exdb/mnist/."
  },
  {
    "objectID": "chapters/06-svm.html#introduction",
    "href": "chapters/06-svm.html#introduction",
    "title": "7  Support Vector Machines",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nSuppose that we are given a collection of data made up of samples from two different classes, and we would like to develop an algorithm that can distinguish between the two classes. For example, given a picture that is either a dog or a cat, we’d like to be able to say which of the pictures are dogs, and which are cats. For another example, we might want to be able to distinguish “real” emails from “spam.” This type of problem is called a classification problem.\nTypically, one approaches a classification problem by beginning with a large set of data for which you know the classes, and you use that data to train an algorithm to correctly distinguish the classes for the test cases where you already know the answer. For example, you start with a few thousand pictures labelled “dog” and “cat” and you build your algorithm so that it does a good job distinguishing the dogs from the cats in this initial set of training data. Then you apply your algorithm to pictures that aren’t labelled and rely on the predictions you get, hoping that whatever let your algorithm distinguish between the particular examples will generalize to allow it to correctly classify images that aren’t pre-labelled.\nBecause classification is such a central problem, there are many approaches to it. We will see several of them through the course of these lectures. We will begin with a particular classification algorithm called “Support Vector Machines” (SVM) that is based on linear algebra. The SVM algorithm is widely used in practice and has a beautiful geometric interpretation, so it will serve as a good beginning for later discussion of more complicated classification algorithms.\nIncidentally, I’m not sure why this algorithm is called a “machine”; the algorithm was introduced in the paper [1] where it is called the “Optimal Margin Classifier” and as we shall see that is a much better name for it.\nMy presentation of this material was heavily influenced by the beautiful paper [2]."
  },
  {
    "objectID": "chapters/06-svm.html#a-simple-example",
    "href": "chapters/06-svm.html#a-simple-example",
    "title": "7  Support Vector Machines",
    "section": "7.2 A simple example",
    "text": "7.2 A simple example\nLet us begin our discussion with a very simple dataset (see [3] and [4]). This data consists of various measurements of physical characteristics of 344 penguins of 3 different species: Gentoo, Adelie, and Chinstrap. If we focus our attention for the moment on the Adelie and Gentoo species, and plot their body mass against their culmen depth, we obtain the following scatterplot.\n\n\n\nFigure 7.1: Penguin Scatterplot\n\n\nIncidentally, a bird’s culmen is the upper ridge of their beak, and the culmen depth is a measure of the thickness of the beak. There’s a nice picture at [4] for the penguin enthusiasts.\nA striking feature of this scatter plot is that there is a clear separation between the clusters of Adelie and Gentoo penguins. Adelie penguins have deeper culmens and less body mass than Gentoo penguins. These characteristics seem like they should provide a way to classify a penguin between these two species based on these two measurements.\nOne way to express the separation between these two clusters is to observe that one can draw a line on the graph with the property that all of the Adelie penguins lie on one side of that line and all of the Gentoo penguins lie on the other. In Figure 7.2 I’ve drawn in such a line (which I found by eyeballing the picture in Figure 7.1). The line has the equation \\[\nY = 1.25X+2.\n\\]\n\n\n\nFigure 7.2: Penguins with Separating Line\n\n\nThe fact that all of the Gentoo penguins lie above this line means that, for the Gentoo penguins, their body mass in grams is at least \\(400\\) more than \\(250\\) times their culmen depth in mm. (Note that the \\(y\\) axis of the graph is scaled by \\(200\\) grams).\n\\[\n\\mathrm{Gentoo\\ mass}> 250(\\mathrm{Gentoo\\ culmen\\ depth})+400\n\\]\nwhile\n\\[\n\\mathrm{Adelie\\ mass}<250(\\mathrm{Adelie\\ culmen\\ depth})+400.\n\\]\nNow, if we measure a penguin caught in the wild, we can compute \\(250(\\mathrm{culmen\\ depth})+400\\) for that penguin and if this number is greater than the penguin’s mass, we say it’s an Adelie; otherwise, a Gentoo. Based on the experimental data we’ve collected – the training data – this seems likely to work pretty well."
  },
  {
    "objectID": "chapters/06-svm.html#the-general-case",
    "href": "chapters/06-svm.html#the-general-case",
    "title": "7  Support Vector Machines",
    "section": "7.3 The general case",
    "text": "7.3 The general case\nTo generalize this approach, let’s imagine now that we have \\(n\\) samples and \\(k\\) features (or measurements) for each sample. As before, we can represent this data as an \\(n\\times k\\) data matrix \\(X\\). In the penguin example, our data matrix would be \\(344\\times 2\\), with one row for each penguin and the columns representing the mass and the culmen depth. In addition to this numerical data, we have a classification that assigns each row to one of two classes. Let’s represent the classes by a \\(n\\times 1\\) vector \\(Y\\), where \\(y_{i}=+1\\) if the \\(i^{th}\\) sample is in one class, and \\(y_{i}=-1\\) if that \\(i^{th}\\) sample is in the other. Our goal is to predict \\(Y\\) based on \\(X\\) – but unlike in linear regression, \\(Y\\) takes on the values of \\(\\pm 1\\).\nIn the penguin case, we were able to find a line that separated the two classes and then classify points by which side of the line the point was on. We can generalize this notion to higher dimensions. Before attacking that generalization, let’s recall a few facts about the generalization to \\(\\mathbf{R}^{k}\\) of the idea of a line.\n\n7.3.1 Hyperplanes\nThe correct generalization of a line given by an equation \\(w_1 x_1+ w_2 w_2+b=0\\) in \\(\\mathbf{R}^{2}\\) is an equation \\(f(x)=0\\) where \\(f(x)\\) is a degree one polynomial \\[\nf(x) = f(x_1,\\ldots, x_k) = w_1 x_1 + w_2 x_2 +\\cdots + w_k x_k + b\n\\tag{7.1}\\]\nIt’s easier to understand the geometry of an equation like \\(f(x)=0\\) in Equation 7.1 if we think of the coefficients \\(w_i\\) as forming a nonzero vector \\(w = (w_1,\\ldots, w_k)\\) in \\(\\mathbf{R}^{k}\\) and writing the formula for \\(f(x)\\) as \\[\nf(x) = w\\cdot x +b\n\\].\nLemma: Let \\(f(x)=w\\cdot x+b\\) with \\(w\\in\\mathbf{R}^{k}\\) a nonzero vector and \\(b\\) a constant in \\(\\mathbf{R}\\).\n\nThe inequalities \\(f(x)>0\\) and \\(f(x)<0\\) divide up \\(\\mathbf{R}^{k}\\) into two disjoint subsets (called half spaces), in the way that a line in \\(\\mathbf{R}^{2}\\) divides the plane in half.\nThe vector \\(w\\) is normal vector to the hyperplane \\(f(x)=0\\). Concretely this means that if \\(p\\) and \\(q\\) are any two points in that hyperplane, then \\(w\\cdot (p-q)=0\\).\nLet \\(p=(u_1,\\ldots,u_k)\\) be a point in \\(\\mathbf{R}^{k}\\). Then the perpendicular distance \\(D\\) from \\(p\\) to the hyperplane \\(f(x)=0\\) is \\[\nD = \\frac{f(p)}{\\|w\\|}\n\\]\n\nProof: The first part is clear since the inequalities are mutually exclusive. For the secon part, suppose that \\(p\\) and \\(q\\) satisfy \\(f(x)=0\\). Then \\(w\\cdot p+b = w\\cdot q+b=0\\). Subtracting these two equations gives \\(w\\cdot (p-q)=0\\), so \\(p-q\\) is orthogonal to \\(w\\).\nFor the third part, consider Figure 7.3. The point \\(q\\) is an arbitrary point on the hyperplane defined by the equation \\(w\\cdot x+b=0\\). The distance from the hyperplane to \\(p\\) is measured along the dotted line perpendicular to the hyperplane. The dot product \\(w\\cdot (p-q) = \\|w\\|\\|p-q\\|\\cos(\\theta)\\) where \\(\\theta\\) is the angle between \\(p-q\\) and \\(w\\) – which is complementary to the angle between \\(p-q\\) and the hyperplane. The distance \\(D\\) is therefore \\[\nD=\\frac{w\\cdot(p-q)}{\\|w\\|}.\n\\] However, since \\(q\\) lies on the hyperplane, we know that \\(w\\cdot q+b=0\\) so \\(w\\cdot q = -b\\). Therefore \\(w\\cdot(p-q)=w\\cdot p+b=f(p)\\), which is the formula we seek.\n\n\n\nFigure 7.3: Distance to a Hyperplane\n\n\n\n\n7.3.2 Linear separability and Margins\nNow we can return to our classification scheme. The following definition generalizes our two dimensional picture from the penguin data.\nDefinition: Suppose that we have an \\(n\\times k\\) data matrix \\(X\\) and a set of labels \\(Y\\) that assign the \\(n\\) samples to one of two classes. Then the labelled data is said to be linearly separable if there is a vector \\(w\\) and a constant \\(b\\) so that, if \\(f(x)=w\\cdot x+b\\), then \\(f(x)>0\\) whenever \\(x=(x_1,\\ldots, x_k)\\) is a row of \\(X\\) – a sample – belonging to the \\(+1\\) class, and \\(f(x)<0\\) whenever \\(x\\) belongs to the \\(-1\\) class. The solutions to the equation \\(f(x)=0\\) in this situation form a hyperplane that is called a separating hyperplane for the data.\nIn the situation where our data falls into two classes that are linearly separable, our classification strategy is to find a separating hyperplane \\(f\\) for our training data. Then, given a point \\(x\\) whose class we don’t know, we can evaluate \\(f(x)\\) and assign \\(x\\) to a class depending on whether \\(f(x)>0\\) or \\(f(x)<0\\).\nThis definition begs two questions about a particular dataset:\n\nHow do we tell if the two classes are linearly separable?\nIf the two sets are linearly separable, there are infinitely many separating hyperplanes. To see this, look back at the penguin example and notice that we can ‘wiggle’ the red line a little bit and it will still separate the two sets. Which is the ‘best’ separating hyperplane?\n\nLet’s try to make the first of these two questions concrete. We have two sets of points \\(A\\) and \\(B\\) in \\(\\mathbf{R}^{k}\\), and we want to (try to) find a vector \\(w\\) and a constant \\(b\\) so that \\(f(x)=w\\cdot x+b\\) takes strictly positive values for \\(x\\in A\\) and strictly negative ones for \\(x\\in B\\). Let’s approach the problem by first choosing \\(w\\) and then asking whether there is a \\(b\\) that will work. In the two dimensional case, this is equivalent to choosing the slope of our line, and then asking if we can find an intercept so that the line passes between the two classes.\nIn algebraic terms, we are trying to solve the following system of inequalities: given \\(w\\), find \\(b\\) so that: \\[\nw\\cdot x+b>0 \\hbox{ for all $x$ in A}\n\\] and \\[\nw\\cdot x+b<0\\hbox{ for all $x$ in B}.\n\\] This is only going to be possible if there is a gap between the smallest value of \\(w\\cdot x\\) for \\(x\\in A\\) and the largest value of \\(w\\cdot x\\) for \\(x\\in B\\). In other words, given \\(w\\) there is a \\(b\\) so that \\(f(x)=w\\cdot x+b\\) separates \\(A\\) and \\(B\\) if \\[\n\\max_{x\\in B}w\\cdot x < \\min_{x\\in A} w\\cdot x.\n\\] If this holds, then choose \\(b\\) so that \\(-b\\) lies in this open interval and you will obtain a separating hyperplane.\nProposition: The sets \\(A\\) and \\(B\\) are linearly separable if there is a \\(w\\) so that \\[\n\\max_{x\\in B}w\\cdot x < \\min_{x\\in A} w\\cdot x\n\\] If this inequality holds for some \\(w\\), and \\(-b\\) within this open interval, then \\(f(x)=w\\cdot x+b\\) is a separating hyperplane for \\(A\\) and \\(B\\).\n*Figure 7.4 is an illustration of this argument for a subset of the penguin data. Here, we have fixed \\(w=(1.25,-1)\\) coming from the line \\(y=1.25x+2\\) that we eyeballed earlier. For each Gentoo (green) point \\(x_{i}\\), we computed \\(-b=w\\cdot x_{i}\\) and drew the line \\(f(x) = w\\cdot x - w\\cdot x_{i}\\) giving a family of parallel lines through each of the green points. Similarly for each Adelie (blue) point we drew the corresponding line. The maximum value of \\(w\\cdot x\\) for the blue points turned out to be \\(1.998\\) and the minimum value of \\(w\\cdot x\\) for the green points turned out to be \\(2.003\\). Thus we have two lines with a gap between them, and any parallel line in that gap will separate the two sets.\nFinally, among all the lines with this particular \\(w\\), it seems that the best separating line is the one running right down the middle of the gap between the boundary lines. Any other line in the gap will be closer to either the blue or green set that the midpoint line is.\n\n\n\nFigure 7.4: Lines in Penguin Data for \\(w=(1.25,-1)\\)\n\n\nLet’s put all of this together and see if we can make sense of it in general.\nSuppose that \\(A^{+}\\) and \\(A^{-}\\) are finite point sets in \\(\\mathbf{R}^{k}\\) and \\(w\\in\\mathbf{R}^{k}\\) such that \\[\nB^{-}(w)=\\max_{x\\in A^{-}}w\\cdot x < \\min_{x\\in A^{+}}w\\cdot x=B^{+}(w).\n\\] Let \\(x^{-}\\) be a point in \\(A^{-}\\) with \\(w\\cdot x^{-}=B^{-}(w)\\) and \\(x^{+}\\) be a point in \\(A\\) with \\(w\\cdot x^{+}=B^{+}(w)\\). The two hyperplanes \\(f^{\\pm}(x) = w\\cdot x - B^{\\pm}\\) have the property that: \\[\nf^{+}(x)\\ge 0\\hbox{ for }x\\in A^{+}\\hbox{ and }f^{+}(x)<0\\hbox{ for }x\\in A^{-}\n\\] and \\[\nf^{-}(x)\\le 0\\hbox{ for }x\\in A^{-}\\hbox{ and }f^{-}(x)>0\\hbox{ for }x\\in A^{+}\n\\]\nHyperplanes like \\(f^{+}\\) and \\(f^{-}\\), which “just touch” a set of points, are called supporting hyperplanes.\nDefinition: Let \\(A\\) be a set of points in \\(\\mathbf{R}^{k}\\). A hyperplane \\(f(x)=w\\cdot x+b=0\\) is called a supporting hyperplane for \\(A\\) if \\(f(x)\\ge 0\\) for all \\(x\\in A\\) and \\(f(x)=0\\) for at least one point in \\(A\\), or if \\(f(x)\\le 0\\) for all \\(x\\in A\\) and \\(f(x)=0\\) for at least one point in \\(A\\).\nThe gap between the two supporting hyperplanes \\(f^{+}\\) and \\(f^{-}\\) is called the margin between \\(A\\) and \\(B\\) for \\(w\\).\nDefinition: Let \\(f^{+}\\) and \\(f^{-}\\) be as in the discussion above for point sets \\(A^{+}\\) and \\(A^{-}\\) and vector \\(w\\). Then the orthogonal distance between the two hyperplanes \\(f^{+}\\) and \\(f^{-}\\) is called the geometric margin \\(\\tau_{w}(A^{+},A^{-})\\) (along \\(w\\)) between \\(A^{+}\\) and \\(A^{-}\\). We have \\[\n\\tau_{w}(A^{+},A^{-})=\\frac{B^{+}(w)-B^{-}(w)}{\\|w\\|}.\n\\]\nNow we can propose an answer to our second question about the best classifying hyperplane.\nDefinition: The optimal margin \\(\\tau(A^{+},A^{-})\\) between \\(A^{+}\\) and \\(A^{-}\\) is the largest value of \\(\\tau_{w}\\) over all possible \\(w\\) for which \\(B^{-}(w)<B^{+}(w)\\): \\[\n\\tau(A^{+},A^{-}) = \\max_{w} \\tau_{w}(A^{+},A^{-}).\n\\] If \\(w\\) is such that \\(\\tau_{w}=\\tau\\), then the hyperplane \\(f(x)=w\\cdot x - \\frac{(B^{+}+B^{-})}{2}\\) is the optimal margin classifying hyperplane.\nThe optimal classifying hyperplane runs “down the middle” of the gap between the two supporting hyperplanes \\(f^{+}\\) and \\(f^{-}\\) that give the sides of the optimal margin.\nWe can make one more observation about the maximal margin. If we find a vector \\(w\\) so that \\(f^{+}(x) = w\\cdot x -B^{+}\\) and \\(f^{-}(x) = w\\cdot x-B^{-}\\) are the two supporting hyperplanes such that the gap between them is the optimal margin, then this gap gives us an estimate on how close together the points in \\(A^{+}\\) and \\(A^{-}\\) can be. This is visible in Figure 7.4, where it’s clear that to get from a blue point to a green one, you have to cross the gap between the two supporting hyperplanes.\nProposition: The closest distance between points in \\(A^{+}\\) and \\(A^{-}\\) is greater than or equal to the optimal margin: \\[\n\\min_{p\\in A^{+},q\\in A^{-}} \\|p-q\\|\\ge \\tau(A^{+},A^{-})\n\\].\nProof: We have \\(f^{+}(p) = w\\cdot p - B^{+}\\ge 0\\) and \\(f^{-}(q) = w\\cdot q -B^{-}\\le 0\\). These two inequalities imply that \\[\nw\\cdot (p-q)\\ge B^{+}-B^{-}>0.\n\\] Therefore \\[\n\\|p-q\\|\\|w\\|\\ge |w\\cdot (p-q)|\\ge |B^{+}-B^{-}|\n\\] and so \\[\n\\|p-q\\| \\ge \\frac{B^{+}-B^{-}}{\\|w\\|} = \\tau(A^{+},A^{-})\n\\]\nIf this inequality were always strict – that is, if the optimal margin equalled the minimum distance between points in the two clusters – then this would give us an approach to finding this optimal margin.\nUnfortunately, that isn’t the case. In Figure 7.5, we show a very simple case involving only six points in total in which the distance between the closest points in \\(A^{+}\\) and \\(A^{-}\\) is larger than the optimal margin.\n\n\n\nFigure 7.5: Shortest distance between + and - points can be greater than the optimal margin\n\n\nAt least now our problem is clear. Given our two point sets \\(A^{+}\\) and \\(A^{-}\\), find \\(w\\) so that \\(\\tau_{w}(A^{+},A^{-})\\) is maximal among all \\(w\\) where \\(B^{-}(w)<B^{+}(w)\\). This is an optimization problem, but unlike the optimization problems that arose in our discussions of linear regression and principal component analysis, it does not have a closed form solution. We will need to find an algorithm to determine \\(w\\) by successive approximations. Developing that algorithm will require thinking about a new concept known as convexity."
  },
  {
    "objectID": "chapters/06-svm.html#convexity-convex-hulls-and-margins",
    "href": "chapters/06-svm.html#convexity-convex-hulls-and-margins",
    "title": "7  Support Vector Machines",
    "section": "7.4 Convexity, Convex Hulls, and Margins",
    "text": "7.4 Convexity, Convex Hulls, and Margins\nIn this section we introduce the notion of a convex set and the particular case of the convex hull of a finite set of points. As we will see, these ideas will give us a different interpretation of the margin between two sets and will eventually lead to an algorithm for finding the optimal margin classifier.\nDefinition: A subset \\(U\\) of \\(\\mathbf{R}^{k}\\) is convex if, for any pair of points \\(p\\) and \\(q\\) in \\(U\\), every point \\(t\\) on the line segment joining \\(p\\) and \\(q\\) also belongs to \\(U\\). In vector form, for every \\(0\\le s\\le 1\\), the point \\(t(s) = (1-s)p+sq\\) belongs to \\(U\\). (Note that \\(t(0)=p\\), \\(t(1)=q\\), and so \\(t(s)\\) traces out the segment joining \\(p\\) to \\(q\\).)\n*Figure 7.6 illustrates the difference between convex sets and non-convex ones.\n\n\n\nFigure 7.6: Convex vs Non-Convex Sets\n\n\nThe key idea from convexity that we will need to solve our optimization problem and find the optimal margin is the idea of the convex hull of a finite set of points in \\(\\mathbf{R}^{k}\\).\nDefinition: Let \\(S=\\{q_1,\\ldots, q_{N}\\}\\) be a finite set of \\(N\\) points in \\(\\mathbf{R}^{k}\\). The convex hull \\(C(S)\\) of \\(S\\) is the set of points \\[\np = \\sum_{i=1}^{N} \\lambda_{i}q_{i}\n\\] as \\(\\lambda_{1},\\ldots,\\lambda_{N}\\) runs over all positive real numbers such that \\[\n\\sum_{i=1}^{N} \\lambda_{i} = 1.\n\\]\nThere are a variety of ways to think about the convex hull \\(C(S)\\) of a set of points \\(S\\), but perhaps the most useful is that it is the smallest convex set that contains all of the points of \\(S\\). That is the content of the next lemma.\nLemma: \\(C(S)\\) is convex. Furthermore, let \\(U\\) be any convex set containing all of the points of \\(S\\). Then \\(U\\) contains \\(C(S)\\).\nProof: To show that \\(C(S)\\) is convex, we apply the definition. Let \\(p_1\\) and \\(p_2\\) be two points in \\(C(S)\\), so that let \\(p_{j}=\\sum_{i=1}^{N} \\lambda^{(j)}_{i}q_{i}\\) where \\(\\sum_{i=1}^{N}\\lambda^{(j)}_{i} = 1\\) for \\(j=1,2\\). Then a little algebra shows that \\[\n(1-s)p_1+sp_{2} = \\sum_{i=1}^{N} (s\\lambda^{(1)}_{i}+(1-s)\\lambda^{(2)}_{i})q_{i}\n\\] and \\(\\sum_{i=1}^{N} (s\\lambda^{(1)}_{i}+(1-s)\\lambda^{(2)}_{i}) = 1\\). Therefore all of the points \\((1-s)p_{1}+sp_{2}\\) belong to \\(C(S)\\), and therefore \\(C(S)\\) is convex.\nFor the second part, we proceed by induction. Let \\(U\\) be a convex set containing \\(S\\). Then by the definition of convexity, \\(U\\) contains all sums \\(\\lambda_{i}q_{i}+\\lambda_{j}q_{j}\\) where \\(\\lambda_i+\\lambda_j=1\\). Now suppose that \\(U\\) contains all the sums \\(\\sum_{i=1}^{N} \\lambda_{i}q_{i}\\) where exactly \\(m-1\\) of the \\(\\lambda_{i}\\) are non-zero for some \\(m<N\\).\nConsider a sum \\[\nq = \\sum_{i=1}^{N}\\lambda_{i}q_{i}\n\\] with exactly \\(m\\) of the \\(\\lambda_{i}\\not=0\\). For simplicity let’s assume that \\(\\lambda_{i}\\not=0\\) for \\(i=1,\\ldots, m\\). Now let \\(T=\\sum_{i=1}^{m-1}\\lambda_{i}\\) and set \\[\nq' = \\sum_{i=1}^{m-1}\\frac{\\lambda_{i}}{T}q_{i}.\n\\] This point \\(q'\\) belongs to \\(U\\) by the inductive hypothesis. Also, \\((1-T)=\\lambda_{m}\\). Therefore by convexity of \\(U\\), \\[\nq = (1-T)q_{m}+Tq'\n\\] also belongs to \\(U\\). It follows that all of \\(C(S)\\) belongs to \\(U\\).\nIn Figure 7.7 we show our penguin data together with the convex hull of points corresponding to the two types of penguins. Notice that the boundary of each convex hull is a finite collection of line segments that join the “outermost” points in the point set.\n\n\n\nFigure 7.7: The Convex Hull\n\n\nOne very simple example of a convex set is a half-plane. More specifically, if \\(f(x)=w\\cdot x+b=0\\) is a hyperplane, then the two “sides” of the hyperplane, meaning the subsets \\(\\{x: f(x)\\ge 0\\}\\) and \\(\\{x: f(x)\\le 0\\}\\), are both convex. (This is exercise 1 in Section 7.7 ).\nAs a result of this observation, and the Lemma above, we can conclude that if \\(f(x)=w\\cdot x+b=0\\) is a supporting hyperplane for the set \\(S\\) – meaning that either \\(f(x)\\ge 0\\) for all \\(x\\in S\\), or \\(f(x)\\le 0\\) for all \\(x\\in S\\), with at least one point \\(x\\in S\\) such that \\(f(x)=0\\) – then \\(f(x)=0\\) is a supporting hyperplane for the entire convex hull. After all, if \\(f(x)\\ge 0\\) for all points \\(x\\in S\\), then \\(S\\) is contained in the convex set of points where \\(f(x)\\ge 0\\), and therefore \\(C(S)\\) is contained in that set as well.\nInterestingly, however, the converse is true as well – the supporting hyperplanes of \\(C(S)\\) are exactly the same as those for \\(S\\).\nLemma: Let \\(S\\) be a finite set of points in \\(\\mathbf{R}^{k}\\) and let \\(f(x)=w\\cdot x +b=0\\) be a supporting hyperplane for \\(C(S)\\). Then \\(f(x)\\) is a supporting hyperplane for \\(S\\).\nProof: Suppose \\(f(x)=0\\) is a supporting hyperplane for \\(C(S)\\). Let’s assume that \\(f(x)\\ge 0\\) for all \\(x\\in C(S)\\) and \\(f(x^{*})=0\\) for a point \\(x^{*}\\in C(S)\\), since the case where \\(f(x)\\le 0\\) is identical. Since \\(S\\subset C(S)\\), we have \\(f(x)\\ge 0\\) for all \\(x\\in S\\). To show that \\(f(x)=0\\) is a supporting hyperplane, we need to know that \\(f(x)=0\\) for at least one point \\(x\\in S\\).\nLet \\(x'\\) be the point in \\(S\\) where \\(f(x')\\) is minimal among all \\(x\\in S\\). Note that \\(f(x')\\ge 0\\). Then the hyperplane \\(g(x) = f(x)-f(x')\\) has the property that \\(g(x)\\ge 0\\) on all of \\(S\\), and \\(g(x')=0\\). Since the halfplane \\(g(x)\\ge 0\\) is convex and contains all of \\(S\\), we have \\(C(S)\\) contained in that halfplane. So, on the one hand we have \\(g(x^{*})=f(x^{*})-f(x')\\ge 0\\). On the other hand \\(f(x^{*})=0\\), so \\(-f(x')\\ge 0\\), so \\(f(x')\\le 0\\). Since \\(f(x')\\) is also greater or equal to zero, we have \\(f(x')=0\\), and so we have found a point of \\(S\\) on the hyperplane \\(f(x)=0\\). Therefore \\(f(x)=0\\) is also a supporting hyperplane for \\(S\\).\nThis argument can be used to give an alternative description of \\(C(S)\\) as the intersection of all halfplanes containing \\(S\\) arising from supporting hyperplanes for \\(S\\). This is exercise 2 in Section 7.7. It also has as a corollary that \\(C(S)\\) is a closed set.\nLemma: \\(C(S)\\) is compact.\nProof: Exercise 2 in Section 7.7 shows that it is the intersection of closed sets in \\(\\mathbf{R}^{k}\\), so it is closed. Exercise 3 shows that \\(C(S)\\) is bounded. Thus it is compact.\nNow let’s go back to our optimal margin problem, so that we have linearly separable sets of points \\(A^{+}\\) and \\(A^{-}\\). Recall that we showed that the optimal margin was at most the minimal distance between points in \\(A^{+}\\) and \\(A^{-}\\), but that there could be a gap between the minimal distance and the optimal margin – see Figure 7.5 for a reminder.\nIt turns out that by considering the minimal distance between \\(C(A^{+})\\) and \\(C(A^{-})\\), we can “close this gap.” The following proposition shows that we can change the problem of finding the optimal margin into the problem of finding the closest distance between the convex hulls of \\(C(A^{+})\\) and \\(C(A^{-})\\). The following proposition generalizes the Proposition at the end of Section 7.3.2.\nProposition: Let \\(A^{+}\\) and \\(A^{-}\\) be linearly separable sets in \\(\\mathbf{R}^{k}\\). Let \\(p\\in C(A^{+})\\) and \\(q\\in C(A^{-})\\) be any two points. Then \\[\n\\|p-q\\|\\ge \\tau(A^{+},A^{-}).\n\\]\nProof: As in the earlier proof, choose supporting hyperplanes \\(f^{+}(x)=w\\cdot x-B^{+}=0\\) and \\(f^{-}(x)=w\\cdot x-B^{-}\\) for \\(A^{+}\\) and \\(A^{-}\\). By our discussion above, these are also supporting hyperplanes for \\(C(A^{+})\\) and \\(C(A^{-})\\). Therefore if \\(p\\in C(A^{+})\\) and \\(q\\in C(A^{-})\\), we have \\(w\\cdot p-B^{+}\\ge 0\\) and \\(w\\cdot q-B^{-}\\le 0\\). As before \\[\nw\\cdot(p-q)\\ge B^{+}-B^{-}>0\n\\] and so \\[\n\\|p-q\\|\\ge\\frac{B^{+}-B^{-}}{\\|w\\|}=\\tau_{w}(A^{+},A^{-})\n\\] Since this holds for any \\(w\\), we have the result for \\(\\tau(A^{+},A^{-})\\).\nThe reason this result is useful is that, as we’ve seen, if we restrict \\(p\\) and \\(q\\) to \\(A^{+}\\) and \\(A^{-}\\), then there can be a gap between the minimal distance and the optimal margin. If we allow \\(p\\) and \\(q\\) to range over the convex hulls of these sets, then that gap disappears.\nOne other consequence of this is that if \\(A^{+}\\) and \\(A^{-}\\) are linearly separable then their convex hulls are disjoint.\nCorollary: If \\(A^{+}\\) and \\(A^{-}\\) are linearly separable then \\(\\|p-q\\|>0\\) for all \\(p\\in C(A^{+})\\) and \\(q\\in C(A^{-})\\)\nProof: The sets are linearly separable precisely when \\(\\tau>0\\).\nOur strategy now is to show that if \\(p\\) and \\(q\\) are points in \\(C(A^{+})\\) and \\(C(A^{-})\\) respectively that are at minimal distance \\(D\\), and if we set \\(w=p-q\\), then we obtain supporting hyperplanes with margin equal to \\(\\|p-q\\|\\). Since this margin is the largest possible margin, this \\(w\\) must be the optimal \\(w\\). This transforms the problem of finding the optimal margin into the problem of finding the closest points in the convex hulls.\nLemma: Let \\[\nD=\\min_{p\\in C(A^{+}),q\\in C(A^{-})} \\|p-q\\|.\n\\] Then there are points \\(p^*\\in C(A^{+})\\) and \\(q^{*}\\in C(A^{-})\\) with \\(\\|p^{*}-q^{*}\\|=D\\). If \\(p_1^{*},q_1^{*}\\) and \\(p_2^{*},q_2^{*}\\) are two pairs of points satisfying this condition, then \\(p_1^{*}-q_1^{*}=p_2^{*}-q_{2}^{*}\\).\nProof: Consider the set of differences \\[\nV = \\{p-q: p\\in C(A^{+}),q\\in C(A^{-})\\}.\n\\]\n\n\\(V\\) is compact. This is because it is the image of the compact set \\(C(A^{+})\\times C(A^{-})\\) in \\(\\mathbf{R}^{k}\\times\\mathbf{R}^{k}\\) under the continuous map \\(h(x,y)=x-y\\).\nthe function \\(d(v)=\\|v\\|\\) is continuous and satisfies \\(d(v)\\ge D>0\\) for all \\(v\\in V\\).\n\nSince \\(d\\) is a continuous function on a compact set, it attains its minimum \\(D\\) and so there is a \\(v=p^{*}-q^{*}\\) with \\(d(v)=D\\).\nNow suppose that there are two distinct points \\(v_1=p_1^*-q_1^*\\) and \\(v_2=p_2^*-q_2^*\\) with \\(d(v_1)=d(v_2)=D\\). Consider the line segment \\[\nt(s) = (1-s)v_1+sv_2\\hbox{ where }0\\le s\\le 1\n\\] joining \\(v_1\\) and \\(v_2\\).\nNow \\[\nt(s) = ((1-s)p_1^*+sp_2^*)-((1-s)q_1^*+sq_2^*).\n\\] Both terms in this difference belong to \\(C(A^{+})\\) and \\(C(A^{-})\\) respectively, regardless of \\(s\\), by convexity, and therefore \\(t(s)\\) belongs to \\(V\\) for all \\(0\\le s\\le 1\\).\nThis little argument shows that \\(V\\) is convex. In geometric terms, \\(v_1\\) and \\(v_2\\) are two points in the set \\(V\\) equidistant from the origin and the segment joining them is a chord of a circle; as Figure 7.8 shows, in that situation there must be a point on the line segment joining them that’s closer to the origin than they are. Since all the points on that segment are in \\(V\\) by convexity, this would contradict the assumption that \\(v_1\\) is the closet point in \\(V\\) to the origin.\n\n\n\nFigure 7.8: Chord of a circle\n\n\nIn algebraic terms, since \\(D\\) is the minimal value of \\(\\|v\\|\\) for all \\(v\\in V\\), we must have \\(t(s)\\ge D\\).\nOn the other hand \\[\n\\frac{d}{ds}\\|t(s)\\|^2 = \\frac{d}{ds}(t(s)\\cdot t(s)) =t(s)\\cdot \\frac{dt(s)}{ds} = t(s)\\cdot(v_2-v_1).\n\\] Therefore \\[\n\\frac{d}{ds}\\|t(s)\\|^2|_{s=0} = v_{1}\\cdot(v_{2}-v_{1})=v_{1}\\cdot v_{2}-\\|v_{1}\\|^2\\le 0\n\\] since \\(v_{1}\\cdot v_{2}\\le D^{2}\\) and \\(\\|v_{1}\\|^2=D^2\\). If \\(v_{1}\\cdot v_{2}<D^{2}\\), then this derivative would be negative, which would mean that there is a value of \\(s\\) where \\(t(s)\\) would be less than \\(D\\). Since that can’t happen, we conclude that \\(v_{1}\\cdot v_{2}=D^{2}\\) which means that \\(v_{1}=v_{2}\\) – the vectors have the same magnitude \\(D\\) and are parallel. This establishes uniqueness.\nNote: The essential ideas of this argument show that a compact convex set in \\(\\mathbf{R}^{k}\\) has a unique point closest to the origin. The convex set in this instance, \\[\nV=\\{p-q:p\\in C(A^{+}),q\\in C(A^{-})\\},\n\\] is called the difference \\(C(A^{+})-C(A^{-})\\), and it is generally true that the difference of convex sets is convex.\nNow we can conclude this line of argument.\nTheorem: Let \\(p\\) and \\(q\\) be points in \\(C(A^{+})\\) and \\(C(A^{-})\\) respectively are such that \\(\\|p-q\\|\\) is minimal among all such pairs. Let \\(w=p-q\\) and set \\(B^{+}=w\\cdot p\\) and \\(B^{-}=w\\cdot q\\). Then \\(f^{+}(x)=w\\cdot x-B^{+}=0\\) and \\(f^{-}(x)=w\\cdot x-B^{-}\\) are supporting hyperplanes for \\(C(A^{+})\\) and \\(C(A^{-})\\) respectively and the associated margin \\[\n\\tau_{w}(A^{+},A^{-})=\\frac{B^{+}-B^{-}}{\\|w\\|} = \\|p-q\\|\n\\] is optimal.\nProof: First we show that \\(f^{+}(x)=0\\) is a supporting hyperplane for \\(C(A^{+})\\). Suppose not. Then there is a point \\(p'\\in C(A^{+})\\) such that \\(f^{+}(x)<0\\). Consider the line segment \\(t(s) = (1-s)p+sp'\\) running from \\(p\\) to \\(p'\\). By convexity it is entirely contained in \\(C(A^{+})\\). Now look at the distance from points on this segment to \\(q\\): \\[\nD(s)=\\|t(s)-q\\|^2.\n\\] We have \\[\n\\frac{dD(s)}{ds}|_{s=0} = 2(p-q)\\cdot (p'-p) = 2w\\cdot (p'-p) = 2\\left[(f^{+}(p')+B^{+})-(f^{+}(p)+B^{+})\\right]\n\\] so \\[\n\\frac{dD(s)}{ds}|_{s=0} = 2(f^{+}(p')-f^{+}(p))<0\n\\] since \\(f(p)=0\\). This means that \\(D(s)\\) is decreasing along \\(t(s)\\) and so there is a point \\(s'\\) along \\(t(s)\\) where \\(\\|t(s')-q\\|<D\\). This contradicts the fact that \\(D\\) is the minimal distance. The same argument shows that \\(f^{-}(x)=0\\) is also a supporting hyperplane.\nNow the margin for this \\(w\\) is \\[\n\\tau_{w}(A^{+},A^{-}) = \\frac{w\\cdot (p-q)}{\\|w\\|} = \\|p-q\\|=D\n\\] and as \\(w\\) varies we know this is the largest possible \\(\\tau\\) that can occur. Thus this is the maximal margin.\n*Figure 7.9 shows how considering the closest point in the convex hulls “fixes” the problem that we saw in Figure 7.5. The closest point occurs at a point on the boundary of the convex hull that is not one of the points in \\(A^{+}\\) or \\(A^{-}\\).\n\n\n\nFigure 7.9: Closest distance between convex hulls gives optimal margin"
  },
  {
    "objectID": "chapters/06-svm.html#finding-the-optimal-margin-classifier",
    "href": "chapters/06-svm.html#finding-the-optimal-margin-classifier",
    "title": "7  Support Vector Machines",
    "section": "7.5 Finding the Optimal Margin Classifier",
    "text": "7.5 Finding the Optimal Margin Classifier\nNow that we have translated our problem into geometry, we can attempt to develop an algorithm for solving it. To recap, we have two sets of points \\[\nA^{+}=\\{x^+_1,\\ldots, x^+_{n_{+}}\\}\n\\] and \\[\nA^{-}=\\{x^-_1,\\ldots, x^-_{n_{-}}\\}\n\\] in \\(\\mathbf{R}^{k}\\) that are linearly separable.\nWe wish to find points \\(p\\in C(A^{+})\\) and \\(q\\in C(A^{-})\\) such that \\[\n\\|p-q\\|=\\min_{p'\\in C(A^{+}),q'\\in C(A^{-})} \\|p'-q'\\|.\n\\]\nUsing the definition of the convex hull we can express this more concretely. Since \\(p\\in C(A^{+})\\), there are coefficients \\(\\lambda^{+}_{i}\\ge 0\\) for \\(i=1,\\ldots,n_{+}\\) and \\(\\lambda^{-}_{i}\\ge 0\\) for \\(i=1,\\ldots, n_{-}\\) so that \\[\n\\begin{aligned}\np&=&\\sum_{i=1}^{n_{+}}\\lambda^{+}_{i} x^{+}_{i} \\\\\nq&=&\\sum_{i=1}^{n_{-}}\\lambda^{-}_{i} x^{-}_{i} \\\\\n\\end{aligned}\n\\] where \\(\\sum_{i=1}^{n_{\\pm}} \\lambda_{i}^{\\pm}=1\\).\nWe can summarize this as follows:\nOptimization Problem 1: Write \\(\\lambda^{\\pm}=(\\lambda^{\\pm}_{1},\\ldots, \\lambda^{\\pm}_{n_{\\pm}})\\) Define \\[\nw(\\lambda^+,\\lambda^-) = \\sum_{i=1}^{n_{+}}\\lambda^{+}_{i}x^{+}_{i} - \\sum_{i=1}^{n_{-}}\\lambda^{-}x^{-}_{i}\n\\] To find the supporting hyperplanes that define the optimal margin between \\(A^{+}\\) and \\(A^{-}\\), find \\(\\lambda^{+}\\) and \\(\\lambda^{-}\\) such that \\(\\|w(\\lambda^{+},\\lambda^{-})\\|^2\\) is minimal among all such \\(w\\) where all \\(\\lambda^{\\pm}_{i}\\ge 0\\) and \\(\\sum_{i=1}^{n_{\\pm}} \\lambda^{\\pm}_{i}=1\\).\nThis is an example of a constrained optimization problem. It’s worth observing that the objective function \\(\\|w(\\lambda^{+},\\lambda^{-})\\|^2\\) is just a quadratic function in the \\(\\lambda^{\\pm}.\\) Indeed we can expand \\[\n\\|w(\\lambda^{+},\\lambda^{-})\\|^2 = (\\sum_{i=1}^{n_{+}}\\lambda^{+}_{i}x_{i}- \\sum_{i=1}^{n_{-}}\\lambda^{-}x^{-}_{i})\\cdot(\\sum_{i=1}^{n_{+}}\\lambda^{+}_{i}x_{i}- \\sum_{i=1}^{n_{-}}\\lambda^{-}x^{-}_{i})\n\\] to obtain \\[\n\\|w(\\lambda^{+},\\lambda^{-})\\|^2 = R -2S +T\n\\] where \\[\n\\begin{aligned}\nR &=& \\sum_{i=1}^{n_{+}}\\sum_{j=1}^{n_{+}}\\lambda^{+}_{i}\\lambda^{+}_{j}(x^{+}_{i}\\cdot x^{+}_{j}) \\\\\nS &=& \\sum_{i=1}^{n_{+}}\\sum_{j=1}^{n_{-}}\\lambda^{+}_{i}\\lambda^{-}_{j}(x^{+}_{i}\\cdot x^{-}_{j}) \\\\\nT &=& \\sum_{i=1}^{n_{-}}\\sum_{j=1}^{n_{-}}\\lambda^{-}_{i}\\lambda^{-}_{j}(x^{-}_{i}\\cdot x^{-}_{j}) \\\\\n\\end{aligned}\n\\tag{7.2}\\] Thus the function we are trying to minimize is relatively simple.\nOn the other hand, unlike optimization problems we have seen earlier in these lectures, in which we can apply Lagrange multipliers, in this case some of the constraints are inequalities – namely the requirement that all of the \\(\\lambda^{\\pm}\\ge 0\\) – rather than equalities. There is an extensive theory of such problems that derives from the idea of Lagrange multipliers. However, in these notes, we will not dive into that theory but will instead construct an algorithm for solving the problem directly.\n\n7.5.1 Relaxing the constraints\nOur first step in attacking this problem is to adjust our constraints and our objective function slightly so that the problem becomes easier to attack.\nOptimization Problem 2: This is a slight revision of problem 1 above. We minimize: \\[\nQ(\\lambda^{+},\\lambda^{-}) = \\|w(\\lambda^{+},\\lambda^{-})\\|^2-\\sum_{i=1}^{n_{+}}\\lambda^{+}_{i}-\\sum_{i=1}^{n_{-}}\\lambda^{-}_{i}\n\\] subject to the constraints that all \\(\\lambda^{\\pm}_{i}\\ge 0\\) and \\[\n\\alpha = \\sum_{i=1}^{n_{+}}\\lambda^+_{i} = \\sum_{i=1}^{n_{-}}\\lambda^{-}_{i}.\n\\]\nProblem 2 is like problem 1, except we don’t require the sums of the \\(\\lambda^{\\pm}_{i}\\) to be one, but only that they be equal to each other; and we modify the objective function slightly. It turns out that the solution to this optimization problem easily yields the solution to our original one.\nLemma: Suppose \\(\\lambda^{+}\\) and \\(\\lambda^{-}\\) satisfy the constraints of problem 2 and yield the minimal value for the objective function \\(Q(\\lambda^{+},\\lambda^{-})\\). Then \\(\\alpha\\not=0\\). Rescale the \\(\\lambda^{\\pm}\\) to have sum equal to one by dividing by \\(\\alpha\\), yielding \\(\\tau^{\\pm}=(1/\\alpha)\\lambda^{\\pm}\\). Then \\(w(\\tau^{+},\\tau^{-})\\) is a solution to optimization problem 1.\nProof: To show that \\(\\alpha\\not=0\\), suppose that \\(\\lambda^{\\pm}_{i}=0\\) for all \\(i\\not=1\\) and \\(\\lambda=\\lambda^{+}_{1}=\\lambda^{-}_{1}\\). The one-variable quadratic function \\(Q(\\lambda)\\) takes its minimum value at \\(\\lambda=1/\\|x_{1}^{+}-x_{1}^{-}\\|^2\\) and its value at that point is negative. Therefore the minimum value of \\(Q\\) is negative, which means \\(\\alpha\\not=0\\) at that minimum point.\nFor the equivalence, notice that \\(\\tau^{\\pm}\\) still satisfy the constraints of problem 2. Therefore \\[\nQ(\\lambda^{+},\\lambda^{-}) = \\|w(\\lambda^{+},\\lambda^{-})\\|^2-2\\alpha\\le \\|w(\\tau^{+},\\tau^{-})\\|^2-2.\n\\] On the other hand, suppose that \\(\\sigma^{\\pm}\\) are a solution to problem 1. Then \\[\n\\|w(\\sigma^{+},\\sigma^{-})\\|^2\\le \\|w(\\tau^{+},\\tau^{-})\\|^2.\n\\] Therefore \\[\n\\alpha^2 \\|w(\\sigma^{+},\\sigma^{-})\\|^2 = \\|w(\\alpha\\sigma^{+},\\alpha\\sigma^{-})\\|^2\\le \\|w(\\lambda^{+},\\lambda^{-})\\|^2\n\\] and finally \\[\n\\|w(\\alpha\\sigma^{+},\\alpha\\sigma^{-})\\|^2-2\\alpha\\le Q(\\lambda^{+},\\lambda^{-})=\\|w(\\alpha\\tau^{+},\\alpha\\tau^{-})\\|^2-2\\alpha.\n\\] Since \\(Q\\) is the minimal value, we have \\[\n\\alpha^{2}\\|w(\\sigma^{+},\\sigma^{-})\\|^2 = \\alpha^{2}\\|w(\\tau^{+},\\tau^{-})\\|^2\n\\] so that indeed \\(w(\\tau^{+},\\tau^{-})\\) gives a solution to Problem 1.\n\n\n7.5.2 Sequential Minimal Optimization\nNow we outline an algorithm for solving Problem 2 that is called Sequential Minimal Optimization that was introduced by John Platt in 1998 (See [5] and Chapter 12 of [6]). The algorithm is based on the principle of “gradient ascent”, where we exploit the fact that the negative gradient of a function points in the direction of its most rapid decrease and we take small steps in the direction of the negative gradient until we reach the minimum.\nHowever, in this case simplify this idea a little. Recall that the objective function \\(Q(\\lambda^{+},\\lambda^{-})\\) is a quadratic function in the \\(\\lambda\\)’s and that we need to preserve the condition that \\(\\sum \\lambda^{+}_{i}=\\sum\\lambda^{-}_{i}\\). So our approach is going to be to take, one at a time, a pair \\(\\lambda^{+}_{i}\\) and \\(\\lambda^{-}_{j}\\) and change them together so that the equality of the sums is preserved and the change reduces the value of the objective function. Iterating this will take us to a minimum.\nSo, for example, let’s look at \\(\\lambda^{+}_i\\) and \\(\\lambda^{-}_{j}\\) and, for the moment, think of all of the other \\(\\lambda\\)’s as constants. Then our objective function reduces to a quadratic function of these two variables that looks something like: \\[\nQ(\\lambda_{i}^{+},\\lambda_{j}^{-}) = a(\\lambda^{+}_i)^2+b\\lambda^{+}_i\\lambda^{-}_j+c(\\lambda^{-}_{i})^2+d\\lambda^{+}_i+e\\lambda^{-}_{j}+f.\n\\] The constraints that remain are \\(\\lambda^{\\pm}\\ge 0\\), and we are going to try to minimize \\(Q\\) by changing \\(\\lambda_{i}^{+}\\) and \\(\\lambda_{j}^{-}\\) by the same amount \\(\\delta\\). Furthermore, since we still must have \\(\\lambda_{i}^{+}+\\delta\\ge 0\\) and \\(\\lambda_{j}^{-}+\\delta\\ge 0\\), we have\n\\[\n\\delta\\ge M=\\max\\{-\\lambda_{i}^{+},-\\lambda_{j}^{-}\\}\n\\tag{7.3}\\]\nIn terms of this single variable \\(\\delta\\), our optimization problem becomes the job of finding the minimum of a quadratic polynomial in one variable subject to the constraint in Equation 7.3. This is easy! There are two cases: the critical point of the quadratic is to the left of \\(M\\), in which case the minimum value occurs at \\(M\\); or the critical point of the quadratic is to the right of \\(M\\), in which case the critical point occurs there. This is illustrated in Figure 7.10.\n\n\n\nFigure 7.10: Minimizing the 1-variable quadratic objective function\n\n\nComputationally, let’s write \\[\nw_{\\delta,i,j}(\\lambda^{+},\\lambda^{-}) = w(\\lambda^{+},\\lambda^{-})+\\delta(x^{+}_{i}-x^{-}_{j}).\n\\] Then \\[\n\\frac{d}{d\\delta}(\\|w_{\\delta,i,j}(\\lambda^{+},\\lambda^{-})\\|^2-2\\alpha)  = 2w_{\\delta,i,j}(\\lambda^{+},\\lambda^{-})\\cdot(x^{+}_{i}-x^{-}_{j})-2\n\\] and using the definition of \\(w_{\\delta,i,j}\\) we obtain the following formula for the critical value of \\(\\delta\\) by setting this derivative to zero: \\[\n\\delta_{i,j} = \\frac{(1-w(\\lambda^{+},\\lambda^{-})\\cdot(x_{i}^{+}-x_{j}^{-})}{\\|x^+_{i}-x^{-}_{j}\\|^2}\n\\]\nUsing this information we can describe the SMO algorithm.\nAlgorithm (SMO, see [5]):\nGiven: Two linearly separable sets of points \\(A^{+}=\\{x_{1}^{+},\\ldots,x_{n_{+}}^{+}\\}\\) and \\(A^{-}=\\{x_{1}^{-},\\ldots, x_{n_{-}}^{-}\\}\\) in \\(\\mathbf{R}^{k}\\).\nFind: Points \\(p\\) and \\(q\\) belonging to \\(C(A^{+})\\) and \\(C(A^{-})\\) respectively such that \\[\n\\|p-q\\|^2=\\min_{p'\\in C(A^{+}),q'\\in C(A^{-})} \\|p'-q'\\|^2\n\\]\nInitialization: Set \\(\\lambda_{i}^{+}=\\frac{1}{n_{+}}\\) for \\(i=1,\\ldots, n_{+}\\) and \\(\\lambda_{i}^{-}=\\frac{1}{n_{-}}\\) for \\(i=1,\\ldots, n_{-}\\). Set \\[\np(\\lambda^{+})=\\sum_{i=1}^{n_{+}}\\lambda^{+}_{i}x^{+}_{i}\n\\] and \\[\nq(\\lambda^{-})=\\sum_{i=1}^{n_{-}}\\lambda^{-}_{i}x^{-}_{i}\n\\] Notice that \\(w(\\lambda^{+},\\lambda^{-})=p(\\lambda^{+})-q(\\lambda^{-})\\). Let \\(\\alpha=\\sum_{i=1}^{n_{+}}\\lambda^{+}=\\sum_{i=1}^{n_{-}}\\lambda^{-}\\). These sums will remain equal to each other throughout the operation of the algorithm.\nRepeat the following steps until maximum value of \\(\\delta^{*}\\) computed in each iteration is smaller than some tolerance (so that the change in all of the \\(\\lambda\\)’s is very small):\n\nFor each pair \\(i,j\\) with \\(1\\le i\\le n_{+}\\) and \\(1\\le j\\le n_{-}\\), compute \\[\nM_{i,j} = \\max\\{-\\lambda_{i}^{+},-\\lambda_{j}^{-}\\}\n\\] and \\[\n\\delta_{i,j} = \\frac{1-(p(\\lambda^{+})-q(\\lambda^{-}))\\cdot(x_{i}^{+}-x_{j}^{-})}{\\|x^+_{i}-x^{-}_{j}\\|^2}.\n\\] If \\(\\delta_{i,j}\\ge M\\) then set \\(\\delta^{*}=\\delta_{i,j}\\); otherwise set \\(\\delta^{*}=M\\). Then update the \\(\\lambda^{\\pm}\\) by the equations: \\[\n\\begin{aligned}\n\\lambda^{+}_{i}&=&\\lambda^{+}_{i}+\\delta_{i,j}^{*} \\\\\n\\lambda^{+}_{j}&=&\\lambda^{-}_{j}+\\delta_{i,j}^{*} \\\\\n\\end{aligned}\n\\]\n\nWhen this algorithm finishes, \\(p\\approx p(\\lambda^{+})\\) and \\(q\\approx q(\\lambda^{-})\\) will be very good approximations to the desired closest points.\nRecall that if we set \\(w=p-q\\), then the optimal margin classifier is\n\\[\nf(x)=w\\cdot x - \\frac{B^{+}+B^{-}}{2}=0\n\\]\nwhere \\(B^{+}=w\\cdot p\\) and \\(B^{-}=w\\cdot q\\). Since \\(w=p-q\\) we can simplify this to obtain\n\\[\nf(x)=(p-q)\\cdot x -\\frac{\\|p\\|^2-\\|q\\|^2}{2}=0.\n\\]\nIn Figure 7.11, we show the result of applying this algorithm to the penguin data and illustrate the closest points as found by an implementation of the SMO algorithm, together with the optimal classifying line.\nBearing in mind that the y-axis is scaled by a factor of 200, we obtain the following rule for distinguishing between Adelie and Gentoo penguins – if the culmen depth and body mass put you above the red line, you are a Gentoo penguin, otherwise you are an Adelie.\n\n\n\nFigure 7.11: Closest points in convex hulls of penguin data"
  },
  {
    "objectID": "chapters/06-svm.html#inseparable-sets",
    "href": "chapters/06-svm.html#inseparable-sets",
    "title": "7  Support Vector Machines",
    "section": "7.6 Inseparable Sets",
    "text": "7.6 Inseparable Sets\nNot surprisingly, real life is often more complicated than the penguin example we’ve discussed at length in these notes. In particular, sometimes we have to work with sets that are not linearly separable. Instead, we might have two point clouds, the bulk of which are separable, but because of some outliers there is no hyperplane we can draw that separates the two sets into two halfplanes.\nFortunately, all is not lost. There are two common ways to address this problem, and while we won’t take the time to develop the theory behind them, we can at least outline how they work.\n\n7.6.1 Best Separating Hyperplanes\nIf our sets are not linearly separable, then their convex hulls overlap and so our technique for finding the closest points of the convex hulls won’t work. In this case, we can “shrink” the convex hull by considering combinations of points \\(\\sum_{i}\\lambda_{i}x_{i}\\) where \\(\\sum\\lambda_{i}=1\\) and \\(C\\ge\\lambda_{i}\\ge 0\\) for some \\(C\\le 1\\). For \\(C\\) small enough, reduced convex hulls will be linearly separable – although some outlier points from each class will lie outside of them – and we can find hyperplane that separates the reduced hulls.\nIn practice, this means we allow a few points to lie on the “wrong side” of the hyperplane. Our tolerance for these mistakes depends on \\(C\\), but we can include \\(C\\) in the optimization problem to try to find the smallest \\(C\\) that “works”.\n\n\n7.6.2 Nonlinear kernels\nThe second option is to look not for separating hyperplanes but instead for separating curves – perhaps polynomials or even more exotic curves. This can be achieved by taking advantage of the form of Equation 7.2. As you see there, the only way the points \\(x_{i}^{\\pm}\\) enter in to the function being minimized is through the inner products \\(x_{i}^{\\pm}\\cdot x_{j}^{\\pm}\\). We can adopt a different inner product than the usual Euclidean one, and reconsider the problem using this different inner product. This amounts to embedding our points in a higher dimensional space where they are more likely to be linearly separable. Again, we will not pursue the mathematics of this further in these notes."
  },
  {
    "objectID": "chapters/06-svm.html#sec-exercises",
    "href": "chapters/06-svm.html#sec-exercises",
    "title": "7  Support Vector Machines",
    "section": "7.7 Exercises",
    "text": "7.7 Exercises\n\nProve that, if \\(f(x)=w\\cdot x+b=0\\) is a hyperplane in \\(\\mathbf{R}^{k}\\), then the two “sides” of this hyperplane, consisting of the points where \\(f(x)\\ge 0\\) and \\(f(x)\\le 0\\), are both convex sets.\nProve that \\(C(S)\\) is the intersection of all the halfplanes \\(f(x)\\ge 0\\) as \\(f(x)=w\\cdot x+b\\) runs through all supporting hyperplanes for \\(S\\) where \\(f(x)\\ge 0\\) for all \\(p\\in S\\).\nProve that \\(C(S)\\) is bounded. Hint: show that \\(S\\) is contained in a sphere of sufficiently large radius centered at zero, and then that \\(C(S)\\) is contained in that sphere as well.\nConfirm the final formula for the optimal margin classifier at the end of the lecture.\n\n\n\n\n\n[1] Boser, B., Guyon, I. and Vapnik, V. A training algorithm for optimal margin classifiers. In Colt ’92: Proceedings of the fifth annual workshop on computational learning theory (D. Haussler, ed) pp 144–52. ACM.\n\n\n[2] Bennett, K. P. and Bredensteiner, E. J. (2000). Duality and geometry in SVM classifiers. In Proceedings of the seventeenth international conference on machine learning (P. Langley, ed). Morgan Kaufmann Publishers.\n\n\n[3] Gorman, K. B., Williams, T. D. and Fraser, W. R. (2014). Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis). PLoS ONE 9(3) –13.\n\n\n[4] Horst, A. Palmer penguins.Available at https://github.com/allisonhorst/palmerpenguins.\n\n\n[5] Platt, J. C. (1998). Sequential minimal optimization: A fast algorithm for training support vector machines. Microsoft Research.\n\n\n[6] Schölkopf, B., Burges, C. and Smola, A. (1998). Advances in kernel methods: Support vector learning. MIT Press."
  },
  {
    "objectID": "chapters/20-references.html",
    "href": "chapters/20-references.html",
    "title": "References",
    "section": "",
    "text": "[1] U.C. Irvine ML\nRepository. Auto MPG Dataset.Available at http://https://archive.ics.uci.edu/ml/datasets/Auto+MPG.\n\n\n[2] U.C. Irvine ML\nRepository. Sentiment Labelled Sentences Data\nSet.Available at https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences.\n\n\n[3] Jack Daoud.\nMarketing Analytics.Available at https://www.kaggle.com/datasets/jackdaoud/marketing-data.\n\n\n[4] Boser, B., Guyon, I. and Vapnik, V. A training algorithm for optimal margin\nclassifiers. In Colt ’92: Proceedings of the fifth annual\nworkshop on computational learning theory (D. Haussler, ed) pp\n144–52. ACM.\n\n\n[5] LeCun, Y., Cortes, C. and Burges, C. The MNIST\nDatabase.Available at http://yann.lecun.com/exdb/mnist/.\n\n\n[6] Gorman, K. B.,\nWilliams, T. D. and Fraser, W. R. (2014). Ecological sexual\ndimorphism and environmental variability within a community of antarctic\npenguins (genus pygoscelis). PLoS ONE 9(3)\n–13.\n\n\n[7] Horst, A. Palmer\npenguins.Available at https://github.com/allisonhorst/palmerpenguins.\n\n\n[8] Schölkopf, B.,\nBurges, C. and Smola, A. (1998). Advances in kernel\nmethods: Support vector learning. MIT Press.\n\n\n[9] Platt, J. C.\n(1998). Sequential\nminimal optimization: A fast algorithm for training support vector\nmachines. Microsoft Research.\n\n\n[10] Bennett, K. P.\nand Bredensteiner, E. J. (2000). Duality\nand geometry in SVM classifiers. In Proceedings of the seventeenth\ninternational conference on machine learning (P. Langley, ed).\nMorgan Kaufmann Publishers.\n\n\n[11] Bertsekas, D. P.\nand Tsitsiklis, J. N. (2008).\nIntroduction to probability. Athena Scientific."
  }
]